{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "        self.pretrain_vec = [] # should match index order of words in dict.\n",
    "\n",
    "    def add_word(self, word, vec=None):\n",
    "        if vec is None:\n",
    "            if word not in self.word2idx:\n",
    "                self.idx2word.append(word)\n",
    "                self.word2idx[word] = len(self.idx2word) - 1\n",
    "        else:\n",
    "            if word not in self.word2idx:\n",
    "                self.pretrain_vec.append(vec)\n",
    "                self.idx2word.append(word)\n",
    "                self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, path, language):\n",
    "        self.dictionary = Dictionary()\n",
    "        if language is not None:\n",
    "            self.pretrained = self.add_pretrained(os.path.join('', 'wiki.' + language + '.vec'))\n",
    "        #self.trainid, self.trainlab, self.trainidx = self.tokenize_by_user(os.path.join(path, 'train.csv'),True)\n",
    "        #self.validid, self.validlab, self.valididx = self.tokenize_by_user(os.path.join(path, 'valid.csv'),False)\n",
    "        #self.testid, self.testlab, self.testidx = self.tokenize_by_user(os.path.join(path, 'test.csv'),False)\n",
    "        self.X_train, self.y_train = self.tokenize(os.path.join('', 'train.csv'),True)\n",
    "        #self.X_valid, self.y_valid = self.tokenize(os.path.join(path, 'valid.csv'),False)\n",
    "        self.X_test, self.y_test = self.tokenize(os.path.join('', 'test.csv'),False)\n",
    "\n",
    "    def add_pretrained(self, path):\n",
    "        assert os.path.exists(path)\n",
    "\n",
    "        # Add words with pretrained vectors to the dictionary\n",
    "        # might be weird because no eos was added?\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                words = line.split()\n",
    "                if len(words) == 2: #first line\n",
    "                    continue\n",
    "                word = words[0]\n",
    "                vec = words[1:]\n",
    "                if len(vec) != 300:\n",
    "                    continue #this skips the space embedding\n",
    "                #vec = np.array(list(map(float, vec)))\n",
    "                vec = list(map(float,vec))\n",
    "                tokens += 1\n",
    "                \n",
    "                self.dictionary.add_word(word, vec)\n",
    "    def tokenize(self, path, header):\n",
    "        \"\"\"Tokenizes a text file.\"\"\"\n",
    "        assert os.path.exists(path)\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.reader(f)\n",
    "            tokens = 0\n",
    "            prev = None\n",
    "            if header:\n",
    "                first = True\n",
    "            else:\n",
    "                first = False\n",
    "            tweet_count = 0\n",
    "            user_idx = -1\n",
    "            for row in reader:\n",
    "                if first:\n",
    "                    first = False\n",
    "                    continue\n",
    "                if len(row) is not 6:\n",
    "                    continue\n",
    "                \n",
    "                tweet = row[0]\n",
    "                label = row[1]\n",
    "                if not label.isdigit():\n",
    "                    continue\n",
    "                extra = row[2:5] #bio, tweet pic, profile pic, user id\n",
    "                if row[2] != prev: #new user\n",
    "                    prev = row[2]\n",
    "                    tweet_count = 0\n",
    "                    user_idx += 1\n",
    "\n",
    "                words = tweet.split()\n",
    "                tokens += len(words)\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "\n",
    "        # Tokenize file content\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            x = np.zeros(user_idx+1,dtype='object')\n",
    "            y = np.zeros(user_idx+1,dtype='int')\n",
    "            #ids = torch.LongTensor(tokens)\n",
    "            #idxs = torch.LongTensor(user_idx+1)\n",
    "            #labels = torch.LongTensor(user_idx+1)\n",
    "            #print(user_idx+1)\n",
    "            token = 0\n",
    "            prev = None\n",
    "\n",
    "            reader = csv.reader(f)\n",
    "            if header:\n",
    "                first = True\n",
    "            else:\n",
    "                first = False\n",
    "            user_idx = -1\n",
    "            for row in reader:\n",
    "                if first:\n",
    "                    first = False\n",
    "                    continue\n",
    "                if len(row) is not 6:\n",
    "                    continue\n",
    "                \n",
    "                tweet = row[0]\n",
    "                label = row[1]\n",
    "                if not label.isdigit():\n",
    "                    continue\n",
    "                extra = row[2:5] #bio, tweet pic, profile pic, user id\n",
    "                if row[2] != prev:\n",
    "                    tweet_idx = -1\n",
    "                    user_idx += 1\n",
    "                    prev = row[2]\n",
    "                    y[user_idx] = int(label)\n",
    "                    x[user_idx] = []\n",
    "                    #print(token, \"NEW USER\")\n",
    "                    #idxs[user_idx] = token\n",
    "                \n",
    "\n",
    "                words = tweet.split()\n",
    "                token = 0\n",
    "                tweet_idx+=1\n",
    "                if tweet_idx >=20:\n",
    "                    #print(tweet_idx)\n",
    "                    continue\n",
    "                x[user_idx].append([])\n",
    "                for word in words:\n",
    "                    x[user_idx][tweet_idx].append(self.dictionary.word2idx[word])\n",
    "                    token+=1\n",
    "                \n",
    "\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#attention functions\n",
    "\n",
    "def batch_matmul_bias(seq, weight, bias, nonlinearity=''):\n",
    "    s = None\n",
    "    bias_dim = bias.size()\n",
    "    for i in range(seq.size(0)):\n",
    "        _s = torch.mm(seq[i], weight) \n",
    "        _s_bias = _s + bias.expand(bias_dim[0], _s.size()[0]).transpose(0,1)\n",
    "        if(nonlinearity=='tanh'):\n",
    "            _s_bias = torch.tanh(_s_bias)\n",
    "        _s_bias = _s_bias.unsqueeze(0)\n",
    "        if(s is None):\n",
    "            s = _s_bias\n",
    "        else:\n",
    "            s = torch.cat((s,_s_bias),0)\n",
    "    return s.squeeze()\n",
    "\n",
    "def batch_matmul(seq, weight, nonlinearity=''):\n",
    "    s = None\n",
    "    for i in range(seq.size(0)):\n",
    "        _s = torch.mm(seq[i], weight)\n",
    "        if(nonlinearity=='tanh'):\n",
    "            _s = torch.tanh(_s)\n",
    "        _s = _s.unsqueeze(0)\n",
    "        if(s is None):\n",
    "            s = _s\n",
    "        else:\n",
    "            s = torch.cat((s,_s),0)\n",
    "    return s.squeeze()\n",
    "\n",
    "def attention_mul(rnn_outputs, att_weights):\n",
    "    attn_vectors = None\n",
    "    for i in range(rnn_outputs.size(0)):\n",
    "        h_i = rnn_outputs[i]\n",
    "        a_i = att_weights[i].unsqueeze(1).expand_as(h_i)\n",
    "        h_i = a_i * h_i\n",
    "        h_i = h_i.unsqueeze(0)\n",
    "        if(attn_vectors is None):\n",
    "            attn_vectors = h_i\n",
    "        else:\n",
    "            attn_vectors = torch.cat((attn_vectors,h_i),0)\n",
    "    return torch.sum(attn_vectors, 0).unsqueeze(0)\n",
    "\n",
    "'''\n",
    "def batch_matmul_bias(seq, weight, bias, nonlinearity=''):\n",
    "    s = None\n",
    "    bias_dim = bias.size()\n",
    "    for i in range(seq.size(0)):\n",
    "        _s = torch.mm(seq[i], weight) \n",
    "        _s_bias = _s + bias.expand(bias_dim[0], _s.size()[0]).transpose(0,1)\n",
    "        if(nonlinearity=='tanh'):\n",
    "            _s_bias = torch.tanh(_s_bias)\n",
    "        _s_bias = _s_bias.unsqueeze(0)\n",
    "        if(s is None):\n",
    "            s = _s_bias\n",
    "        else:\n",
    "            s = torch.cat((s,_s_bias),0)\n",
    "    return s.squeeze()\n",
    "    \n",
    "def batch_matmul(seq, weight, nonlinearity=''):\n",
    "    s = None\n",
    "    for i in range(seq.size(0)):\n",
    "        _s = torch.mm(seq[i], weight)\n",
    "        if(nonlinearity=='tanh'):\n",
    "            _s = torch.tanh(_s)\n",
    "        _s = _s.unsqueeze(0)\n",
    "        if(s is None):\n",
    "            s = _s\n",
    "        else:\n",
    "            s = torch.cat((s,_s),0)\n",
    "    return s.squeeze()\n",
    "\n",
    "def attention_mul(self, rnn_outputs, att_weights):\n",
    "    attn_vectors = None\n",
    "    for i in range(rnn_outputs.size(0)):\n",
    "        h_i = rnn_outputs[i]\n",
    "        a_i = att_weights[i].unsqueeze(1).expand_as(h_i)\n",
    "        h_i = a_i * h_i\n",
    "        h_i = h_i.unsqueeze(0)\n",
    "        if(attn_vectors is None):\n",
    "            attn_vectors = h_i\n",
    "        else:\n",
    "            attn_vectors = torch.cat((attn_vectors,h_i),0)\n",
    "    return torch.sum(attn_vectors, 0).unsqueeze(0)\n",
    "'''\n",
    "class AttentionWordRNN(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self, embeds, batch_size, num_tokens, embed_size, word_gru_hidden, dropout, bidirectional= True):        \n",
    "        \n",
    "        super(AttentionWordRNN, self).__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.num_tokens = num_tokens\n",
    "        self.embed_size = embed_size\n",
    "        self.word_gru_hidden = word_gru_hidden\n",
    "        self.bidirectional = bidirectional\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "        \n",
    "        \n",
    "        self.lookup = nn.Embedding(num_tokens, embed_size)\n",
    "\n",
    "        #init lookup table\n",
    "        \n",
    "\n",
    "        \n",
    "        initrange = 0.1\n",
    "\n",
    "        k = len(embeds) # the first k indices are pretrained. the rest are unknown\n",
    "        \n",
    "        if k is not 0:\n",
    "            first = np.array(embeds)\n",
    "            second = np.random.uniform(-initrange,initrange,size=(num_tokens-k,embed_size))\n",
    "            self.lookup.weight.data.copy_(torch.from_numpy(np.concatenate((first,second),axis=0)))\n",
    "        else:\n",
    "            self.lookup.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "\n",
    "        if bidirectional == True:\n",
    "            self.word_gru = nn.GRU(embed_size, word_gru_hidden, bidirectional= True)\n",
    "            self.weight_W_word = nn.Parameter(torch.Tensor(2* word_gru_hidden,2*word_gru_hidden))\n",
    "            self.bias_word = nn.Parameter(torch.Tensor(2* word_gru_hidden,1))\n",
    "            self.weight_proj_word = nn.Parameter(torch.Tensor(2*word_gru_hidden, 1))\n",
    "        else:\n",
    "            self.word_gru = nn.GRU(embed_size, word_gru_hidden, bidirectional= False)\n",
    "            self.weight_W_word = nn.Parameter(torch.Tensor(word_gru_hidden, word_gru_hidden))\n",
    "            self.bias_word = nn.Parameter(torch.Tensor(word_gru_hidden,1))\n",
    "            self.weight_proj_word = nn.Parameter(torch.Tensor(word_gru_hidden, 1))\n",
    "            \n",
    "        self.softmax_word = nn.Softmax()\n",
    "        #self.word_gru.data.uniform_(-initrange,initrange)\n",
    "        self.weight_W_word.data.uniform_(-initrange, initrange)\n",
    "        self.weight_proj_word.data.uniform_(-initrange,initrange)\n",
    "        self.bias_word.data.uniform_(-initrange,initrange)\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, embed, state_word):\n",
    "        # embeddings\n",
    "        #print(embed)\n",
    "        embedded = self.drop(self.lookup(embed))\n",
    "        # word level gru\n",
    "        #state_word = self.drop(state_word) #idk\n",
    "        output_word, state_word = self.word_gru(embedded, state_word)\n",
    "        state_word = self.drop(state_word) #idk\n",
    "        output_word = self.drop(output_word)\n",
    "#         print output_word.size()\n",
    "        word_squish = self.drop(batch_matmul_bias(output_word, self.weight_W_word,self.bias_word, nonlinearity='tanh'))\n",
    "        word_attn = self.drop(batch_matmul(word_squish, self.weight_proj_word))\n",
    "        word_attn_norm = self.drop(self.softmax_word(word_attn.transpose(1,0)))\n",
    "        word_attn_vectors = self.drop(attention_mul(output_word, word_attn_norm.transpose(1,0)))\n",
    "        return word_attn_vectors, state_word, word_attn_norm\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        if self.bidirectional == True:\n",
    "            return Variable(torch.zeros(2, self.batch_size, self.word_gru_hidden))\n",
    "        else:\n",
    "            return Variable(torch.zeros(1, self.batch_size, self.word_gru_hidden))\n",
    "\n",
    "class AttentionSentRNN(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self, batch_size, sent_gru_hidden, word_gru_hidden, n_classes, dropout, bidirectional= True):        \n",
    "        \n",
    "        super(AttentionSentRNN, self).__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.sent_gru_hidden = sent_gru_hidden\n",
    "        self.n_classes = n_classes\n",
    "        self.word_gru_hidden = word_gru_hidden\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "        initrange = 0.1\n",
    "        \n",
    "        \n",
    "        if bidirectional == True:\n",
    "            self.sent_gru = nn.GRU(2 * word_gru_hidden, sent_gru_hidden, bidirectional= True)        \n",
    "            self.weight_W_sent = nn.Parameter(torch.Tensor(2* sent_gru_hidden ,2* sent_gru_hidden))\n",
    "            self.bias_sent = nn.Parameter(torch.Tensor(2* sent_gru_hidden,1))\n",
    "            self.weight_proj_sent = nn.Parameter(torch.Tensor(2* sent_gru_hidden, 1))\n",
    "            self.final_linear = nn.Linear(2* sent_gru_hidden, n_classes)\n",
    "        else:\n",
    "            self.sent_gru = nn.GRU(word_gru_hidden, sent_gru_hidden, bidirectional= True)        \n",
    "            self.weight_W_sent = nn.Parameter(torch.Tensor(sent_gru_hidden ,sent_gru_hidden))\n",
    "            self.bias_sent = nn.Parameter(torch.Tensor(sent_gru_hidden,1))\n",
    "            self.weight_proj_sent = nn.Parameter(torch.Tensor(sent_gru_hidden, 1))\n",
    "            self.final_linear = nn.Linear(sent_gru_hidden, n_classes)\n",
    "        self.softmax_sent = nn.Softmax()\n",
    "        self.final_softmax = nn.Softmax()\n",
    "        self.bias_sent.data.uniform_(-initrange, initrange)\n",
    "        #self.sent_gru.data.uniform_(-initrange,initrange)\n",
    "        self.weight_W_sent.data.uniform_(-initrange, initrange)\n",
    "        self.weight_proj_sent.data.uniform_(-initrange,initrange)\n",
    "        \n",
    "        \n",
    "    def forward(self, word_attention_vectors, state_sent):\n",
    "        #MANUALLY DROPOUT THE GRU\n",
    "        #state_word = self.drop(state_sent)\n",
    "        output_sent, state_sent = self.sent_gru(word_attention_vectors, state_sent)   \n",
    "        state_word = self.drop(state_sent)\n",
    "        output_sent = self.drop(output_sent)\n",
    "        sent_squish = self.drop(batch_matmul_bias(output_sent, self.weight_W_sent,self.bias_sent, nonlinearity='tanh'))\n",
    "        sent_attn = self.drop(batch_matmul(sent_squish, self.weight_proj_sent))\n",
    "        sent_attn_norm = self.drop(self.softmax_sent(sent_attn.transpose(1,0)))\n",
    "        sent_attn_vectors = self.drop(attention_mul(output_sent, sent_attn_norm.transpose(1,0)))    \n",
    "        # final classifier\n",
    "        final_map = self.final_linear(sent_attn_vectors.squeeze(0))\n",
    "        return F.log_softmax(final_map), state_sent, sent_attn_norm\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        if self.bidirectional == True:\n",
    "            return Variable(torch.zeros(2, self.batch_size, self.sent_gru_hidden))\n",
    "        else:\n",
    "            return Variable(torch.zeros(1, self.batch_size, self.sent_gru_hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import model\n",
    "#import data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import sklearn.metrics\n",
    "\n",
    "dropout=0\n",
    "my_batch=64\n",
    "lang='en'\n",
    "datapath = './data/'+lang\n",
    "corpus = Corpus(datapath, lang)\n",
    "ntokens = len(corpus.dictionary)\n",
    "pretrain = corpus.dictionary.pretrain_vec\n",
    "\n",
    "word_attn = AttentionWordRNN(embeds=pretrain, batch_size=my_batch, num_tokens=ntokens, embed_size=300, \n",
    "                             word_gru_hidden=100, dropout=dropout, bidirectional= True)\n",
    "\n",
    "sent_attn = AttentionSentRNN(batch_size=my_batch, sent_gru_hidden=100, word_gru_hidden=100, \n",
    "                             n_classes=2, dropout=dropout, bidirectional= True)\n",
    "\n",
    "def train_data(mini_batch, targets, word_attn_model, sent_attn_model, word_optimizer, sent_optimizer, criterion):\n",
    "    state_word = word_attn_model.init_hidden().cuda()\n",
    "    state_sent = sent_attn_model.init_hidden().cuda()\n",
    "    max_sents, batch_size, max_tokens = mini_batch.size()\n",
    "    word_optimizer.zero_grad()\n",
    "    sent_optimizer.zero_grad()\n",
    "    s = None\n",
    "    for i in range(max_sents):\n",
    "        #torch.cuda.empty_cache()\n",
    "        _s, state_word, _ = word_attn_model(mini_batch[i,:,:].transpose(0,1), state_word)\n",
    "        if(s is None):\n",
    "            s = _s\n",
    "        else:\n",
    "            s = torch.cat((s,_s),0)            \n",
    "    y_pred, state_sent, _ = sent_attn_model(s, state_sent)\n",
    "    loss = criterion(y_pred.cuda(), targets)\n",
    "\n",
    "    state_word = None\n",
    "    state_sent = None\n",
    "    max_sents = None\n",
    "    batch_size = None\n",
    "    max_tokens = None \n",
    "    mini_batch = None\n",
    "    torch.cuda.empty_cache()\n",
    "    loss.backward()\n",
    "    \n",
    "    word_optimizer.step()\n",
    "    sent_optimizer.step()\n",
    "    \n",
    "    return loss.data.item()\n",
    "\n",
    "\n",
    "\n",
    "def get_predictions(val_tokens, word_attn_model, sent_attn_model):\n",
    "    max_sents, batch_size, max_tokens = val_tokens.size()\n",
    "    state_word = word_attn_model.init_hidden().cuda()\n",
    "    state_sent = sent_attn_model.init_hidden().cuda()\n",
    "    s = None\n",
    "    for i in range(max_sents):\n",
    "        _s, state_word, _ = word_attn_model(val_tokens[i,:,:].transpose(0,1), state_word)\n",
    "        if(s is None):\n",
    "            s = _s\n",
    "        else:\n",
    "            s = torch.cat((s,_s),0)            \n",
    "    y_pred, state_sent, _ = sent_attn_model(s, state_sent)    \n",
    "    return y_pred\n",
    "\n",
    "\n",
    "\n",
    "#learning_rate = 0.001\n",
    "#momentum = 0.9\n",
    "#word_optimizer = torch.optim.SGD(word_attn.parameters(), lr=learning_rate, momentum= momentum)\n",
    "#sent_optimizer = torch.optim.SGD(sent_attn.parameters(), lr=learning_rate, momentum= momentum)\n",
    "word_optimizer = torch.optim.Adam(word_attn.parameters())\n",
    "sent_optimizer = torch.optim.Adam(sent_attn.parameters())\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "word_attn.cuda()\n",
    "sent_attn.cuda()\n",
    "\n",
    "\n",
    "\n",
    "def pad_batch(mini_batch):\n",
    "    mini_batch_size = len(mini_batch)\n",
    "    max_sent_len = int(np.mean([len(x) for x in mini_batch]))\n",
    "    max_token_len = int(np.mean([len(val) for sublist in mini_batch for val in sublist]))\n",
    "    main_matrix = np.zeros((mini_batch_size, max_sent_len, max_token_len), dtype= np.int)\n",
    "    for i in range(main_matrix.shape[0]):\n",
    "        for j in range(main_matrix.shape[1]):\n",
    "            for k in range(main_matrix.shape[2]):\n",
    "                try:\n",
    "                    main_matrix[i,j,k] = mini_batch[i][j][k]\n",
    "                except IndexError:\n",
    "                    pass\n",
    "    #return Variable(torch.from_numpy(main_matrix).transpose(0,1))\n",
    "    return Variable(torch.LongTensor(main_matrix).transpose(0,1))\n",
    "\n",
    "\n",
    "\n",
    "def test_accuracy_mini_batch(tokens, labels, word_attn, sent_attn):\n",
    "    y_pred = get_predictions(tokens, word_attn, sent_attn)\n",
    "    #print(\"PRED\",y_pred)\n",
    "    _, y_pred = torch.max(y_pred, 1)\n",
    "    correct = np.ndarray.flatten(y_pred.data.cpu().numpy())\n",
    "    labels = np.ndarray.flatten(labels.data.cpu().numpy())\n",
    "    #print(\"CORR\",correct)\n",
    "    #print(\"LABELS\",labels)\n",
    "    num_correct = sum(correct == labels)\n",
    "    return float(num_correct) / len(correct)\n",
    "\n",
    "def test_accuracy_full_batch(tokens, labels, mini_batch_size, word_attn, sent_attn):\n",
    "    p = []\n",
    "    p_nonlinear = []\n",
    "    l = []\n",
    "    g = gen_minibatch(tokens, labels, mini_batch_size)\n",
    "    for token, label in g:\n",
    "        y_pred = get_predictions(token.cuda(), word_attn, sent_attn)\n",
    "        #print(\"BEFORE\",y_pred)\n",
    "        p_nonlinear.append(np.ndarray.flatten(y_pred[:,1].data.cpu().numpy()))\n",
    "        _, y_pred = torch.max(y_pred, 1)\n",
    "        #print(\"AFTER\",y_pred)\n",
    "        p.append(np.ndarray.flatten(y_pred.data.cpu().numpy()))\n",
    "        l.append(np.ndarray.flatten(label.data.cpu().numpy()))\n",
    "    p = [item for sublist in p for item in sublist]\n",
    "    l = [item for sublist in l for item in sublist]\n",
    "    p_nonlinear = [np.exp(item) for sublist in p_nonlinear for item in sublist]\n",
    "    p = np.array(p)\n",
    "    l = np.array(l)\n",
    "    #print(\"TOKEN LEN\",len(tokens))\n",
    "    #print(\"NONLINEAR\",p_nonlinear)\n",
    "    #print(\"PREDICT\",p)\n",
    "    #print(\"LABEL\",l)\n",
    "    num_correct = sum(p == l)\n",
    "    return float(num_correct)/ len(p), sklearn.metrics.roc_auc_score(l, p_nonlinear)\n",
    "\n",
    "def test_data(mini_batch, targets, word_attn_model, sent_attn_model):    \n",
    "    state_word = word_attn_model.init_hidden().cuda()\n",
    "    state_sent = sent_attn_model.init_hidden().cuda()\n",
    "    max_sents, batch_size, max_tokens = mini_batch.size()\n",
    "    s = None\n",
    "    for i in range(max_sents):\n",
    "        _s, state_word, _ = word_attn_model(mini_batch[i,:,:].transpose(0,1), state_word)\n",
    "        if(s is None):\n",
    "            s = _s\n",
    "        else:\n",
    "            s = torch.cat((s,_s),0)            \n",
    "    y_pred, state_sent,_ = sent_attn_model(s, state_sent)\n",
    "    loss = criterion(y_pred.cuda(), targets)     \n",
    "    return loss.data.item()\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert inputs.shape[0] == targets.shape[0]\n",
    "    if shuffle:\n",
    "        indices = np.arange(inputs.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "    #print(inputs.shape[0] - batchsize+1, batchsize, \"HOO\")\n",
    "    for start_idx in range(0, inputs.shape[0] - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]\n",
    "\n",
    "\n",
    "\n",
    "def gen_minibatch(tokens, labels, mini_batch_size, shuffle= True):\n",
    "    for token, label in iterate_minibatches(tokens, labels, mini_batch_size, shuffle= shuffle):\n",
    "        token = pad_batch(token)\n",
    "        yield token.cuda(), Variable(torch.LongTensor(label), requires_grad= False).cuda()\n",
    "\n",
    "def check_val_loss(val_tokens, val_labels, mini_batch_size, word_attn_model, sent_attn_model):\n",
    "    val_loss = []\n",
    "    for token, label in iterate_minibatches(val_tokens, val_labels, mini_batch_size, shuffle= True):\n",
    "        val_loss.append(test_data(pad_batch(token).cuda(), Variable(torch.LongTensor(label), requires_grad= False).cuda(), \n",
    "                                  word_attn_model, sent_attn_model))\n",
    "    return np.mean(val_loss)\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def train_early_stopping(mini_batch_size, X_train, y_train, X_test, y_test, word_attn_model, sent_attn_model, \n",
    "                         word_attn_optimiser, sent_attn_optimiser, loss_criterion, num_epoch, \n",
    "                         print_val_loss_every = 1000, print_loss_every = 50):\n",
    "    #for i in word_attn_model.parameters():\n",
    "        #print(i.data, \"PARAM\")\n",
    "    max_eval_acc = 0\n",
    "    max_train_acc = 0\n",
    "    max_eval_aucroc = 0\n",
    "    max_train_aucroc = 0\n",
    "    word_attn_model.train()\n",
    "    sent_attn_model.train()\n",
    "    start = time.time()\n",
    "    loss_full = []\n",
    "    loss_epoch = []\n",
    "    accuracy_epoch = []\n",
    "    loss_smooth = []\n",
    "    accuracy_full = []\n",
    "    epoch_counter = 0\n",
    "    g = gen_minibatch(X_train, y_train, mini_batch_size)\n",
    "    for i in range(1, num_epoch + 1):\n",
    "        try:\n",
    "            word_attn_model.train()\n",
    "            sent_attn_model.train()\n",
    "            tokens, labels = next(g)\n",
    "            loss = train_data(tokens, labels, word_attn_model, sent_attn_model, word_attn_optimiser, sent_attn_optimiser, loss_criterion)\n",
    "            acc = test_accuracy_mini_batch(tokens, labels, word_attn_model, sent_attn_model)\n",
    "            accuracy_full.append(acc)\n",
    "            accuracy_epoch.append(acc)\n",
    "            loss_full.append(loss)\n",
    "            loss_epoch.append(loss)\n",
    "            # print loss every n passes\n",
    "            if i % print_loss_every == 0:\n",
    "                print('Loss at %d minibatches, %d epoch,(%s) is %f' %(i, epoch_counter, timeSince(start), np.mean(loss_epoch)))\n",
    "                print('Accuracy at %d minibatches is %f' % (i, np.mean(accuracy_epoch)))\n",
    "            # check validation loss every n passes\n",
    "            if i % print_val_loss_every == 0:\n",
    "                word_attn_model.eval()\n",
    "                sent_attn_model.eval()\n",
    "                val_loss = check_val_loss(X_test, y_test, mini_batch_size, word_attn_model, sent_attn_model)\n",
    "                print('Average training loss at this epoch..minibatch..%d..is %f' % (i, np.mean(loss_epoch)))\n",
    "                print('Validation loss after %d passes is %f' %(i, val_loss))\n",
    "                if val_loss > np.mean(loss_full):\n",
    "                    print('Validation loss is higher than training loss at %d is %f , stopping training!' % (i, val_loss))\n",
    "                    print('Average training loss at %d is %f' % (i, np.mean(loss_full)))\n",
    "        except StopIteration:\n",
    "            epoch_counter += 1\n",
    "            print('Reached %d epochs' % epoch_counter)\n",
    "            print('i %d' % i)\n",
    "            word_attn_model.eval()\n",
    "            sent_attn_model.eval()\n",
    "            acc, aucroc = test_accuracy_full_batch(corpus.X_test, corpus.y_test, my_batch, word_attn, sent_attn)\n",
    "            if acc>max_eval_acc:\n",
    "                max_eval_acc = acc\n",
    "            if aucroc>max_eval_aucroc:\n",
    "                max_eval_aucroc = aucroc\n",
    "            print(\"Test accuracy:\",acc)\n",
    "            print(\"Max test accruacy:\",max_eval_acc)\n",
    "            print(\"Test aucroc:\",aucroc)\n",
    "            print(\"Max test aucroc:\",max_eval_aucroc)\n",
    "            word_attn_model.train()\n",
    "            sent_attn_model.train()\n",
    "            acc, aucroc = test_accuracy_full_batch(corpus.X_train, corpus.y_train, my_batch, word_attn, sent_attn)\n",
    "            if acc>max_train_acc:\n",
    "                max_train_acc = acc\n",
    "            if aucroc>max_train_aucroc:\n",
    "                max_train_aucroc = aucroc\n",
    "            print(\"Train accuracy:\",acc)\n",
    "            print(\"Max train accruacy:\",max_train_acc)\n",
    "            print(\"Train aucroc:\",aucroc)\n",
    "            print(\"Max train aucroc:\",max_train_aucroc)\n",
    "            #if epoch_counter == 1:\n",
    "                #break\n",
    "            g = gen_minibatch(X_train, y_train, mini_batch_size)\n",
    "            loss_epoch = []\n",
    "            accuracy_epoch = []\n",
    "            if epoch_counter==20:\n",
    "                break\n",
    "    return loss_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:160: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:216: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:220: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at 1 minibatches, 0 epoch,(0m 1s) is 0.647809\n",
      "Accuracy at 1 minibatches is 0.750000\n",
      "Average training loss at this epoch..minibatch..1..is 0.647809\n",
      "Validation loss after 1 passes is 0.606373\n",
      "Loss at 2 minibatches, 0 epoch,(0m 2s) is 0.645069\n",
      "Accuracy at 2 minibatches is 0.710938\n",
      "Average training loss at this epoch..minibatch..2..is 0.645069\n",
      "Validation loss after 2 passes is 0.614336\n",
      "Loss at 3 minibatches, 0 epoch,(0m 3s) is 0.628871\n",
      "Accuracy at 3 minibatches is 0.713542\n",
      "Average training loss at this epoch..minibatch..3..is 0.628871\n",
      "Validation loss after 3 passes is 0.623119\n",
      "Loss at 4 minibatches, 0 epoch,(0m 4s) is 0.639505\n",
      "Accuracy at 4 minibatches is 0.695312\n",
      "Average training loss at this epoch..minibatch..4..is 0.639505\n",
      "Validation loss after 4 passes is 0.618722\n",
      "Loss at 5 minibatches, 0 epoch,(0m 5s) is 0.622061\n",
      "Accuracy at 5 minibatches is 0.706250\n",
      "Average training loss at this epoch..minibatch..5..is 0.622061\n",
      "Validation loss after 5 passes is 0.598139\n",
      "Reached 1 epochs\n",
      "i 6\n",
      "Test accuracy: 0.703125\n",
      "Max test accruacy: 0.703125\n",
      "Test aucroc: 0.7304093567251462\n",
      "Max test aucroc: 0.7304093567251462\n",
      "Train accuracy: 0.7125\n",
      "Max train accruacy: 0.7125\n",
      "Train aucroc: 0.7583905415713196\n",
      "Max train aucroc: 0.7583905415713196\n",
      "Loss at 7 minibatches, 1 epoch,(0m 8s) is 0.595125\n",
      "Accuracy at 7 minibatches is 0.703125\n",
      "Average training loss at this epoch..minibatch..7..is 0.595125\n",
      "Validation loss after 7 passes is 0.593260\n",
      "Loss at 8 minibatches, 1 epoch,(0m 9s) is 0.589800\n",
      "Accuracy at 8 minibatches is 0.710938\n",
      "Average training loss at this epoch..minibatch..8..is 0.589800\n",
      "Validation loss after 8 passes is 0.548152\n",
      "Loss at 9 minibatches, 1 epoch,(0m 10s) is 0.573411\n",
      "Accuracy at 9 minibatches is 0.713542\n",
      "Average training loss at this epoch..minibatch..9..is 0.573411\n",
      "Validation loss after 9 passes is 0.535595\n",
      "Loss at 10 minibatches, 1 epoch,(0m 11s) is 0.561981\n",
      "Accuracy at 10 minibatches is 0.722656\n",
      "Average training loss at this epoch..minibatch..10..is 0.561981\n",
      "Validation loss after 10 passes is 0.551904\n",
      "Loss at 11 minibatches, 1 epoch,(0m 12s) is 0.552724\n",
      "Accuracy at 11 minibatches is 0.725000\n",
      "Average training loss at this epoch..minibatch..11..is 0.552724\n",
      "Validation loss after 11 passes is 0.541816\n",
      "Reached 2 epochs\n",
      "i 12\n",
      "Test accuracy: 0.703125\n",
      "Max test accruacy: 0.703125\n",
      "Test aucroc: 0.8393608074011775\n",
      "Max test aucroc: 0.8393608074011775\n",
      "Train accuracy: 0.703125\n",
      "Max train accruacy: 0.7125\n",
      "Train aucroc: 0.840448288690476\n",
      "Max train aucroc: 0.840448288690476\n",
      "Loss at 13 minibatches, 2 epoch,(0m 15s) is 0.581129\n",
      "Accuracy at 13 minibatches is 0.625000\n",
      "Average training loss at this epoch..minibatch..13..is 0.581129\n",
      "Validation loss after 13 passes is 0.513364\n",
      "Loss at 14 minibatches, 2 epoch,(0m 16s) is 0.538459\n",
      "Accuracy at 14 minibatches is 0.695312\n",
      "Average training loss at this epoch..minibatch..14..is 0.538459\n",
      "Validation loss after 14 passes is 0.521784\n",
      "Loss at 15 minibatches, 2 epoch,(0m 17s) is 0.526160\n",
      "Accuracy at 15 minibatches is 0.703125\n",
      "Average training loss at this epoch..minibatch..15..is 0.526160\n",
      "Validation loss after 15 passes is 0.490453\n",
      "Loss at 16 minibatches, 2 epoch,(0m 18s) is 0.493845\n",
      "Accuracy at 16 minibatches is 0.742188\n",
      "Average training loss at this epoch..minibatch..16..is 0.493845\n",
      "Validation loss after 16 passes is 0.475240\n",
      "Loss at 17 minibatches, 2 epoch,(0m 19s) is 0.496605\n",
      "Accuracy at 17 minibatches is 0.734375\n",
      "Average training loss at this epoch..minibatch..17..is 0.496605\n",
      "Validation loss after 17 passes is 0.480784\n",
      "Reached 3 epochs\n",
      "i 18\n",
      "Test accuracy: 0.7734375\n",
      "Max test accruacy: 0.7734375\n",
      "Test aucroc: 0.8570175438596492\n",
      "Max test aucroc: 0.8570175438596492\n",
      "Train accuracy: 0.746875\n",
      "Max train accruacy: 0.746875\n",
      "Train aucroc: 0.8651854641310488\n",
      "Max train aucroc: 0.8651854641310488\n",
      "Loss at 19 minibatches, 3 epoch,(0m 22s) is 0.511846\n",
      "Accuracy at 19 minibatches is 0.734375\n",
      "Average training loss at this epoch..minibatch..19..is 0.511846\n",
      "Validation loss after 19 passes is 0.406821\n",
      "Loss at 20 minibatches, 3 epoch,(0m 23s) is 0.450920\n",
      "Accuracy at 20 minibatches is 0.781250\n",
      "Average training loss at this epoch..minibatch..20..is 0.450920\n",
      "Validation loss after 20 passes is 0.422005\n",
      "Loss at 21 minibatches, 3 epoch,(0m 24s) is 0.431082\n",
      "Accuracy at 21 minibatches is 0.791667\n",
      "Average training loss at this epoch..minibatch..21..is 0.431082\n",
      "Validation loss after 21 passes is 0.400601\n",
      "Loss at 22 minibatches, 3 epoch,(0m 25s) is 0.406958\n",
      "Accuracy at 22 minibatches is 0.812500\n",
      "Average training loss at this epoch..minibatch..22..is 0.406958\n",
      "Validation loss after 22 passes is 0.361437\n",
      "Loss at 23 minibatches, 3 epoch,(0m 26s) is 0.397558\n",
      "Accuracy at 23 minibatches is 0.825000\n",
      "Average training loss at this epoch..minibatch..23..is 0.397558\n",
      "Validation loss after 23 passes is 0.365967\n",
      "Reached 4 epochs\n",
      "i 24\n",
      "Test accuracy: 0.8671875\n",
      "Max test accruacy: 0.8671875\n",
      "Test aucroc: 0.9227888216652262\n",
      "Max test aucroc: 0.9227888216652262\n",
      "Train accuracy: 0.853125\n",
      "Max train accruacy: 0.853125\n",
      "Train aucroc: 0.9188596491228069\n",
      "Max train aucroc: 0.9188596491228069\n",
      "Loss at 25 minibatches, 4 epoch,(0m 29s) is 0.399689\n",
      "Accuracy at 25 minibatches is 0.843750\n",
      "Average training loss at this epoch..minibatch..25..is 0.399689\n",
      "Validation loss after 25 passes is 0.338482\n",
      "Loss at 26 minibatches, 4 epoch,(0m 30s) is 0.312144\n",
      "Accuracy at 26 minibatches is 0.882812\n",
      "Average training loss at this epoch..minibatch..26..is 0.312144\n",
      "Validation loss after 26 passes is 0.300117\n",
      "Loss at 27 minibatches, 4 epoch,(0m 31s) is 0.307180\n",
      "Accuracy at 27 minibatches is 0.880208\n",
      "Average training loss at this epoch..minibatch..27..is 0.307180\n",
      "Validation loss after 27 passes is 0.301040\n",
      "Loss at 28 minibatches, 4 epoch,(0m 32s) is 0.283033\n",
      "Accuracy at 28 minibatches is 0.890625\n",
      "Average training loss at this epoch..minibatch..28..is 0.283033\n",
      "Validation loss after 28 passes is 0.298530\n",
      "Loss at 29 minibatches, 4 epoch,(0m 33s) is 0.293638\n",
      "Accuracy at 29 minibatches is 0.890625\n",
      "Average training loss at this epoch..minibatch..29..is 0.293638\n",
      "Validation loss after 29 passes is 0.275731\n",
      "Reached 5 epochs\n",
      "i 30\n",
      "Test accuracy: 0.8984375\n",
      "Max test accruacy: 0.8984375\n",
      "Test aucroc: 0.9400584795321638\n",
      "Max test aucroc: 0.9400584795321638\n",
      "Train accuracy: 0.909375\n",
      "Max train accruacy: 0.909375\n",
      "Train aucroc: 0.965665236051502\n",
      "Max train aucroc: 0.965665236051502\n",
      "Loss at 31 minibatches, 5 epoch,(0m 35s) is 0.130177\n",
      "Accuracy at 31 minibatches is 0.968750\n",
      "Average training loss at this epoch..minibatch..31..is 0.130177\n",
      "Validation loss after 31 passes is 0.268017\n",
      "Loss at 32 minibatches, 5 epoch,(0m 36s) is 0.170480\n",
      "Accuracy at 32 minibatches is 0.953125\n",
      "Average training loss at this epoch..minibatch..32..is 0.170480\n",
      "Validation loss after 32 passes is 0.272015\n",
      "Loss at 33 minibatches, 5 epoch,(0m 38s) is 0.171614\n",
      "Accuracy at 33 minibatches is 0.953125\n",
      "Average training loss at this epoch..minibatch..33..is 0.171614\n",
      "Validation loss after 33 passes is 0.272555\n",
      "Loss at 34 minibatches, 5 epoch,(0m 39s) is 0.178102\n",
      "Accuracy at 34 minibatches is 0.953125\n",
      "Average training loss at this epoch..minibatch..34..is 0.178102\n",
      "Validation loss after 34 passes is 0.257084\n",
      "Loss at 35 minibatches, 5 epoch,(0m 40s) is 0.179972\n",
      "Accuracy at 35 minibatches is 0.950000\n",
      "Average training loss at this epoch..minibatch..35..is 0.179972\n",
      "Validation loss after 35 passes is 0.191450\n",
      "Reached 6 epochs\n",
      "i 36\n",
      "Test accuracy: 0.8984375\n",
      "Max test accruacy: 0.8984375\n",
      "Test aucroc: 0.9576023391812866\n",
      "Max test aucroc: 0.9576023391812866\n",
      "Train accuracy: 0.9625\n",
      "Max train accruacy: 0.9625\n",
      "Train aucroc: 0.9885311195354863\n",
      "Max train aucroc: 0.9885311195354863\n",
      "Loss at 37 minibatches, 6 epoch,(0m 42s) is 0.129518\n",
      "Accuracy at 37 minibatches is 0.968750\n",
      "Average training loss at this epoch..minibatch..37..is 0.129518\n",
      "Validation loss after 37 passes is 0.215296\n",
      "Loss at 38 minibatches, 6 epoch,(0m 43s) is 0.141921\n",
      "Accuracy at 38 minibatches is 0.960938\n",
      "Average training loss at this epoch..minibatch..38..is 0.141921\n",
      "Validation loss after 38 passes is 0.237799\n",
      "Loss at 39 minibatches, 6 epoch,(0m 45s) is 0.144750\n",
      "Accuracy at 39 minibatches is 0.958333\n",
      "Average training loss at this epoch..minibatch..39..is 0.144750\n",
      "Validation loss after 39 passes is 0.222209\n",
      "Loss at 40 minibatches, 6 epoch,(0m 46s) is 0.132154\n",
      "Accuracy at 40 minibatches is 0.968750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at this epoch..minibatch..40..is 0.132154\n",
      "Validation loss after 40 passes is 0.201171\n",
      "Loss at 41 minibatches, 6 epoch,(0m 47s) is 0.123768\n",
      "Accuracy at 41 minibatches is 0.971875\n",
      "Average training loss at this epoch..minibatch..41..is 0.123768\n",
      "Validation loss after 41 passes is 0.236251\n",
      "Reached 7 epochs\n",
      "i 42\n",
      "Test accuracy: 0.921875\n",
      "Max test accruacy: 0.921875\n",
      "Test aucroc: 0.9554499554499555\n",
      "Max test aucroc: 0.9576023391812866\n",
      "Train accuracy: 0.9875\n",
      "Max train accruacy: 0.9875\n",
      "Train aucroc: 0.997942789968652\n",
      "Max train aucroc: 0.997942789968652\n",
      "Loss at 43 minibatches, 7 epoch,(0m 49s) is 0.090855\n",
      "Accuracy at 43 minibatches is 0.968750\n",
      "Average training loss at this epoch..minibatch..43..is 0.090855\n",
      "Validation loss after 43 passes is 0.210671\n",
      "Loss at 44 minibatches, 7 epoch,(0m 50s) is 0.065009\n",
      "Accuracy at 44 minibatches is 0.984375\n",
      "Average training loss at this epoch..minibatch..44..is 0.065009\n",
      "Validation loss after 44 passes is 0.231206\n",
      "Loss at 45 minibatches, 7 epoch,(0m 51s) is 0.052000\n",
      "Accuracy at 45 minibatches is 0.989583\n",
      "Average training loss at this epoch..minibatch..45..is 0.052000\n",
      "Validation loss after 45 passes is 0.234324\n",
      "Loss at 46 minibatches, 7 epoch,(0m 52s) is 0.055782\n",
      "Accuracy at 46 minibatches is 0.988281\n",
      "Average training loss at this epoch..minibatch..46..is 0.055782\n",
      "Validation loss after 46 passes is 0.200124\n",
      "Loss at 47 minibatches, 7 epoch,(0m 54s) is 0.053932\n",
      "Accuracy at 47 minibatches is 0.987500\n",
      "Average training loss at this epoch..minibatch..47..is 0.053932\n",
      "Validation loss after 47 passes is 0.217111\n",
      "Reached 8 epochs\n",
      "i 48\n",
      "Test accuracy: 0.9296875\n",
      "Max test accruacy: 0.9296875\n",
      "Test aucroc: 0.9777777777777777\n",
      "Max test aucroc: 0.9777777777777777\n",
      "Train accuracy: 0.9875\n",
      "Max train accruacy: 0.9875\n",
      "Train aucroc: 0.9998040752351097\n",
      "Max train aucroc: 0.9998040752351097\n",
      "Loss at 49 minibatches, 8 epoch,(0m 56s) is 0.043944\n",
      "Accuracy at 49 minibatches is 0.984375\n",
      "Average training loss at this epoch..minibatch..49..is 0.043944\n",
      "Validation loss after 49 passes is 0.251451\n",
      "Loss at 50 minibatches, 8 epoch,(0m 57s) is 0.042226\n",
      "Accuracy at 50 minibatches is 0.984375\n",
      "Average training loss at this epoch..minibatch..50..is 0.042226\n",
      "Validation loss after 50 passes is 0.198937\n",
      "Loss at 51 minibatches, 8 epoch,(0m 58s) is 0.031973\n",
      "Accuracy at 51 minibatches is 0.989583\n",
      "Average training loss at this epoch..minibatch..51..is 0.031973\n",
      "Validation loss after 51 passes is 0.258152\n",
      "Loss at 52 minibatches, 8 epoch,(0m 59s) is 0.027768\n",
      "Accuracy at 52 minibatches is 0.992188\n",
      "Average training loss at this epoch..minibatch..52..is 0.027768\n",
      "Validation loss after 52 passes is 0.233090\n",
      "Loss at 53 minibatches, 8 epoch,(1m 1s) is 0.023589\n",
      "Accuracy at 53 minibatches is 0.993750\n",
      "Average training loss at this epoch..minibatch..53..is 0.023589\n",
      "Validation loss after 53 passes is 0.212351\n",
      "Reached 9 epochs\n",
      "i 54\n",
      "Test accuracy: 0.9140625\n",
      "Max test accruacy: 0.9296875\n",
      "Test aucroc: 0.9654970760233919\n",
      "Max test aucroc: 0.9777777777777777\n",
      "Train accuracy: 1.0\n",
      "Max train accruacy: 1.0\n",
      "Train aucroc: 1.0\n",
      "Max train aucroc: 1.0\n",
      "Loss at 55 minibatches, 9 epoch,(1m 3s) is 0.003733\n",
      "Accuracy at 55 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..55..is 0.003733\n",
      "Validation loss after 55 passes is 0.213199\n",
      "Loss at 56 minibatches, 9 epoch,(1m 4s) is 0.006307\n",
      "Accuracy at 56 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..56..is 0.006307\n",
      "Validation loss after 56 passes is 0.271705\n",
      "Loss at 57 minibatches, 9 epoch,(1m 5s) is 0.006257\n",
      "Accuracy at 57 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..57..is 0.006257\n",
      "Validation loss after 57 passes is 0.314609\n",
      "Validation loss is higher than training loss at 57 is 0.314609 , stopping training!\n",
      "Average training loss at 57 is 0.286208\n",
      "Loss at 58 minibatches, 9 epoch,(1m 6s) is 0.006609\n",
      "Accuracy at 58 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..58..is 0.006609\n",
      "Validation loss after 58 passes is 0.277473\n",
      "Loss at 59 minibatches, 9 epoch,(1m 7s) is 0.006728\n",
      "Accuracy at 59 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..59..is 0.006728\n",
      "Validation loss after 59 passes is 0.250436\n",
      "Reached 10 epochs\n",
      "i 60\n",
      "Test accuracy: 0.9140625\n",
      "Max test accruacy: 0.9296875\n",
      "Test aucroc: 0.9665130568356375\n",
      "Max test aucroc: 0.9777777777777777\n",
      "Train accuracy: 1.0\n",
      "Max train accruacy: 1.0\n",
      "Train aucroc: 1.0\n",
      "Max train aucroc: 1.0\n",
      "Loss at 61 minibatches, 10 epoch,(1m 10s) is 0.002018\n",
      "Accuracy at 61 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..61..is 0.002018\n",
      "Validation loss after 61 passes is 0.200652\n",
      "Loss at 62 minibatches, 10 epoch,(1m 11s) is 0.001990\n",
      "Accuracy at 62 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..62..is 0.001990\n",
      "Validation loss after 62 passes is 0.301983\n",
      "Validation loss is higher than training loss at 62 is 0.301983 , stopping training!\n",
      "Average training loss at 62 is 0.264555\n",
      "Loss at 63 minibatches, 10 epoch,(1m 12s) is 0.001791\n",
      "Accuracy at 63 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..63..is 0.001791\n",
      "Validation loss after 63 passes is 0.269811\n",
      "Validation loss is higher than training loss at 63 is 0.269811 , stopping training!\n",
      "Average training loss at 63 is 0.259590\n",
      "Loss at 64 minibatches, 10 epoch,(1m 13s) is 0.001688\n",
      "Accuracy at 64 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..64..is 0.001688\n",
      "Validation loss after 64 passes is 0.283016\n",
      "Validation loss is higher than training loss at 64 is 0.283016 , stopping training!\n",
      "Average training loss at 64 is 0.254808\n",
      "Loss at 65 minibatches, 10 epoch,(1m 14s) is 0.001487\n",
      "Accuracy at 65 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..65..is 0.001487\n",
      "Validation loss after 65 passes is 0.248707\n",
      "Reached 11 epochs\n",
      "i 66\n",
      "Test accuracy: 0.9375\n",
      "Max test accruacy: 0.9375\n",
      "Test aucroc: 0.97953216374269\n",
      "Max test aucroc: 0.97953216374269\n",
      "Train accuracy: 1.0\n",
      "Max train accruacy: 1.0\n",
      "Train aucroc: 1.0\n",
      "Max train aucroc: 1.0\n",
      "Loss at 67 minibatches, 11 epoch,(1m 17s) is 0.000347\n",
      "Accuracy at 67 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..67..is 0.000347\n",
      "Validation loss after 67 passes is 0.294432\n",
      "Validation loss is higher than training loss at 67 is 0.294432 , stopping training!\n",
      "Average training loss at 67 is 0.245726\n",
      "Loss at 68 minibatches, 11 epoch,(1m 18s) is 0.000349\n",
      "Accuracy at 68 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..68..is 0.000349\n",
      "Validation loss after 68 passes is 0.334066\n",
      "Validation loss is higher than training loss at 68 is 0.334066 , stopping training!\n",
      "Average training loss at 68 is 0.241421\n",
      "Loss at 69 minibatches, 11 epoch,(1m 19s) is 0.000358\n",
      "Accuracy at 69 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..69..is 0.000358\n",
      "Validation loss after 69 passes is 0.336263\n",
      "Validation loss is higher than training loss at 69 is 0.336263 , stopping training!\n",
      "Average training loss at 69 is 0.237265\n",
      "Loss at 70 minibatches, 11 epoch,(1m 20s) is 0.000353\n",
      "Accuracy at 70 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..70..is 0.000353\n",
      "Validation loss after 70 passes is 0.124375\n",
      "Loss at 71 minibatches, 11 epoch,(1m 21s) is 0.000315\n",
      "Accuracy at 71 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..71..is 0.000315\n",
      "Validation loss after 71 passes is 0.340195\n",
      "Validation loss is higher than training loss at 71 is 0.340195 , stopping training!\n",
      "Average training loss at 71 is 0.229365\n",
      "Reached 12 epochs\n",
      "i 72\n",
      "Test accuracy: 0.921875\n",
      "Max test accruacy: 0.9375\n",
      "Test aucroc: 0.9742897727272728\n",
      "Max test aucroc: 0.97953216374269\n",
      "Train accuracy: 1.0\n",
      "Max train accruacy: 1.0\n",
      "Train aucroc: 1.0\n",
      "Max train aucroc: 1.0\n",
      "Loss at 73 minibatches, 12 epoch,(1m 24s) is 0.000094\n",
      "Accuracy at 73 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..73..is 0.000094\n",
      "Validation loss after 73 passes is 0.343036\n",
      "Validation loss is higher than training loss at 73 is 0.343036 , stopping training!\n",
      "Average training loss at 73 is 0.225606\n",
      "Loss at 74 minibatches, 12 epoch,(1m 25s) is 0.000156\n",
      "Accuracy at 74 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..74..is 0.000156\n",
      "Validation loss after 74 passes is 0.316350\n",
      "Validation loss is higher than training loss at 74 is 0.316350 , stopping training!\n",
      "Average training loss at 74 is 0.221971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at 75 minibatches, 12 epoch,(1m 26s) is 0.000151\n",
      "Accuracy at 75 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..75..is 0.000151\n",
      "Validation loss after 75 passes is 0.359492\n",
      "Validation loss is higher than training loss at 75 is 0.359492 , stopping training!\n",
      "Average training loss at 75 is 0.218450\n",
      "Loss at 76 minibatches, 12 epoch,(1m 27s) is 0.000135\n",
      "Accuracy at 76 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..76..is 0.000135\n",
      "Validation loss after 76 passes is 0.319328\n",
      "Validation loss is higher than training loss at 76 is 0.319328 , stopping training!\n",
      "Average training loss at 76 is 0.215038\n",
      "Loss at 77 minibatches, 12 epoch,(1m 28s) is 0.000128\n",
      "Accuracy at 77 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..77..is 0.000128\n",
      "Validation loss after 77 passes is 0.370132\n",
      "Validation loss is higher than training loss at 77 is 0.370132 , stopping training!\n",
      "Average training loss at 77 is 0.211731\n",
      "Reached 13 epochs\n",
      "i 78\n",
      "Test accuracy: 0.9296875\n",
      "Max test accruacy: 0.9375\n",
      "Test aucroc: 0.9655797101449275\n",
      "Max test aucroc: 0.97953216374269\n",
      "Train accuracy: 1.0\n",
      "Max train accruacy: 1.0\n",
      "Train aucroc: 1.0\n",
      "Max train aucroc: 1.0\n",
      "Loss at 79 minibatches, 13 epoch,(1m 31s) is 0.000063\n",
      "Accuracy at 79 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..79..is 0.000063\n",
      "Validation loss after 79 passes is 0.207426\n",
      "Loss at 80 minibatches, 13 epoch,(1m 32s) is 0.000064\n",
      "Accuracy at 80 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..80..is 0.000064\n",
      "Validation loss after 80 passes is 0.348212\n",
      "Validation loss is higher than training loss at 80 is 0.348212 , stopping training!\n",
      "Average training loss at 80 is 0.205413\n",
      "Loss at 81 minibatches, 13 epoch,(1m 33s) is 0.000075\n",
      "Accuracy at 81 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..81..is 0.000075\n",
      "Validation loss after 81 passes is 0.387396\n",
      "Validation loss is higher than training loss at 81 is 0.387396 , stopping training!\n",
      "Average training loss at 81 is 0.202393\n",
      "Loss at 82 minibatches, 13 epoch,(1m 34s) is 0.000083\n",
      "Accuracy at 82 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..82..is 0.000083\n",
      "Validation loss after 82 passes is 0.364765\n",
      "Validation loss is higher than training loss at 82 is 0.364765 , stopping training!\n",
      "Average training loss at 82 is 0.199462\n",
      "Loss at 83 minibatches, 13 epoch,(1m 35s) is 0.000076\n",
      "Accuracy at 83 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..83..is 0.000076\n",
      "Validation loss after 83 passes is 0.130311\n",
      "Reached 14 epochs\n",
      "i 84\n",
      "Test accuracy: 0.9296875\n",
      "Max test accruacy: 0.9375\n",
      "Test aucroc: 0.9685990338164251\n",
      "Max test aucroc: 0.97953216374269\n",
      "Train accuracy: 1.0\n",
      "Max train accruacy: 1.0\n",
      "Train aucroc: 1.0\n",
      "Max train aucroc: 1.0\n",
      "Loss at 85 minibatches, 14 epoch,(1m 37s) is 0.000074\n",
      "Accuracy at 85 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..85..is 0.000074\n",
      "Validation loss after 85 passes is 0.300571\n",
      "Validation loss is higher than training loss at 85 is 0.300571 , stopping training!\n",
      "Average training loss at 85 is 0.193845\n",
      "Loss at 86 minibatches, 14 epoch,(1m 39s) is 0.000055\n",
      "Accuracy at 86 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..86..is 0.000055\n",
      "Validation loss after 86 passes is 0.380685\n",
      "Validation loss is higher than training loss at 86 is 0.380685 , stopping training!\n",
      "Average training loss at 86 is 0.191153\n",
      "Loss at 87 minibatches, 14 epoch,(1m 40s) is 0.000068\n",
      "Accuracy at 87 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..87..is 0.000068\n",
      "Validation loss after 87 passes is 0.254534\n",
      "Validation loss is higher than training loss at 87 is 0.254534 , stopping training!\n",
      "Average training loss at 87 is 0.188536\n",
      "Loss at 88 minibatches, 14 epoch,(1m 41s) is 0.000061\n",
      "Accuracy at 88 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..88..is 0.000061\n",
      "Validation loss after 88 passes is 0.355614\n",
      "Validation loss is higher than training loss at 88 is 0.355614 , stopping training!\n",
      "Average training loss at 88 is 0.185988\n",
      "Loss at 89 minibatches, 14 epoch,(1m 42s) is 0.000056\n",
      "Accuracy at 89 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..89..is 0.000056\n",
      "Validation loss after 89 passes is 0.374846\n",
      "Validation loss is higher than training loss at 89 is 0.374846 , stopping training!\n",
      "Average training loss at 89 is 0.183509\n",
      "Reached 15 epochs\n",
      "i 90\n",
      "Test accuracy: 0.921875\n",
      "Max test accruacy: 0.9375\n",
      "Test aucroc: 0.9716108452950558\n",
      "Max test aucroc: 0.97953216374269\n",
      "Train accuracy: 1.0\n",
      "Max train accruacy: 1.0\n",
      "Train aucroc: 1.0\n",
      "Max train aucroc: 1.0\n",
      "Loss at 91 minibatches, 15 epoch,(1m 44s) is 0.000057\n",
      "Accuracy at 91 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..91..is 0.000057\n",
      "Validation loss after 91 passes is 0.313430\n",
      "Validation loss is higher than training loss at 91 is 0.313430 , stopping training!\n",
      "Average training loss at 91 is 0.181095\n",
      "Loss at 92 minibatches, 15 epoch,(1m 46s) is 0.000043\n",
      "Accuracy at 92 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..92..is 0.000043\n",
      "Validation loss after 92 passes is 0.422810\n",
      "Validation loss is higher than training loss at 92 is 0.422810 , stopping training!\n",
      "Average training loss at 92 is 0.178744\n",
      "Loss at 93 minibatches, 15 epoch,(1m 47s) is 0.000050\n",
      "Accuracy at 93 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..93..is 0.000050\n",
      "Validation loss after 93 passes is 0.349079\n",
      "Validation loss is higher than training loss at 93 is 0.349079 , stopping training!\n",
      "Average training loss at 93 is 0.176453\n",
      "Loss at 94 minibatches, 15 epoch,(1m 48s) is 0.000048\n",
      "Accuracy at 94 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..94..is 0.000048\n",
      "Validation loss after 94 passes is 0.397012\n",
      "Validation loss is higher than training loss at 94 is 0.397012 , stopping training!\n",
      "Average training loss at 94 is 0.174220\n",
      "Loss at 95 minibatches, 15 epoch,(1m 49s) is 0.000043\n",
      "Accuracy at 95 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..95..is 0.000043\n",
      "Validation loss after 95 passes is 0.383835\n",
      "Validation loss is higher than training loss at 95 is 0.383835 , stopping training!\n",
      "Average training loss at 95 is 0.172042\n",
      "Reached 16 epochs\n",
      "i 96\n",
      "Test accuracy: 0.9375\n",
      "Max test accruacy: 0.9375\n",
      "Test aucroc: 0.9710575719649562\n",
      "Max test aucroc: 0.97953216374269\n",
      "Train accuracy: 1.0\n",
      "Max train accruacy: 1.0\n",
      "Train aucroc: 1.0\n",
      "Max train aucroc: 1.0\n",
      "Loss at 97 minibatches, 16 epoch,(1m 51s) is 0.000089\n",
      "Accuracy at 97 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..97..is 0.000089\n",
      "Validation loss after 97 passes is 0.334528\n",
      "Validation loss is higher than training loss at 97 is 0.334528 , stopping training!\n",
      "Average training loss at 97 is 0.169920\n",
      "Loss at 98 minibatches, 16 epoch,(1m 52s) is 0.003161\n",
      "Accuracy at 98 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..98..is 0.003161\n",
      "Validation loss after 98 passes is 0.403372\n",
      "Validation loss is higher than training loss at 98 is 0.403372 , stopping training!\n",
      "Average training loss at 98 is 0.167923\n",
      "Loss at 99 minibatches, 16 epoch,(1m 54s) is 0.002114\n",
      "Accuracy at 99 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..99..is 0.002114\n",
      "Validation loss after 99 passes is 0.336100\n",
      "Validation loss is higher than training loss at 99 is 0.336100 , stopping training!\n",
      "Average training loss at 99 is 0.165900\n",
      "Loss at 100 minibatches, 16 epoch,(1m 55s) is 0.001597\n",
      "Accuracy at 100 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..100..is 0.001597\n",
      "Validation loss after 100 passes is 0.458579\n",
      "Validation loss is higher than training loss at 100 is 0.458579 , stopping training!\n",
      "Average training loss at 100 is 0.163926\n",
      "Loss at 101 minibatches, 16 epoch,(1m 56s) is 0.001286\n",
      "Accuracy at 101 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..101..is 0.001286\n",
      "Validation loss after 101 passes is 0.395488\n",
      "Validation loss is higher than training loss at 101 is 0.395488 , stopping training!\n",
      "Average training loss at 101 is 0.161998\n",
      "Reached 17 epochs\n",
      "i 102\n",
      "Test accuracy: 0.9140625\n",
      "Max test accruacy: 0.9375\n",
      "Test aucroc: 0.9649565461171854\n",
      "Max test aucroc: 0.97953216374269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 1.0\n",
      "Max train accruacy: 1.0\n",
      "Train aucroc: 1.0\n",
      "Max train aucroc: 1.0\n",
      "Loss at 103 minibatches, 17 epoch,(1m 58s) is 0.000024\n",
      "Accuracy at 103 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..103..is 0.000024\n",
      "Validation loss after 103 passes is 0.477298\n",
      "Validation loss is higher than training loss at 103 is 0.477298 , stopping training!\n",
      "Average training loss at 103 is 0.160115\n",
      "Loss at 104 minibatches, 17 epoch,(1m 59s) is 0.000038\n",
      "Accuracy at 104 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..104..is 0.000038\n",
      "Validation loss after 104 passes is 0.410697\n",
      "Validation loss is higher than training loss at 104 is 0.410697 , stopping training!\n",
      "Average training loss at 104 is 0.158275\n",
      "Loss at 105 minibatches, 17 epoch,(2m 0s) is 0.000030\n",
      "Accuracy at 105 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..105..is 0.000030\n",
      "Validation loss after 105 passes is 0.449444\n",
      "Validation loss is higher than training loss at 105 is 0.449444 , stopping training!\n",
      "Average training loss at 105 is 0.156476\n",
      "Loss at 106 minibatches, 17 epoch,(2m 2s) is 0.000026\n",
      "Accuracy at 106 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..106..is 0.000026\n",
      "Validation loss after 106 passes is 0.529407\n",
      "Validation loss is higher than training loss at 106 is 0.529407 , stopping training!\n",
      "Average training loss at 106 is 0.154718\n",
      "Loss at 107 minibatches, 17 epoch,(2m 3s) is 0.000028\n",
      "Accuracy at 107 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..107..is 0.000028\n",
      "Validation loss after 107 passes is 0.344276\n",
      "Validation loss is higher than training loss at 107 is 0.344276 , stopping training!\n",
      "Average training loss at 107 is 0.153000\n",
      "Reached 18 epochs\n",
      "i 108\n",
      "Test accuracy: 0.875\n",
      "Max test accruacy: 0.9375\n",
      "Test aucroc: 0.9634408602150537\n",
      "Max test aucroc: 0.97953216374269\n",
      "Train accuracy: 1.0\n",
      "Max train accruacy: 1.0\n",
      "Train aucroc: 1.0\n",
      "Max train aucroc: 1.0\n",
      "Loss at 109 minibatches, 18 epoch,(2m 5s) is 0.000015\n",
      "Accuracy at 109 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..109..is 0.000015\n",
      "Validation loss after 109 passes is 0.423687\n",
      "Validation loss is higher than training loss at 109 is 0.423687 , stopping training!\n",
      "Average training loss at 109 is 0.151319\n",
      "Loss at 110 minibatches, 18 epoch,(2m 6s) is 0.000065\n",
      "Accuracy at 110 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..110..is 0.000065\n",
      "Validation loss after 110 passes is 0.522882\n",
      "Validation loss is higher than training loss at 110 is 0.522882 , stopping training!\n",
      "Average training loss at 110 is 0.149675\n",
      "Loss at 111 minibatches, 18 epoch,(2m 7s) is 0.000048\n",
      "Accuracy at 111 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..111..is 0.000048\n",
      "Validation loss after 111 passes is 0.407148\n",
      "Validation loss is higher than training loss at 111 is 0.407148 , stopping training!\n",
      "Average training loss at 111 is 0.148066\n",
      "Loss at 112 minibatches, 18 epoch,(2m 9s) is 0.000141\n",
      "Accuracy at 112 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..112..is 0.000141\n",
      "Validation loss after 112 passes is 0.390793\n",
      "Validation loss is higher than training loss at 112 is 0.390793 , stopping training!\n",
      "Average training loss at 112 is 0.146495\n",
      "Loss at 113 minibatches, 18 epoch,(2m 10s) is 0.000117\n",
      "Accuracy at 113 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..113..is 0.000117\n",
      "Validation loss after 113 passes is 0.507051\n",
      "Validation loss is higher than training loss at 113 is 0.507051 , stopping training!\n",
      "Average training loss at 113 is 0.144953\n",
      "Reached 19 epochs\n",
      "i 114\n",
      "Test accuracy: 0.890625\n",
      "Max test accruacy: 0.9375\n",
      "Test aucroc: 0.9771309771309772\n",
      "Max test aucroc: 0.97953216374269\n",
      "Train accuracy: 1.0\n",
      "Max train accruacy: 1.0\n",
      "Train aucroc: 1.0\n",
      "Max train aucroc: 1.0\n",
      "Loss at 115 minibatches, 19 epoch,(2m 12s) is 0.000723\n",
      "Accuracy at 115 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..115..is 0.000723\n",
      "Validation loss after 115 passes is 0.521266\n",
      "Validation loss is higher than training loss at 115 is 0.521266 , stopping training!\n",
      "Average training loss at 115 is 0.143451\n",
      "Loss at 116 minibatches, 19 epoch,(2m 13s) is 0.000367\n",
      "Accuracy at 116 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..116..is 0.000367\n",
      "Validation loss after 116 passes is 0.522260\n",
      "Validation loss is higher than training loss at 116 is 0.522260 , stopping training!\n",
      "Average training loss at 116 is 0.141972\n",
      "Loss at 117 minibatches, 19 epoch,(2m 14s) is 0.000253\n",
      "Accuracy at 117 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..117..is 0.000253\n",
      "Validation loss after 117 passes is 0.612720\n",
      "Validation loss is higher than training loss at 117 is 0.612720 , stopping training!\n",
      "Average training loss at 117 is 0.140524\n",
      "Loss at 118 minibatches, 19 epoch,(2m 15s) is 0.000197\n",
      "Accuracy at 118 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..118..is 0.000197\n",
      "Validation loss after 118 passes is 0.568904\n",
      "Validation loss is higher than training loss at 118 is 0.568904 , stopping training!\n",
      "Average training loss at 118 is 0.139104\n",
      "Loss at 119 minibatches, 19 epoch,(2m 16s) is 0.000161\n",
      "Accuracy at 119 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..119..is 0.000161\n",
      "Validation loss after 119 passes is 0.594294\n",
      "Validation loss is higher than training loss at 119 is 0.594294 , stopping training!\n",
      "Average training loss at 119 is 0.137714\n",
      "Reached 20 epochs\n",
      "i 120\n",
      "Test accuracy: 0.8671875\n",
      "Max test accruacy: 0.9375\n",
      "Test aucroc: 0.9685179685179686\n",
      "Max test aucroc: 0.97953216374269\n",
      "Train accuracy: 1.0\n",
      "Max train accruacy: 1.0\n",
      "Train aucroc: 1.0\n",
      "Max train aucroc: 1.0\n"
     ]
    }
   ],
   "source": [
    "loss_full= train_early_stopping(my_batch, corpus.X_train, corpus.y_train, corpus.X_test, corpus.y_test, word_attn, sent_attn, word_optimizer, sent_optimizer, \n",
    "                            criterion, 5000, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(test_accuracy_full_batch(corpus.X_test, corpus.y_test, my_batch, word_attn, sent_attn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test_accuracy_full_batch(corpus.X_train, corpus.y_train, my_batch, word_attn, sent_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
