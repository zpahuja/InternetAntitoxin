{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import csv\n",
    "import collections\n",
    "from img_to_vec import Img2Vec\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "img2vec = Img2Vec(cuda=True)\n",
    "\n",
    "\n",
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = [] #index gives word\n",
    "        self.idx2abv = [] #discrete feature: is index nazi abv\n",
    "        self.idx2phrase = []\n",
    "        self.idx2german = []\n",
    "        self.idx2group = []\n",
    "        self.pretrain_vec = [] # should match index order of words in dict.\n",
    "        \n",
    "        self.abvs = []\n",
    "        self.phrases = [] #only 1-word phrases + echo\n",
    "        self.german = [] #only 1-word german \n",
    "        self.groups = [] #only 1-word groups\n",
    "        \n",
    "        self.load_nazi_features()\n",
    "    \n",
    "    def load_nazi_features(self):\n",
    "        with open('abbreviations.txt', 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                words = line.split()\n",
    "                #print(words[0].lower())\n",
    "                self.abvs.append(words[0].lower())\n",
    "        self.phrases.append('(((')\n",
    "        self.phrases.append(')))')\n",
    "        '''\n",
    "        with open('phrases.txt', 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                words = line.split()\n",
    "                if len(words)==1:\n",
    "                    self.phrases.append(words[0].lower())\n",
    "        \n",
    "        with open('german.txt', 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                words = line.split()\n",
    "                if len(words)==1:\n",
    "                    self.german.append(words[0].lower())\n",
    "        with open('groups.txt', 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                words = line.split()\n",
    "                if len(words)==1:\n",
    "                    self.groups.append(words[0].lower())\n",
    "        '''\n",
    "    \n",
    "    #just abbreviations for now\n",
    "    #cause finding chunks means keeping track of last 6 words\n",
    "    def find_nazi_features(self,word):#create lookup for discrete feature\n",
    "        if word.lower() in self.abvs:\n",
    "            #print(\"ding\",word.lower())\n",
    "            self.idx2abv.append([0.1]) #yes\n",
    "        else:\n",
    "            self.idx2abv.append([0]) #no\n",
    "            \n",
    "        #For subphrases\n",
    "        found = False\n",
    "        for i in self.phrases:\n",
    "            if i in word.lower():\n",
    "                #print(\"ding\",word.lower())\n",
    "                found = True\n",
    "                break\n",
    "                \n",
    "        if found:\n",
    "            self.idx2phrase.append([0.1])\n",
    "        else:\n",
    "            self.idx2phrase.append([0])\n",
    "            \n",
    "        '''\n",
    "        #For subphrases\n",
    "        found = False\n",
    "        for i in self.german:\n",
    "            if i in word.lower():\n",
    "                #print(\"ding\",word.lower())\n",
    "                found = True\n",
    "                break\n",
    "                \n",
    "        if found:\n",
    "            self.idx2german.append([1])\n",
    "        else:\n",
    "            self.idx2german.append([0])\n",
    "            \n",
    "        #For subphrases\n",
    "        found = False\n",
    "        for i in self.groups:\n",
    "            if i in word.lower():\n",
    "                #print(\"ding\",word.lower())\n",
    "                found = True\n",
    "                break\n",
    "                \n",
    "        if found:\n",
    "            self.idx2group.append([1])\n",
    "        else:\n",
    "            self.idx2group.append([0])\n",
    "        '''\n",
    "            \n",
    "        \n",
    "    def add_word(self, word, ahead_six=None, vec=None):\n",
    "        if vec is None:\n",
    "            if word not in self.word2idx:\n",
    "                self.idx2word.append(word)\n",
    "                self.word2idx[word] = len(self.idx2word) - 1\n",
    "                self.find_nazi_features(word)\n",
    "        else:\n",
    "            if word not in self.word2idx:\n",
    "                self.pretrain_vec.append(vec)\n",
    "                self.idx2word.append(word)\n",
    "                self.word2idx[word] = len(self.idx2word) - 1\n",
    "                self.find_nazi_features(word)\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, path, language):\n",
    "        self.dictionary = Dictionary()\n",
    "        if language is not None:\n",
    "            self.pretrained = self.add_pretrained(os.path.join('', 'wiki.' + language + '.vec'))\n",
    "        #self.trainid, self.trainlab, self.trainidx = self.tokenize_by_user(os.path.join(path, 'train.csv'),True)\n",
    "        #self.validid, self.validlab, self.valididx = self.tokenize_by_user(os.path.join(path, 'valid.csv'),False)\n",
    "        #self.testid, self.testlab, self.testidx = self.tokenize_by_user(os.path.join(path, 'test.csv'),False)\n",
    "        self.X_train, self.y_train, self.pic_train, self.feat_train = self.tokenize(os.path.join('', 'train.csv'),True)\n",
    "        #self.X_valid, self.y_valid = self.tokenize(os.path.join(path, 'valid.csv'),False)\n",
    "        self.X_test, self.y_test, self.pic_test, self.feat_test = self.tokenize(os.path.join('', 'test.csv'),False)\n",
    "        \n",
    "\n",
    "    def add_pretrained(self, path):\n",
    "        assert os.path.exists(path)\n",
    "\n",
    "        # Add words with pretrained vectors to the dictionary\n",
    "        # might be weird because no eos was added?\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                words = line.split()\n",
    "                if len(words) == 2: #first line\n",
    "                    continue\n",
    "                word = words[0]\n",
    "                vec = words[1:]\n",
    "                if len(vec) != 300:\n",
    "                    continue #this skips the space embedding\n",
    "                #vec = np.array(list(map(float, vec)))\n",
    "                vec = list(map(float,vec))\n",
    "                tokens += 1\n",
    "                \n",
    "                self.dictionary.add_word(word, vec)\n",
    "    def tokenize(self, path, header):\n",
    "        \"\"\"Tokenizes a text file.\"\"\"\n",
    "        assert os.path.exists(path)\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.reader(f)\n",
    "            tokens = 0\n",
    "            prev = None\n",
    "            if header:\n",
    "                first = True\n",
    "            else:\n",
    "                first = False\n",
    "            tweet_count = 0\n",
    "            user_idx = -1\n",
    "            for row in reader:\n",
    "                if first:\n",
    "                    first = False\n",
    "                    continue\n",
    "                if len(row) is not 6:\n",
    "                    continue\n",
    "                \n",
    "                tweet = row[0]\n",
    "                label = row[1]\n",
    "                if not label.isdigit():\n",
    "                    continue\n",
    "                extra = row[2:5] #bio, tweet pic, profile pic, user id\n",
    "                if row[2] != prev: #new user\n",
    "                    prev = row[2]\n",
    "                    tweet_count = 0\n",
    "                    user_idx += 1\n",
    "                    \n",
    "                    \n",
    "                    words = row[2].split() #add bio\n",
    "                    for word in words:\n",
    "                        self.dictionary.add_word(word)\n",
    "\n",
    "                words = tweet.split()\n",
    "                tokens += len(words)\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "\n",
    "        # Tokenize file content\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            x = np.zeros(user_idx+1,dtype='object')\n",
    "            y = np.zeros(user_idx+1,dtype='int')\n",
    "            z = np.zeros((user_idx+1,512),dtype='float')\n",
    "            q = np.zeros((user_idx+1,2),dtype='float') #does the user ever have any nazi words?\n",
    "            #ids = torch.LongTensor(tokens)\n",
    "            #idxs = torch.LongTensor(user_idx+1)\n",
    "            #labels = torch.LongTensor(user_idx+1)\n",
    "            #print(user_idx+1)\n",
    "            token = 0\n",
    "            prev = None\n",
    "\n",
    "            reader = csv.reader(f)\n",
    "            if header:\n",
    "                first = True\n",
    "            else:\n",
    "                first = False\n",
    "            user_idx = -1\n",
    "            for row in reader:\n",
    "                if first:\n",
    "                    first = False\n",
    "                    continue\n",
    "                if len(row) is not 6:\n",
    "                    continue\n",
    "                \n",
    "                tweet = row[0]\n",
    "                label = row[1]\n",
    "                if not label.isdigit():\n",
    "                    continue\n",
    "                extra = row[2:5] #bio, tweet pic, profile pic, user id\n",
    "                if row[2] != prev:\n",
    "                    print(user_idx,\"user\")\n",
    "                    tweet_idx = -1\n",
    "                    user_idx += 1\n",
    "                    prev = row[2]\n",
    "                    y[user_idx] = int(label)\n",
    "                    x[user_idx] = []\n",
    "                    #print(token, \"NEW USER\")\n",
    "                    #idxs[user_idx] = token\n",
    "                    x[user_idx].append([])\n",
    "                    \n",
    "                    pic_url = row[4]\n",
    "                    if len(pic_url) >4: #http:\n",
    "                        response = requests.get(pic_url)\n",
    "                        if(response.status_code != 200):\n",
    "                            #print(response.status_code)\n",
    "                            z[user_idx] = np.zeros(512)\n",
    "                        else:\n",
    "\n",
    "                            img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "                            #print(img.size)\n",
    "                            vec = img2vec.get_vec(img, tensor=False)\n",
    "                            z[user_idx] = vec\n",
    "                    else:\n",
    "                        z[user_idx] = np.zeros(512)\n",
    "                    \n",
    "                    words = row[2].split() #take the bio as the 'first tweet'\n",
    "                    tweet_idx+=1\n",
    "                    for word in words:\n",
    "                        x[user_idx][0].append(self.dictionary.word2idx[word])\n",
    "                \n",
    "\n",
    "                words = tweet.split()\n",
    "                token = 0\n",
    "                tweet_idx+=1\n",
    "                if tweet_idx >=21:\n",
    "                    #print(tweet_idx)\n",
    "                    continue\n",
    "                x[user_idx].append([])\n",
    "                for word in words:\n",
    "                    idx = self.dictionary.word2idx[word]\n",
    "                    if self.dictionary.idx2abv[idx] == 0.1:\n",
    "                        q[user_idx][0] = 0.1\n",
    "                    if self.dictionary.idx2phrase[idx] == 0.1:\n",
    "                        q[user_idx][1] = 0.1\n",
    "                    x[user_idx][tweet_idx].append(idx)\n",
    "                    token+=1\n",
    "                \n",
    "\n",
    "        return x, y, q, q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#attention functions\n",
    "\n",
    "def batch_matmul_bias(seq, weight, bias, nonlinearity=''):\n",
    "    s = None\n",
    "    bias_dim = bias.size()\n",
    "    for i in range(seq.size(0)):\n",
    "        _s = torch.mm(seq[i], weight) \n",
    "        _s_bias = _s + bias.expand(bias_dim[0], _s.size()[0]).transpose(0,1)\n",
    "        if(nonlinearity=='tanh'):\n",
    "            _s_bias = torch.tanh(_s_bias)\n",
    "        _s_bias = _s_bias.unsqueeze(0)\n",
    "        if(s is None):\n",
    "            s = _s_bias\n",
    "        else:\n",
    "            s = torch.cat((s,_s_bias),0)\n",
    "    return s.squeeze()\n",
    "\n",
    "def batch_matmul(seq, weight, nonlinearity=''):\n",
    "    s = None\n",
    "    for i in range(seq.size(0)):\n",
    "        _s = torch.mm(seq[i], weight)\n",
    "        if(nonlinearity=='tanh'):\n",
    "            _s = torch.tanh(_s)\n",
    "        _s = _s.unsqueeze(0)\n",
    "        if(s is None):\n",
    "            s = _s\n",
    "        else:\n",
    "            s = torch.cat((s,_s),0)\n",
    "    return s.squeeze()\n",
    "\n",
    "def attention_mul(rnn_outputs, att_weights):\n",
    "    attn_vectors = None\n",
    "    for i in range(rnn_outputs.size(0)):\n",
    "        h_i = rnn_outputs[i]\n",
    "        a_i = att_weights[i].unsqueeze(1).expand_as(h_i)\n",
    "        h_i = a_i * h_i\n",
    "        h_i = h_i.unsqueeze(0)\n",
    "        if(attn_vectors is None):\n",
    "            attn_vectors = h_i\n",
    "        else:\n",
    "            attn_vectors = torch.cat((attn_vectors,h_i),0)\n",
    "    return torch.sum(attn_vectors, 0).unsqueeze(0)\n",
    "\n",
    "'''\n",
    "def batch_matmul_bias(seq, weight, bias, nonlinearity=''):\n",
    "    s = None\n",
    "    bias_dim = bias.size()\n",
    "    for i in range(seq.size(0)):\n",
    "        _s = torch.mm(seq[i], weight) \n",
    "        _s_bias = _s + bias.expand(bias_dim[0], _s.size()[0]).transpose(0,1)\n",
    "        if(nonlinearity=='tanh'):\n",
    "            _s_bias = torch.tanh(_s_bias)\n",
    "        _s_bias = _s_bias.unsqueeze(0)\n",
    "        if(s is None):\n",
    "            s = _s_bias\n",
    "        else:\n",
    "            s = torch.cat((s,_s_bias),0)\n",
    "    return s.squeeze()\n",
    "    \n",
    "def batch_matmul(seq, weight, nonlinearity=''):\n",
    "    s = None\n",
    "    for i in range(seq.size(0)):\n",
    "        _s = torch.mm(seq[i], weight)\n",
    "        if(nonlinearity=='tanh'):\n",
    "            _s = torch.tanh(_s)\n",
    "        _s = _s.unsqueeze(0)\n",
    "        if(s is None):\n",
    "            s = _s\n",
    "        else:\n",
    "            s = torch.cat((s,_s),0)\n",
    "    return s.squeeze()\n",
    "\n",
    "def attention_mul(self, rnn_outputs, att_weights):\n",
    "    attn_vectors = None\n",
    "    for i in range(rnn_outputs.size(0)):\n",
    "        h_i = rnn_outputs[i]\n",
    "        a_i = att_weights[i].unsqueeze(1).expand_as(h_i)\n",
    "        h_i = a_i * h_i\n",
    "        h_i = h_i.unsqueeze(0)\n",
    "        if(attn_vectors is None):\n",
    "            attn_vectors = h_i\n",
    "        else:\n",
    "            attn_vectors = torch.cat((attn_vectors,h_i),0)\n",
    "    return torch.sum(attn_vectors, 0).unsqueeze(0)\n",
    "'''\n",
    "class AttentionWordRNN(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self, embeds, abvs, phrases, german, groups, batch_size,\n",
    "                 num_tokens, embed_size, word_gru_hidden, dropout, bidirectional= True):        \n",
    "        \n",
    "        super(AttentionWordRNN, self).__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.num_tokens = num_tokens\n",
    "        self.embed_size = embed_size #add abv feature size on\n",
    "        self.word_gru_hidden = word_gru_hidden\n",
    "        self.bidirectional = bidirectional\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "        \n",
    "        \n",
    "        self.lookup = nn.Embedding(num_tokens, 300)\n",
    "        self.lookup_abv = nn.Embedding(num_tokens, 1)\n",
    "        self.lookup_phr = nn.Embedding(num_tokens, 1)\n",
    "        #self.lookup_ger = nn.Embedding(num_tokens, 1)\n",
    "        #self.lookup_gro = nn.Embedding(num_tokens, 1)\n",
    "        #self.abvs = abvs #doesn't need gradient, it's static/discrete\n",
    "\n",
    "        #init lookup table\n",
    "        #print(len(embeds),embeds[0])\n",
    "        #print(len(abvs),abvs[0])\n",
    "\n",
    "        \n",
    "        initrange = 0.1\n",
    "\n",
    "        k = len(embeds) # the first k indices are pretrained. the rest are unknown\n",
    "        \n",
    "        if k is not 0:\n",
    "            first = np.array(embeds)\n",
    "            second = np.random.uniform(-initrange,initrange,size=(num_tokens-k,300))\n",
    "            self.lookup.weight.data.copy_(torch.from_numpy(np.concatenate((first,second),axis=0)))\n",
    "        else:\n",
    "            self.lookup.weight.data.uniform_(-initrange, initrange)\n",
    "        self.lookup_abv.weight.requires_grad=False\n",
    "        self.lookup_abv.weight.data.copy_(torch.from_numpy(np.array(abvs)))\n",
    "        self.lookup_phr.weight.requires_grad=False\n",
    "        self.lookup_phr.weight.data.copy_(torch.from_numpy(np.array(phrases)))\n",
    "        #self.lookup_ger.weight.requires_grad=False\n",
    "        #self.lookup_ger.weight.data.copy_(torch.from_numpy(np.array(german)))\n",
    "        #self.lookup_gro.weight.requires_grad=False\n",
    "        #self.lookup_gro.weight.data.copy_(torch.from_numpy(np.array(groups)))\n",
    "        \n",
    "\n",
    "        if bidirectional == True:\n",
    "            self.word_gru = nn.GRU(embed_size, word_gru_hidden, bidirectional= True)\n",
    "            self.weight_W_word = nn.Parameter(torch.Tensor(2* word_gru_hidden,2*word_gru_hidden))\n",
    "            self.bias_word = nn.Parameter(torch.Tensor(2* word_gru_hidden,1))\n",
    "            self.weight_proj_word = nn.Parameter(torch.Tensor(2*word_gru_hidden, 1))\n",
    "        else:\n",
    "            self.word_gru = nn.GRU(embed_size, word_gru_hidden, bidirectional= False)\n",
    "            self.weight_W_word = nn.Parameter(torch.Tensor(word_gru_hidden, word_gru_hidden))\n",
    "            self.bias_word = nn.Parameter(torch.Tensor(word_gru_hidden,1))\n",
    "            self.weight_proj_word = nn.Parameter(torch.Tensor(word_gru_hidden, 1))\n",
    "            \n",
    "        self.softmax_word = nn.Softmax()\n",
    "        #self.word_gru.data.uniform_(-initrange,initrange)\n",
    "        self.weight_W_word.data.uniform_(-initrange, initrange)\n",
    "        self.weight_proj_word.data.uniform_(-initrange,initrange)\n",
    "        self.bias_word.data.uniform_(-initrange,initrange)\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, embed, state_word):\n",
    "        # embeddings\n",
    "        #print(embed)\n",
    "        embedded = self.drop(self.lookup(embed))\n",
    "        abv_feature = self.lookup_abv(embed)\n",
    "        phr_feature = self.lookup_phr(embed)\n",
    "        #ger_feature = self.lookup_ger(embed)\n",
    "        #gro_feature = self.lookup_gro(embed)\n",
    "        #print(len(embedded), len(abv_feature))\n",
    "        #embedded = torch.cat((embedded,abv_feature,phr_feature,ger_feature,gro_feature),dim=2)\n",
    "        embedded = torch.cat((embedded,abv_feature,phr_feature),dim=2)\n",
    "        # word level gru\n",
    "        #state_word = self.drop(state_word) #idk\n",
    "        output_word, state_word = self.word_gru(embedded, state_word)\n",
    "        state_word = self.drop(state_word) #idk\n",
    "        output_word = self.drop(output_word)\n",
    "#         print output_word.size()\n",
    "        word_squish = self.drop(batch_matmul_bias(output_word, self.weight_W_word,self.bias_word, nonlinearity='tanh'))\n",
    "        word_attn = self.drop(batch_matmul(word_squish, self.weight_proj_word))\n",
    "        word_attn_norm = self.drop(self.softmax_word(word_attn.transpose(1,0)))\n",
    "        word_attn_vectors = self.drop(attention_mul(output_word, word_attn_norm.transpose(1,0)))\n",
    "        return word_attn_vectors, state_word, word_attn_norm\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        if self.bidirectional == True:\n",
    "            return Variable(torch.zeros(2, self.batch_size, self.word_gru_hidden))\n",
    "        else:\n",
    "            return Variable(torch.zeros(1, self.batch_size, self.word_gru_hidden))\n",
    "\n",
    "class AttentionSentRNN(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self, batch_size, sent_gru_hidden, word_gru_hidden, n_classes, dropout, bidirectional= True):        \n",
    "        \n",
    "        p = 2\n",
    "        super(AttentionSentRNN, self).__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.sent_gru_hidden = sent_gru_hidden\n",
    "        self.n_classes = n_classes\n",
    "        self.word_gru_hidden = word_gru_hidden\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "        initrange = 0.1\n",
    "        \n",
    "        \n",
    "        if bidirectional == True:\n",
    "            self.sent_gru = nn.GRU(2 * word_gru_hidden, sent_gru_hidden, bidirectional= True)        \n",
    "            self.weight_W_sent = nn.Parameter(torch.Tensor(2* sent_gru_hidden+p ,2* sent_gru_hidden+p))\n",
    "            self.bias_sent = nn.Parameter(torch.Tensor(2* sent_gru_hidden+p,1))\n",
    "            self.weight_proj_sent = nn.Parameter(torch.Tensor(2* sent_gru_hidden+p, 1))\n",
    "            self.final_linear = nn.Linear(2* sent_gru_hidden+p, n_classes)\n",
    "        else:\n",
    "            self.sent_gru = nn.GRU(word_gru_hidden, sent_gru_hidden, bidirectional= True)        \n",
    "            self.weight_W_sent = nn.Parameter(torch.Tensor(sent_gru_hidden+p ,sent_gru_hidden+p))\n",
    "            self.bias_sent = nn.Parameter(torch.Tensor(sent_gru_hidden+p,1))\n",
    "            self.weight_proj_sent = nn.Parameter(torch.Tensor(sent_gru_hidden+p, 1))\n",
    "            self.final_linear = nn.Linear(sent_gru_hidden+p, n_classes)\n",
    "        self.softmax_sent = nn.Softmax()\n",
    "        self.final_softmax = nn.Softmax()\n",
    "        self.bias_sent.data.uniform_(-initrange, initrange)\n",
    "        #self.sent_gru.data.uniform_(-initrange,initrange)\n",
    "        self.weight_W_sent.data.uniform_(-initrange, initrange)\n",
    "        self.weight_proj_sent.data.uniform_(-initrange,initrange)\n",
    "        \n",
    "        \n",
    "    def forward(self, word_attention_vectors, state_sent,pics):\n",
    "        #pics is a a batchxvec dump of prof image vectors\n",
    "        #MANUALLY DROPOUT THE GRU\n",
    "        #state_word = self.drop(state_sent)\n",
    "        pics = pics.unsqueeze(0)\n",
    "        pics = pics.repeat(word_attention_vectors.size(0),1,1)\n",
    "        \n",
    "        #word_attention_vectors = torch.cat((word_attention_vectors,pics.unsqueeze(0)),dim=0)\n",
    "        output_sent, state_sent = self.sent_gru(word_attention_vectors, state_sent) \n",
    "        #when it comes out of the gru, concatenate it.\n",
    "        state_word = self.drop(state_sent)\n",
    "        output_sent = self.drop(output_sent)\n",
    "        #print(output_sent.size(), pics.size())\n",
    "        output_sent = torch.cat((output_sent,pics),dim=2)\n",
    "        sent_squish = self.drop(batch_matmul_bias(output_sent, self.weight_W_sent,self.bias_sent, nonlinearity='tanh'))\n",
    "        sent_attn = self.drop(batch_matmul(sent_squish, self.weight_proj_sent))\n",
    "        sent_attn_norm = self.drop(self.softmax_sent(sent_attn.transpose(1,0)))\n",
    "        sent_attn_vectors = self.drop(attention_mul(output_sent, sent_attn_norm.transpose(1,0)))    \n",
    "        # final classifier\n",
    "        final_map = self.final_linear(sent_attn_vectors.squeeze(0))\n",
    "        return F.log_softmax(final_map), state_sent, sent_attn_norm\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        if self.bidirectional == True:\n",
    "            return Variable(torch.zeros(2, self.batch_size, self.sent_gru_hidden))\n",
    "        else:\n",
    "            return Variable(torch.zeros(1, self.batch_size, self.sent_gru_hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1 user\n",
      "0 user\n",
      "1 user\n",
      "2 user\n",
      "3 user\n",
      "4 user\n",
      "5 user\n",
      "6 user\n",
      "7 user\n",
      "8 user\n",
      "9 user\n",
      "10 user\n",
      "11 user\n",
      "12 user\n",
      "13 user\n",
      "14 user\n",
      "15 user\n",
      "16 user\n",
      "17 user\n",
      "18 user\n",
      "19 user\n",
      "20 user\n",
      "21 user\n",
      "22 user\n",
      "23 user\n",
      "24 user\n",
      "25 user\n",
      "26 user\n",
      "27 user\n",
      "28 user\n",
      "29 user\n",
      "30 user\n",
      "31 user\n",
      "32 user\n",
      "33 user\n",
      "34 user\n",
      "35 user\n",
      "36 user\n",
      "37 user\n",
      "38 user\n",
      "39 user\n",
      "40 user\n",
      "41 user\n",
      "42 user\n",
      "43 user\n",
      "44 user\n",
      "45 user\n",
      "46 user\n",
      "47 user\n",
      "48 user\n",
      "49 user\n",
      "50 user\n",
      "51 user\n",
      "52 user\n",
      "53 user\n",
      "54 user\n",
      "55 user\n",
      "56 user\n",
      "57 user\n",
      "58 user\n",
      "59 user\n",
      "60 user\n",
      "61 user\n",
      "62 user\n",
      "63 user\n",
      "64 user\n",
      "65 user\n",
      "66 user\n",
      "67 user\n",
      "68 user\n",
      "69 user\n",
      "70 user\n",
      "71 user\n",
      "72 user\n",
      "73 user\n",
      "74 user\n",
      "75 user\n",
      "76 user\n",
      "77 user\n",
      "78 user\n",
      "79 user\n",
      "80 user\n",
      "81 user\n",
      "82 user\n",
      "83 user\n",
      "84 user\n",
      "85 user\n",
      "86 user\n",
      "87 user\n",
      "88 user\n",
      "89 user\n",
      "90 user\n",
      "91 user\n",
      "92 user\n",
      "93 user\n",
      "94 user\n",
      "95 user\n",
      "96 user\n",
      "97 user\n",
      "98 user\n",
      "99 user\n",
      "100 user\n",
      "101 user\n",
      "102 user\n",
      "103 user\n",
      "104 user\n",
      "105 user\n",
      "106 user\n",
      "107 user\n",
      "108 user\n",
      "109 user\n",
      "110 user\n",
      "111 user\n",
      "112 user\n",
      "113 user\n",
      "114 user\n",
      "115 user\n",
      "116 user\n",
      "117 user\n",
      "118 user\n",
      "119 user\n",
      "120 user\n",
      "121 user\n",
      "122 user\n",
      "123 user\n",
      "124 user\n",
      "125 user\n",
      "126 user\n",
      "127 user\n",
      "128 user\n",
      "129 user\n",
      "130 user\n",
      "131 user\n",
      "132 user\n",
      "133 user\n",
      "134 user\n",
      "135 user\n",
      "136 user\n",
      "137 user\n",
      "138 user\n",
      "139 user\n",
      "140 user\n",
      "141 user\n",
      "142 user\n",
      "143 user\n",
      "144 user\n",
      "145 user\n",
      "146 user\n",
      "147 user\n",
      "148 user\n",
      "149 user\n",
      "150 user\n",
      "151 user\n",
      "152 user\n",
      "153 user\n",
      "154 user\n",
      "155 user\n",
      "156 user\n",
      "157 user\n",
      "158 user\n",
      "159 user\n",
      "160 user\n",
      "161 user\n",
      "162 user\n",
      "163 user\n",
      "164 user\n",
      "165 user\n",
      "166 user\n",
      "167 user\n",
      "168 user\n",
      "169 user\n",
      "170 user\n",
      "171 user\n",
      "172 user\n",
      "173 user\n",
      "174 user\n",
      "175 user\n",
      "176 user\n",
      "177 user\n",
      "178 user\n",
      "179 user\n",
      "180 user\n",
      "181 user\n",
      "182 user\n",
      "183 user\n",
      "184 user\n",
      "185 user\n",
      "186 user\n",
      "187 user\n",
      "188 user\n",
      "189 user\n",
      "190 user\n",
      "191 user\n",
      "192 user\n",
      "193 user\n",
      "194 user\n",
      "195 user\n",
      "196 user\n",
      "197 user\n",
      "198 user\n",
      "199 user\n",
      "200 user\n",
      "201 user\n",
      "202 user\n",
      "203 user\n",
      "204 user\n",
      "205 user\n",
      "206 user\n",
      "207 user\n",
      "208 user\n",
      "209 user\n",
      "210 user\n",
      "211 user\n",
      "212 user\n",
      "213 user\n",
      "214 user\n",
      "215 user\n",
      "216 user\n",
      "217 user\n",
      "218 user\n",
      "219 user\n",
      "220 user\n",
      "221 user\n",
      "222 user\n",
      "223 user\n",
      "224 user\n",
      "225 user\n",
      "226 user\n",
      "227 user\n",
      "228 user\n",
      "229 user\n",
      "230 user\n",
      "231 user\n",
      "232 user\n",
      "233 user\n",
      "234 user\n",
      "235 user\n",
      "236 user\n",
      "237 user\n",
      "238 user\n",
      "239 user\n",
      "240 user\n",
      "241 user\n",
      "242 user\n",
      "243 user\n",
      "244 user\n",
      "245 user\n",
      "246 user\n",
      "247 user\n",
      "248 user\n",
      "249 user\n",
      "250 user\n",
      "251 user\n",
      "252 user\n",
      "253 user\n",
      "254 user\n",
      "255 user\n",
      "256 user\n",
      "257 user\n",
      "258 user\n",
      "259 user\n",
      "260 user\n",
      "261 user\n",
      "262 user\n",
      "263 user\n",
      "264 user\n",
      "265 user\n",
      "266 user\n",
      "267 user\n",
      "268 user\n",
      "269 user\n",
      "270 user\n",
      "271 user\n",
      "272 user\n",
      "273 user\n",
      "274 user\n",
      "275 user\n",
      "276 user\n",
      "277 user\n",
      "278 user\n",
      "279 user\n",
      "280 user\n",
      "281 user\n",
      "282 user\n",
      "283 user\n",
      "284 user\n",
      "285 user\n",
      "286 user\n",
      "287 user\n",
      "288 user\n",
      "289 user\n",
      "290 user\n",
      "291 user\n",
      "292 user\n",
      "293 user\n",
      "294 user\n",
      "295 user\n",
      "296 user\n",
      "297 user\n",
      "298 user\n",
      "299 user\n",
      "300 user\n",
      "301 user\n",
      "302 user\n",
      "303 user\n",
      "304 user\n",
      "305 user\n",
      "306 user\n",
      "307 user\n",
      "308 user\n",
      "309 user\n",
      "310 user\n",
      "311 user\n",
      "312 user\n",
      "313 user\n",
      "314 user\n",
      "315 user\n",
      "316 user\n",
      "317 user\n",
      "318 user\n",
      "319 user\n",
      "320 user\n",
      "321 user\n",
      "322 user\n",
      "323 user\n",
      "324 user\n",
      "325 user\n",
      "326 user\n",
      "327 user\n",
      "328 user\n",
      "329 user\n",
      "330 user\n",
      "331 user\n",
      "332 user\n",
      "333 user\n",
      "334 user\n",
      "335 user\n",
      "336 user\n",
      "337 user\n",
      "338 user\n",
      "339 user\n",
      "340 user\n",
      "341 user\n",
      "342 user\n",
      "343 user\n",
      "344 user\n",
      "345 user\n",
      "346 user\n",
      "347 user\n",
      "348 user\n",
      "349 user\n",
      "350 user\n",
      "351 user\n",
      "352 user\n",
      "353 user\n",
      "354 user\n",
      "355 user\n",
      "356 user\n",
      "357 user\n",
      "358 user\n",
      "359 user\n",
      "360 user\n",
      "361 user\n",
      "362 user\n",
      "363 user\n",
      "364 user\n",
      "365 user\n",
      "366 user\n",
      "367 user\n",
      "368 user\n",
      "369 user\n",
      "370 user\n",
      "371 user\n",
      "-1 user\n",
      "0 user\n",
      "1 user\n",
      "2 user\n",
      "3 user\n",
      "4 user\n",
      "5 user\n",
      "6 user\n",
      "7 user\n",
      "8 user\n",
      "9 user\n",
      "10 user\n",
      "11 user\n",
      "12 user\n",
      "13 user\n",
      "14 user\n",
      "15 user\n",
      "16 user\n",
      "17 user\n",
      "18 user\n",
      "19 user\n",
      "20 user\n",
      "21 user\n",
      "22 user\n",
      "23 user\n",
      "24 user\n",
      "25 user\n",
      "26 user\n",
      "27 user\n",
      "28 user\n",
      "29 user\n",
      "30 user\n",
      "31 user\n",
      "32 user\n",
      "33 user\n",
      "34 user\n",
      "35 user\n",
      "36 user\n",
      "37 user\n",
      "38 user\n",
      "39 user\n",
      "40 user\n",
      "41 user\n",
      "42 user\n",
      "43 user\n",
      "44 user\n",
      "45 user\n",
      "46 user\n",
      "47 user\n",
      "48 user\n",
      "49 user\n",
      "50 user\n",
      "51 user\n",
      "52 user\n",
      "53 user\n",
      "54 user\n",
      "55 user\n",
      "56 user\n",
      "57 user\n",
      "58 user\n",
      "59 user\n",
      "60 user\n",
      "61 user\n",
      "62 user\n",
      "63 user\n",
      "64 user\n",
      "65 user\n",
      "66 user\n",
      "67 user\n",
      "68 user\n",
      "69 user\n",
      "70 user\n",
      "71 user\n",
      "72 user\n",
      "73 user\n",
      "74 user\n",
      "75 user\n",
      "76 user\n",
      "77 user\n",
      "78 user\n",
      "79 user\n",
      "80 user\n",
      "81 user\n",
      "82 user\n",
      "83 user\n",
      "84 user\n",
      "85 user\n",
      "86 user\n",
      "87 user\n",
      "88 user\n",
      "89 user\n",
      "90 user\n",
      "91 user\n",
      "92 user\n",
      "93 user\n",
      "94 user\n",
      "95 user\n",
      "96 user\n",
      "97 user\n",
      "98 user\n",
      "99 user\n",
      "100 user\n",
      "101 user\n",
      "102 user\n",
      "103 user\n",
      "104 user\n",
      "105 user\n",
      "106 user\n",
      "107 user\n",
      "108 user\n",
      "109 user\n",
      "110 user\n",
      "111 user\n",
      "112 user\n",
      "113 user\n",
      "114 user\n",
      "115 user\n",
      "116 user\n",
      "117 user\n",
      "118 user\n",
      "119 user\n",
      "120 user\n",
      "121 user\n",
      "122 user\n",
      "123 user\n",
      "124 user\n",
      "125 user\n",
      "126 user\n",
      "127 user\n",
      "128 user\n",
      "129 user\n",
      "130 user\n",
      "131 user\n",
      "132 user\n",
      "133 user\n",
      "134 user\n",
      "135 user\n",
      "136 user\n",
      "137 user\n",
      "138 user\n",
      "139 user\n",
      "140 user\n",
      "141 user\n",
      "142 user\n",
      "143 user\n",
      "144 user\n",
      "145 user\n",
      "146 user\n",
      "147 user\n",
      "148 user\n",
      "149 user\n",
      "150 user\n",
      "151 user\n",
      "152 user\n",
      "153 user\n"
     ]
    }
   ],
   "source": [
    "#import model\n",
    "#import data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import sklearn.metrics\n",
    "\n",
    "dropout=0\n",
    "my_batch=64\n",
    "lang='en'\n",
    "datapath = './data/'+lang\n",
    "corpus = Corpus(datapath, lang)\n",
    "ntokens = len(corpus.dictionary)\n",
    "pretrain = corpus.dictionary.pretrain_vec\n",
    "idx2abv = corpus.dictionary.idx2abv\n",
    "idx2phrase = corpus.dictionary.idx2phrase\n",
    "#idx2german = corpus.dictionary.idx2german\n",
    "#idx2group = corpus.dictionary.idx2group\n",
    "\n",
    "word_attn = AttentionWordRNN(embeds=pretrain, abvs=idx2abv, phrases=idx2phrase, german=None, groups=None,\n",
    "                             batch_size=my_batch, num_tokens=ntokens, embed_size=300+2, \n",
    "                             word_gru_hidden=100, dropout=dropout, bidirectional= True)\n",
    "\n",
    "sent_attn = AttentionSentRNN(batch_size=my_batch, sent_gru_hidden=100, word_gru_hidden=100, \n",
    "                             n_classes=2, dropout=dropout, bidirectional= True)\n",
    "\n",
    "def train_data(mini_batch, targets, pics, word_attn_model, sent_attn_model, word_optimizer, sent_optimizer, criterion):\n",
    "    state_word = word_attn_model.init_hidden().cuda()\n",
    "    state_sent = sent_attn_model.init_hidden().cuda()\n",
    "    max_sents, batch_size, max_tokens = mini_batch.size()\n",
    "    word_optimizer.zero_grad()\n",
    "    sent_optimizer.zero_grad()\n",
    "    s = None\n",
    "    for i in range(max_sents):\n",
    "        #torch.cuda.empty_cache()\n",
    "        _s, state_word, _ = word_attn_model(mini_batch[i,:,:].transpose(0,1), state_word)\n",
    "        if(s is None):\n",
    "            s = _s\n",
    "        else:\n",
    "            s = torch.cat((s,_s),0)            \n",
    "    y_pred, state_sent, _ = sent_attn_model.forward(s, state_sent,pics)\n",
    "    loss = criterion(y_pred.cuda(), targets)\n",
    "\n",
    "    state_word = None\n",
    "    state_sent = None\n",
    "    max_sents = None\n",
    "    batch_size = None\n",
    "    max_tokens = None \n",
    "    mini_batch = None\n",
    "    torch.cuda.empty_cache()\n",
    "    loss.backward()\n",
    "    \n",
    "    word_optimizer.step()\n",
    "    sent_optimizer.step()\n",
    "    \n",
    "    return loss.data.item()\n",
    "\n",
    "\n",
    "\n",
    "def get_predictions(val_tokens, pics, word_attn_model, sent_attn_model):\n",
    "    max_sents, batch_size, max_tokens = val_tokens.size()\n",
    "    state_word = word_attn_model.init_hidden().cuda()\n",
    "    state_sent = sent_attn_model.init_hidden().cuda()\n",
    "    s = None\n",
    "    for i in range(max_sents):\n",
    "        _s, state_word, _ = word_attn_model(val_tokens[i,:,:].transpose(0,1), state_word)\n",
    "        if(s is None):\n",
    "            s = _s\n",
    "        else:\n",
    "            s = torch.cat((s,_s),0)            \n",
    "    y_pred, state_sent, _ = sent_attn_model.forward(s, state_sent, pics)    \n",
    "    return y_pred\n",
    "\n",
    "\n",
    "\n",
    "#learning_rate = 0.001\n",
    "#momentum = 0.9\n",
    "#word_optimizer = torch.optim.SGD(word_attn.parameters(), lr=learning_rate, momentum= momentum)\n",
    "#sent_optimizer = torch.optim.SGD(sent_attn.parameters(), lr=learning_rate, momentum= momentum)\n",
    "#word_optimizer = torch.optim.Adam(word_attn.parameters())\n",
    "word_optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, word_attn.parameters()))\n",
    "sent_optimizer = torch.optim.Adam(sent_attn.parameters())\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "word_attn.cuda()\n",
    "sent_attn.cuda()\n",
    "\n",
    "\n",
    "\n",
    "def pad_batch(mini_batch):\n",
    "    mini_batch_size = len(mini_batch)\n",
    "    max_sent_len = int(np.mean([len(x) for x in mini_batch]))\n",
    "    max_token_len = int(np.mean([len(val) for sublist in mini_batch for val in sublist]))\n",
    "    main_matrix = np.zeros((mini_batch_size, max_sent_len, max_token_len), dtype= np.int)\n",
    "    for i in range(main_matrix.shape[0]):\n",
    "        for j in range(main_matrix.shape[1]):\n",
    "            for k in range(main_matrix.shape[2]):\n",
    "                try:\n",
    "                    main_matrix[i,j,k] = mini_batch[i][j][k]\n",
    "                except IndexError:\n",
    "                    pass\n",
    "    #return Variable(torch.from_numpy(main_matrix).transpose(0,1))\n",
    "    return Variable(torch.LongTensor(main_matrix).transpose(0,1))\n",
    "\n",
    "\n",
    "\n",
    "def test_accuracy_mini_batch(tokens, labels, pics, word_attn, sent_attn):\n",
    "    y_pred = get_predictions(tokens, pics, word_attn, sent_attn)\n",
    "    _, y_pred = torch.max(y_pred, 1)\n",
    "    correct = np.ndarray.flatten(y_pred.data.cpu().numpy())\n",
    "    labels = np.ndarray.flatten(labels.data.cpu().numpy())\n",
    "    num_correct = sum(correct == labels)\n",
    "    return float(num_correct) / len(correct)\n",
    "\n",
    "def test_accuracy_full_batch(tokens, labels, pics, mini_batch_size, word_attn, sent_attn):\n",
    "    p = []\n",
    "    p_nonlinear = []\n",
    "    l = []\n",
    "    g = gen_minibatch(tokens, labels, pics, mini_batch_size)\n",
    "    for token, label, pic in g:\n",
    "        y_pred = get_predictions(token.cuda(), pic, word_attn, sent_attn)\n",
    "        #print(\"BEFORE\",y_pred)\n",
    "        p_nonlinear.append(np.ndarray.flatten(y_pred[:,1].data.cpu().numpy()))\n",
    "        _, y_pred = torch.max(y_pred, 1)\n",
    "        #print(\"AFTER\",y_pred)\n",
    "        p.append(np.ndarray.flatten(y_pred.data.cpu().numpy()))\n",
    "        l.append(np.ndarray.flatten(label.data.cpu().numpy()))\n",
    "    p = [item for sublist in p for item in sublist]\n",
    "    l = [item for sublist in l for item in sublist]\n",
    "    p_nonlinear = [np.exp(item) for sublist in p_nonlinear for item in sublist]\n",
    "    p = np.array(p)\n",
    "    l = np.array(l)\n",
    "    #print(\"TOKEN LEN\",len(tokens))\n",
    "    #print(\"NONLINEAR\",p_nonlinear)\n",
    "    #print(\"PREDICT\",p)\n",
    "    #print(\"LABEL\",l)\n",
    "    num_correct = sum(p == l)\n",
    "    return float(num_correct)/ len(p), sklearn.metrics.roc_auc_score(l, p_nonlinear)\n",
    "\n",
    "def test_data(mini_batch, targets, pics, word_attn_model, sent_attn_model):    \n",
    "    state_word = word_attn_model.init_hidden().cuda()\n",
    "    state_sent = sent_attn_model.init_hidden().cuda()\n",
    "    max_sents, batch_size, max_tokens = mini_batch.size()\n",
    "    s = None\n",
    "    for i in range(max_sents):\n",
    "        _s, state_word, _ = word_attn_model(mini_batch[i,:,:].transpose(0,1), state_word)\n",
    "        if(s is None):\n",
    "            s = _s\n",
    "        else:\n",
    "            s = torch.cat((s,_s),0)            \n",
    "    y_pred, state_sent,_ = sent_attn_model.forward(s, state_sent,pics)\n",
    "    loss = criterion(y_pred.cuda(), targets)     \n",
    "    return loss.data.item()\n",
    "\n",
    "def iterate_minibatches(inputs, targets, pics, batchsize, shuffle=False):\n",
    "    assert inputs.shape[0] == targets.shape[0]\n",
    "    if shuffle:\n",
    "        indices = np.arange(inputs.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "    #print(inputs.shape[0] - batchsize+1, batchsize, \"HOO\")\n",
    "    for start_idx in range(0, inputs.shape[0] - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt], pics[excerpt]\n",
    "\n",
    "\n",
    "\n",
    "def gen_minibatch(tokens, labels, pics, mini_batch_size, shuffle= True):\n",
    "    for token, label, pic in iterate_minibatches(tokens, labels, pics, mini_batch_size, shuffle= shuffle):\n",
    "        token = pad_batch(token)\n",
    "        #yield token.cuda(), Variable(torch.LongTensor(label), requires_grad= False).cuda(), pic\n",
    "        yield token.cuda(), Variable(torch.LongTensor(label), requires_grad= False).cuda(), Variable(torch.FloatTensor(np.array(pic)), requires_grad=False).cuda()\n",
    "\n",
    "def check_val_loss(val_tokens, val_labels, val_pics, mini_batch_size, word_attn_model, sent_attn_model):\n",
    "    val_loss = []\n",
    "    for token, label, pic in iterate_minibatches(val_tokens, val_labels, val_pics, mini_batch_size, shuffle= True):\n",
    "        val_loss.append(test_data(pad_batch(token).cuda(), Variable(torch.LongTensor(label), requires_grad= False).cuda(),\n",
    "                                  Variable(torch.FloatTensor(np.array(pic)).cuda(), requires_grad= False), \n",
    "                                  word_attn_model, sent_attn_model))\n",
    "    return np.mean(val_loss)\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def train_early_stopping(mini_batch_size, X_train, y_train, pic_train, X_test, y_test, pic_test, word_attn_model, sent_attn_model, \n",
    "                         word_attn_optimiser, sent_attn_optimiser, loss_criterion, num_epoch, \n",
    "                         print_val_loss_every = 1000, print_loss_every = 50):\n",
    "    #for i in word_attn_model.parameters():\n",
    "        #print(i.data, \"PARAM\")\n",
    "    max_eval_acc = 0\n",
    "    max_train_acc = 0\n",
    "    max_eval_aucroc = 0\n",
    "    max_train_aucroc = 0\n",
    "    word_attn_model.train()\n",
    "    sent_attn_model.train()\n",
    "    start = time.time()\n",
    "    loss_full = []\n",
    "    loss_epoch = []\n",
    "    accuracy_epoch = []\n",
    "    loss_smooth = []\n",
    "    accuracy_full = []\n",
    "    epoch_counter = 0\n",
    "    g = gen_minibatch(X_train, y_train, pic_train, mini_batch_size)\n",
    "    for i in range(1, num_epoch + 1):\n",
    "        try:\n",
    "            word_attn_model.train()\n",
    "            sent_attn_model.train()\n",
    "            tokens, labels, pics = next(g)\n",
    "            loss = train_data(tokens, labels, pics, word_attn_model, sent_attn_model, word_attn_optimiser, sent_attn_optimiser, loss_criterion)\n",
    "            acc = test_accuracy_mini_batch(tokens, labels, pics, word_attn_model, sent_attn_model)\n",
    "            accuracy_full.append(acc)\n",
    "            accuracy_epoch.append(acc)\n",
    "            loss_full.append(loss)\n",
    "            loss_epoch.append(loss)\n",
    "            # print loss every n passes\n",
    "            if i % print_loss_every == 0:\n",
    "                print('Loss at %d minibatches, %d epoch,(%s) is %f' %(i, epoch_counter, timeSince(start), np.mean(loss_epoch)))\n",
    "                print('Accuracy at %d minibatches is %f' % (i, np.mean(accuracy_epoch)))\n",
    "            # check validation loss every n passes\n",
    "            if i % print_val_loss_every == 0:\n",
    "                word_attn_model.eval()\n",
    "                sent_attn_model.eval()\n",
    "                val_loss = check_val_loss(X_test, y_test, pic_test, mini_batch_size, word_attn_model, sent_attn_model)\n",
    "                print('Average training loss at this epoch..minibatch..%d..is %f' % (i, np.mean(loss_epoch)))\n",
    "                print('Validation loss after %d passes is %f' %(i, val_loss))\n",
    "                if val_loss > np.mean(loss_full):\n",
    "                    print('Validation loss is higher than training loss at %d is %f , stopping training!' % (i, val_loss))\n",
    "                    print('Average training loss at %d is %f' % (i, np.mean(loss_full)))\n",
    "        except StopIteration:\n",
    "            epoch_counter += 1\n",
    "            print('Reached %d epochs' % epoch_counter)\n",
    "            print('i %d' % i)\n",
    "            word_attn_model.eval()\n",
    "            sent_attn_model.eval()\n",
    "            acc, aucroc = test_accuracy_full_batch(corpus.X_test, corpus.y_test, corpus.pic_test, my_batch, word_attn, sent_attn)\n",
    "            if acc>max_eval_acc:\n",
    "                max_eval_acc = acc\n",
    "            if aucroc>max_eval_aucroc:\n",
    "                max_eval_aucroc = aucroc\n",
    "            print(\"Test accuracy:\",acc)\n",
    "            print(\"Max test accruacy:\",max_eval_acc)\n",
    "            print(\"Test aucroc:\",aucroc)\n",
    "            print(\"Max test aucroc:\",max_eval_aucroc)\n",
    "            word_attn_model.train()\n",
    "            sent_attn_model.train()\n",
    "            tacc, taucroc = test_accuracy_full_batch(corpus.X_train, corpus.y_train, corpus.pic_train, my_batch, word_attn, sent_attn)\n",
    "            if tacc>max_train_acc:\n",
    "                max_train_acc = tacc\n",
    "            if taucroc>max_train_aucroc:\n",
    "                max_train_aucroc = taucroc\n",
    "            print(\"Train accuracy:\",tacc)\n",
    "            print(\"Max train accruacy:\",max_train_acc)\n",
    "            print(\"Train aucroc:\",taucroc)\n",
    "            print(\"Max train aucroc:\",max_train_aucroc)\n",
    "            #if epoch_counter == 1:\n",
    "                #break\n",
    "            g = gen_minibatch(X_train, y_train, pic_train, mini_batch_size)\n",
    "            loss_epoch = []\n",
    "            accuracy_epoch = []\n",
    "            if epoch_counter==20:\n",
    "                return acc, aucroc, max_eval_acc, max_eval_aucroc\n",
    "    return loss_full, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:182: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:247: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:251: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at 1 minibatches, 0 epoch,(0m 0s) is 0.682358\n",
      "Accuracy at 1 minibatches is 0.687500\n",
      "Average training loss at this epoch..minibatch..1..is 0.682358\n",
      "Validation loss after 1 passes is 0.639590\n",
      "Loss at 2 minibatches, 0 epoch,(0m 2s) is 0.663557\n",
      "Accuracy at 2 minibatches is 0.695312\n",
      "Average training loss at this epoch..minibatch..2..is 0.663557\n",
      "Validation loss after 2 passes is 0.628000\n",
      "Loss at 3 minibatches, 0 epoch,(0m 3s) is 0.646223\n",
      "Accuracy at 3 minibatches is 0.703125\n",
      "Average training loss at this epoch..minibatch..3..is 0.646223\n",
      "Validation loss after 3 passes is 0.593806\n",
      "Loss at 4 minibatches, 0 epoch,(0m 4s) is 0.636836\n",
      "Accuracy at 4 minibatches is 0.703125\n",
      "Average training loss at this epoch..minibatch..4..is 0.636836\n",
      "Validation loss after 4 passes is 0.609659\n",
      "Loss at 5 minibatches, 0 epoch,(0m 5s) is 0.623818\n",
      "Accuracy at 5 minibatches is 0.709375\n",
      "Average training loss at this epoch..minibatch..5..is 0.623818\n",
      "Validation loss after 5 passes is 0.577160\n",
      "Reached 1 epochs\n",
      "i 6\n",
      "Test accuracy: 0.71875\n",
      "Max test accruacy: 0.71875\n",
      "Test aucroc: 0.7907608695652174\n",
      "Max test aucroc: 0.7907608695652174\n",
      "Train accuracy: 0.7375\n",
      "Max train accruacy: 0.7375\n",
      "Train aucroc: 0.8417070217917675\n",
      "Max train aucroc: 0.8417070217917675\n",
      "Loss at 7 minibatches, 1 epoch,(0m 8s) is 0.516578\n",
      "Accuracy at 7 minibatches is 0.781250\n",
      "Average training loss at this epoch..minibatch..7..is 0.516578\n",
      "Validation loss after 7 passes is 0.590893\n",
      "Loss at 8 minibatches, 1 epoch,(0m 9s) is 0.542727\n",
      "Accuracy at 8 minibatches is 0.757812\n",
      "Average training loss at this epoch..minibatch..8..is 0.542727\n",
      "Validation loss after 8 passes is 0.594669\n",
      "Loss at 9 minibatches, 1 epoch,(0m 10s) is 0.558369\n",
      "Accuracy at 9 minibatches is 0.744792\n",
      "Average training loss at this epoch..minibatch..9..is 0.558369\n",
      "Validation loss after 9 passes is 0.563409\n",
      "Loss at 10 minibatches, 1 epoch,(0m 12s) is 0.576806\n",
      "Accuracy at 10 minibatches is 0.726562\n",
      "Average training loss at this epoch..minibatch..10..is 0.576806\n",
      "Validation loss after 10 passes is 0.540154\n",
      "Loss at 11 minibatches, 1 epoch,(0m 13s) is 0.566060\n",
      "Accuracy at 11 minibatches is 0.725000\n",
      "Average training loss at this epoch..minibatch..11..is 0.566060\n",
      "Validation loss after 11 passes is 0.559173\n",
      "Reached 2 epochs\n",
      "i 12\n",
      "Test accuracy: 0.7109375\n",
      "Max test accruacy: 0.71875\n",
      "Test aucroc: 0.8363528363528364\n",
      "Max test aucroc: 0.8363528363528364\n",
      "Train accuracy: 0.71875\n",
      "Max train accruacy: 0.7375\n",
      "Train aucroc: 0.8248309178743962\n",
      "Max train aucroc: 0.8417070217917675\n",
      "Loss at 13 minibatches, 2 epoch,(0m 15s) is 0.541836\n",
      "Accuracy at 13 minibatches is 0.718750\n",
      "Average training loss at this epoch..minibatch..13..is 0.541836\n",
      "Validation loss after 13 passes is 0.536915\n",
      "Loss at 14 minibatches, 2 epoch,(0m 17s) is 0.541779\n",
      "Accuracy at 14 minibatches is 0.710938\n",
      "Average training loss at this epoch..minibatch..14..is 0.541779\n",
      "Validation loss after 14 passes is 0.537312\n",
      "Loss at 15 minibatches, 2 epoch,(0m 18s) is 0.514770\n",
      "Accuracy at 15 minibatches is 0.744792\n",
      "Average training loss at this epoch..minibatch..15..is 0.514770\n",
      "Validation loss after 15 passes is 0.537058\n",
      "Loss at 16 minibatches, 2 epoch,(0m 19s) is 0.514075\n",
      "Accuracy at 16 minibatches is 0.746094\n",
      "Average training loss at this epoch..minibatch..16..is 0.514075\n",
      "Validation loss after 16 passes is 0.528393\n",
      "Loss at 17 minibatches, 2 epoch,(0m 20s) is 0.505437\n",
      "Accuracy at 17 minibatches is 0.765625\n",
      "Average training loss at this epoch..minibatch..17..is 0.505437\n",
      "Validation loss after 17 passes is 0.524143\n",
      "Reached 3 epochs\n",
      "i 18\n",
      "Test accuracy: 0.765625\n",
      "Max test accruacy: 0.765625\n",
      "Test aucroc: 0.8102168102168101\n",
      "Max test aucroc: 0.8363528363528364\n",
      "Train accuracy: 0.775\n",
      "Max train accruacy: 0.775\n",
      "Train aucroc: 0.832427536231884\n",
      "Max train aucroc: 0.8417070217917675\n",
      "Loss at 19 minibatches, 3 epoch,(0m 23s) is 0.518492\n",
      "Accuracy at 19 minibatches is 0.781250\n",
      "Average training loss at this epoch..minibatch..19..is 0.518492\n",
      "Validation loss after 19 passes is 0.483002\n",
      "Loss at 20 minibatches, 3 epoch,(0m 24s) is 0.475189\n",
      "Accuracy at 20 minibatches is 0.812500\n",
      "Average training loss at this epoch..minibatch..20..is 0.475189\n",
      "Validation loss after 20 passes is 0.471421\n",
      "Loss at 21 minibatches, 3 epoch,(0m 25s) is 0.463666\n",
      "Accuracy at 21 minibatches is 0.791667\n",
      "Average training loss at this epoch..minibatch..21..is 0.463666\n",
      "Validation loss after 21 passes is 0.463030\n",
      "Loss at 22 minibatches, 3 epoch,(0m 26s) is 0.422646\n",
      "Accuracy at 22 minibatches is 0.812500\n",
      "Average training loss at this epoch..minibatch..22..is 0.422646\n",
      "Validation loss after 22 passes is 0.449664\n",
      "Loss at 23 minibatches, 3 epoch,(0m 28s) is 0.413433\n",
      "Accuracy at 23 minibatches is 0.812500\n",
      "Average training loss at this epoch..minibatch..23..is 0.413433\n",
      "Validation loss after 23 passes is 0.482428\n",
      "Reached 4 epochs\n",
      "i 24\n",
      "Test accuracy: 0.796875\n",
      "Max test accruacy: 0.796875\n",
      "Test aucroc: 0.8792317708333334\n",
      "Max test aucroc: 0.8792317708333334\n",
      "Train accuracy: 0.81875\n",
      "Max train accruacy: 0.81875\n",
      "Train aucroc: 0.9321464561639234\n",
      "Max train aucroc: 0.9321464561639234\n",
      "Loss at 25 minibatches, 4 epoch,(0m 30s) is 0.315174\n",
      "Accuracy at 25 minibatches is 0.859375\n",
      "Average training loss at this epoch..minibatch..25..is 0.315174\n",
      "Validation loss after 25 passes is 0.463648\n",
      "Loss at 26 minibatches, 4 epoch,(0m 31s) is 0.298708\n",
      "Accuracy at 26 minibatches is 0.875000\n",
      "Average training loss at this epoch..minibatch..26..is 0.298708\n",
      "Validation loss after 26 passes is 0.420501\n",
      "Loss at 27 minibatches, 4 epoch,(0m 32s) is 0.388059\n",
      "Accuracy at 27 minibatches is 0.838542\n",
      "Average training loss at this epoch..minibatch..27..is 0.388059\n",
      "Validation loss after 27 passes is 0.450571\n",
      "Loss at 28 minibatches, 4 epoch,(0m 34s) is 0.345560\n",
      "Accuracy at 28 minibatches is 0.859375\n",
      "Average training loss at this epoch..minibatch..28..is 0.345560\n",
      "Validation loss after 28 passes is 0.432042\n",
      "Loss at 29 minibatches, 4 epoch,(0m 35s) is 0.327861\n",
      "Accuracy at 29 minibatches is 0.868750\n",
      "Average training loss at this epoch..minibatch..29..is 0.327861\n",
      "Validation loss after 29 passes is 0.420091\n",
      "Reached 5 epochs\n",
      "i 30\n",
      "Test accuracy: 0.7890625\n",
      "Max test accruacy: 0.796875\n",
      "Test aucroc: 0.8725845410628019\n",
      "Max test aucroc: 0.8792317708333334\n",
      "Train accuracy: 0.909375\n",
      "Max train accruacy: 0.909375\n",
      "Train aucroc: 0.9671945143308174\n",
      "Max train aucroc: 0.9671945143308174\n",
      "Loss at 31 minibatches, 5 epoch,(0m 37s) is 0.267607\n",
      "Accuracy at 31 minibatches is 0.906250\n",
      "Average training loss at this epoch..minibatch..31..is 0.267607\n",
      "Validation loss after 31 passes is 0.353716\n",
      "Loss at 32 minibatches, 5 epoch,(0m 39s) is 0.256055\n",
      "Accuracy at 32 minibatches is 0.937500\n",
      "Average training loss at this epoch..minibatch..32..is 0.256055\n",
      "Validation loss after 32 passes is 0.339015\n",
      "Loss at 33 minibatches, 5 epoch,(0m 40s) is 0.221528\n",
      "Accuracy at 33 minibatches is 0.942708\n",
      "Average training loss at this epoch..minibatch..33..is 0.221528\n",
      "Validation loss after 33 passes is 0.332125\n",
      "Loss at 34 minibatches, 5 epoch,(0m 41s) is 0.180412\n",
      "Accuracy at 34 minibatches is 0.957031\n",
      "Average training loss at this epoch..minibatch..34..is 0.180412\n",
      "Validation loss after 34 passes is 0.325581\n",
      "Loss at 35 minibatches, 5 epoch,(0m 42s) is 0.158055\n",
      "Accuracy at 35 minibatches is 0.965625\n",
      "Average training loss at this epoch..minibatch..35..is 0.158055\n",
      "Validation loss after 35 passes is 0.338947\n",
      "Reached 6 epochs\n",
      "i 36\n",
      "Test accuracy: 0.859375\n",
      "Max test accruacy: 0.859375\n",
      "Test aucroc: 0.9349569349569349\n",
      "Max test aucroc: 0.9349569349569349\n",
      "Train accuracy: 0.98125\n",
      "Max train accruacy: 0.98125\n",
      "Train aucroc: 0.9945241971289034\n",
      "Max train aucroc: 0.9945241971289034\n",
      "Loss at 37 minibatches, 6 epoch,(0m 45s) is 0.052020\n",
      "Accuracy at 37 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..37..is 0.052020\n",
      "Validation loss after 37 passes is 0.315728\n",
      "Loss at 38 minibatches, 6 epoch,(0m 46s) is 0.039688\n",
      "Accuracy at 38 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..38..is 0.039688\n",
      "Validation loss after 38 passes is 0.294991\n",
      "Loss at 39 minibatches, 6 epoch,(0m 47s) is 0.035432\n",
      "Accuracy at 39 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..39..is 0.035432\n",
      "Validation loss after 39 passes is 0.350669\n",
      "Loss at 40 minibatches, 6 epoch,(0m 48s) is 0.036271\n",
      "Accuracy at 40 minibatches is 0.996094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at this epoch..minibatch..40..is 0.036271\n",
      "Validation loss after 40 passes is 0.370826\n",
      "Loss at 41 minibatches, 6 epoch,(0m 50s) is 0.029988\n",
      "Accuracy at 41 minibatches is 0.996875\n",
      "Average training loss at this epoch..minibatch..41..is 0.029988\n",
      "Validation loss after 41 passes is 0.394111\n",
      "Validation loss is higher than training loss at 41 is 0.394111 , stopping training!\n",
      "Average training loss at 41 is 0.374950\n",
      "Reached 7 epochs\n",
      "i 42\n",
      "Test accuracy: 0.890625\n",
      "Max test accruacy: 0.890625\n",
      "Test aucroc: 0.9655797101449276\n",
      "Max test aucroc: 0.9655797101449276\n",
      "Train accuracy: 0.996875\n",
      "Max train accruacy: 0.996875\n",
      "Train aucroc: 0.9958315570081947\n",
      "Max train aucroc: 0.9958315570081947\n",
      "Loss at 43 minibatches, 7 epoch,(0m 52s) is 0.012920\n",
      "Accuracy at 43 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..43..is 0.012920\n",
      "Validation loss after 43 passes is 0.411292\n",
      "Validation loss is higher than training loss at 43 is 0.411292 , stopping training!\n",
      "Average training loss at 43 is 0.364894\n",
      "Loss at 44 minibatches, 7 epoch,(0m 53s) is 0.007589\n",
      "Accuracy at 44 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..44..is 0.007589\n",
      "Validation loss after 44 passes is 0.393140\n",
      "Validation loss is higher than training loss at 44 is 0.393140 , stopping training!\n",
      "Average training loss at 44 is 0.355093\n",
      "Loss at 45 minibatches, 7 epoch,(0m 54s) is 0.006304\n",
      "Accuracy at 45 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..45..is 0.006304\n",
      "Validation loss after 45 passes is 0.414920\n",
      "Validation loss is higher than training loss at 45 is 0.414920 , stopping training!\n",
      "Average training loss at 45 is 0.345847\n",
      "Loss at 46 minibatches, 7 epoch,(0m 56s) is 0.005050\n",
      "Accuracy at 46 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..46..is 0.005050\n",
      "Validation loss after 46 passes is 0.463275\n",
      "Validation loss is higher than training loss at 46 is 0.463275 , stopping training!\n",
      "Average training loss at 46 is 0.337012\n",
      "Loss at 47 minibatches, 7 epoch,(0m 57s) is 0.004350\n",
      "Accuracy at 47 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..47..is 0.004350\n",
      "Validation loss after 47 passes is 0.528374\n",
      "Validation loss is higher than training loss at 47 is 0.528374 , stopping training!\n",
      "Average training loss at 47 is 0.328625\n",
      "Reached 8 epochs\n",
      "i 48\n",
      "Test accuracy: 0.8984375\n",
      "Max test accruacy: 0.8984375\n",
      "Test aucroc: 0.9375\n",
      "Max test aucroc: 0.9655797101449276\n",
      "Train accuracy: 1.0\n",
      "Max train accruacy: 1.0\n",
      "Train aucroc: 1.0\n",
      "Max train aucroc: 1.0\n",
      "Loss at 49 minibatches, 8 epoch,(0m 59s) is 0.001049\n",
      "Accuracy at 49 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..49..is 0.001049\n",
      "Validation loss after 49 passes is 0.559832\n",
      "Validation loss is higher than training loss at 49 is 0.559832 , stopping training!\n",
      "Average training loss at 49 is 0.320636\n",
      "Loss at 50 minibatches, 8 epoch,(1m 1s) is 0.052656\n",
      "Accuracy at 50 minibatches is 0.992188\n",
      "Average training loss at this epoch..minibatch..50..is 0.052656\n",
      "Validation loss after 50 passes is 0.584562\n",
      "Validation loss is higher than training loss at 50 is 0.584562 , stopping training!\n",
      "Average training loss at 50 is 0.315484\n",
      "Loss at 51 minibatches, 8 epoch,(1m 2s) is 0.035930\n",
      "Accuracy at 51 minibatches is 0.994792\n",
      "Average training loss at this epoch..minibatch..51..is 0.035930\n",
      "Validation loss after 51 passes is 0.776991\n",
      "Validation loss is higher than training loss at 51 is 0.776991 , stopping training!\n",
      "Average training loss at 51 is 0.308205\n",
      "Loss at 52 minibatches, 8 epoch,(1m 3s) is 0.027112\n",
      "Accuracy at 52 minibatches is 0.996094\n",
      "Average training loss at this epoch..minibatch..52..is 0.027112\n",
      "Validation loss after 52 passes is 0.840824\n",
      "Validation loss is higher than training loss at 52 is 0.840824 , stopping training!\n",
      "Average training loss at 52 is 0.301215\n",
      "Loss at 53 minibatches, 8 epoch,(1m 4s) is 0.021745\n",
      "Accuracy at 53 minibatches is 0.996875\n",
      "Average training loss at this epoch..minibatch..53..is 0.021745\n",
      "Validation loss after 53 passes is 0.663349\n",
      "Validation loss is higher than training loss at 53 is 0.663349 , stopping training!\n",
      "Average training loss at 53 is 0.294527\n",
      "Reached 9 epochs\n",
      "i 54\n",
      "Test accuracy: 0.8984375\n",
      "Max test accruacy: 0.8984375\n",
      "Test aucroc: 0.9433811802232855\n",
      "Max test aucroc: 0.9655797101449276\n",
      "Train accuracy: 0.996875\n",
      "Max train accruacy: 1.0\n",
      "Train aucroc: 0.9995681174720477\n",
      "Max train aucroc: 1.0\n",
      "Loss at 55 minibatches, 9 epoch,(1m 7s) is 0.000399\n",
      "Accuracy at 55 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..55..is 0.000399\n",
      "Validation loss after 55 passes is 0.722300\n",
      "Validation loss is higher than training loss at 55 is 0.722300 , stopping training!\n",
      "Average training loss at 55 is 0.288133\n",
      "Loss at 56 minibatches, 9 epoch,(1m 8s) is 0.000378\n",
      "Accuracy at 56 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..56..is 0.000378\n",
      "Validation loss after 56 passes is 0.844076\n",
      "Validation loss is higher than training loss at 56 is 0.844076 , stopping training!\n",
      "Average training loss at 56 is 0.282010\n",
      "Loss at 57 minibatches, 9 epoch,(1m 9s) is 0.000400\n",
      "Accuracy at 57 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..57..is 0.000400\n",
      "Validation loss after 57 passes is 0.865412\n",
      "Validation loss is higher than training loss at 57 is 0.865412 , stopping training!\n",
      "Average training loss at 57 is 0.276144\n",
      "Loss at 58 minibatches, 9 epoch,(1m 11s) is 0.000407\n",
      "Accuracy at 58 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..58..is 0.000407\n",
      "Validation loss after 58 passes is 0.851694\n",
      "Validation loss is higher than training loss at 58 is 0.851694 , stopping training!\n",
      "Average training loss at 58 is 0.270518\n",
      "Loss at 59 minibatches, 9 epoch,(1m 12s) is 0.018566\n",
      "Accuracy at 59 minibatches is 0.996875\n",
      "Average training loss at this epoch..minibatch..59..is 0.018566\n",
      "Validation loss after 59 passes is 0.816323\n",
      "Validation loss is higher than training loss at 59 is 0.816323 , stopping training!\n",
      "Average training loss at 59 is 0.266931\n",
      "Reached 10 epochs\n",
      "i 60\n",
      "Test accuracy: 0.8828125\n",
      "Max test accruacy: 0.8984375\n",
      "Test aucroc: 0.9451704545454546\n",
      "Max test aucroc: 0.9655797101449276\n",
      "Train accuracy: 0.996875\n",
      "Max train accruacy: 1.0\n",
      "Train aucroc: 1.0\n",
      "Max train aucroc: 1.0\n",
      "Loss at 61 minibatches, 10 epoch,(1m 14s) is 0.086806\n",
      "Accuracy at 61 minibatches is 0.984375\n",
      "Average training loss at this epoch..minibatch..61..is 0.086806\n",
      "Validation loss after 61 passes is 0.945245\n",
      "Validation loss is higher than training loss at 61 is 0.945245 , stopping training!\n",
      "Average training loss at 61 is 0.263399\n",
      "Loss at 62 minibatches, 10 epoch,(1m 16s) is 0.043612\n",
      "Accuracy at 62 minibatches is 0.992188\n",
      "Average training loss at this epoch..minibatch..62..is 0.043612\n",
      "Validation loss after 62 passes is 0.921615\n",
      "Validation loss is higher than training loss at 62 is 0.921615 , stopping training!\n",
      "Average training loss at 62 is 0.258342\n",
      "Loss at 63 minibatches, 10 epoch,(1m 17s) is 0.029194\n",
      "Accuracy at 63 minibatches is 0.994792\n",
      "Average training loss at this epoch..minibatch..63..is 0.029194\n",
      "Validation loss after 63 passes is 0.830841\n",
      "Validation loss is higher than training loss at 63 is 0.830841 , stopping training!\n",
      "Average training loss at 63 is 0.253474\n",
      "Loss at 64 minibatches, 10 epoch,(1m 18s) is 0.022032\n",
      "Accuracy at 64 minibatches is 0.996094\n",
      "Average training loss at this epoch..minibatch..64..is 0.022032\n",
      "Validation loss after 64 passes is 0.952282\n",
      "Validation loss is higher than training loss at 64 is 0.952282 , stopping training!\n",
      "Average training loss at 64 is 0.248791\n",
      "Loss at 65 minibatches, 10 epoch,(1m 19s) is 0.017822\n",
      "Accuracy at 65 minibatches is 0.996875\n",
      "Average training loss at this epoch..minibatch..65..is 0.017822\n",
      "Validation loss after 65 passes is 0.779722\n",
      "Validation loss is higher than training loss at 65 is 0.779722 , stopping training!\n",
      "Average training loss at 65 is 0.244285\n",
      "Reached 11 epochs\n",
      "i 66\n",
      "Test accuracy: 0.8671875\n",
      "Max test accruacy: 0.8984375\n",
      "Test aucroc: 0.942235666954768\n",
      "Max test aucroc: 0.9655797101449276\n",
      "Train accuracy: 0.996875\n",
      "Max train accruacy: 1.0\n",
      "Train aucroc: 1.0\n",
      "Max train aucroc: 1.0\n",
      "Loss at 67 minibatches, 11 epoch,(1m 22s) is 0.000614\n",
      "Accuracy at 67 minibatches is 1.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at this epoch..minibatch..67..is 0.000614\n",
      "Validation loss after 67 passes is 0.864138\n",
      "Validation loss is higher than training loss at 67 is 0.864138 , stopping training!\n",
      "Average training loss at 67 is 0.239934\n",
      "Loss at 68 minibatches, 11 epoch,(1m 23s) is 0.001117\n",
      "Accuracy at 68 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..68..is 0.001117\n",
      "Validation loss after 68 passes is 0.955011\n",
      "Validation loss is higher than training loss at 68 is 0.955011 , stopping training!\n",
      "Average training loss at 68 is 0.235753\n",
      "Loss at 69 minibatches, 11 epoch,(1m 24s) is 0.001101\n",
      "Accuracy at 69 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..69..is 0.001101\n",
      "Validation loss after 69 passes is 0.956997\n",
      "Validation loss is higher than training loss at 69 is 0.956997 , stopping training!\n",
      "Average training loss at 69 is 0.231706\n",
      "Loss at 70 minibatches, 11 epoch,(1m 25s) is 0.001834\n",
      "Accuracy at 70 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..70..is 0.001834\n",
      "Validation loss after 70 passes is 1.068600\n",
      "Validation loss is higher than training loss at 70 is 1.068600 , stopping training!\n",
      "Average training loss at 70 is 0.227848\n",
      "Loss at 71 minibatches, 11 epoch,(1m 27s) is 0.001780\n",
      "Accuracy at 71 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..71..is 0.001780\n",
      "Validation loss after 71 passes is 0.740546\n",
      "Validation loss is higher than training loss at 71 is 0.740546 , stopping training!\n",
      "Average training loss at 71 is 0.224076\n",
      "Reached 12 epochs\n",
      "i 72\n",
      "Test accuracy: 0.890625\n",
      "Max test accruacy: 0.8984375\n",
      "Test aucroc: 0.9582763337893296\n",
      "Max test aucroc: 0.9655797101449276\n",
      "Train accuracy: 1.0\n",
      "Max train accruacy: 1.0\n",
      "Train aucroc: 1.0\n",
      "Max train aucroc: 1.0\n",
      "Loss at 73 minibatches, 12 epoch,(1m 29s) is 0.002890\n",
      "Accuracy at 73 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..73..is 0.002890\n",
      "Validation loss after 73 passes is 0.937800\n",
      "Validation loss is higher than training loss at 73 is 0.937800 , stopping training!\n",
      "Average training loss at 73 is 0.220450\n",
      "Loss at 74 minibatches, 12 epoch,(1m 30s) is 0.002008\n",
      "Accuracy at 74 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..74..is 0.002008\n",
      "Validation loss after 74 passes is 0.844413\n",
      "Validation loss is higher than training loss at 74 is 0.844413 , stopping training!\n",
      "Average training loss at 74 is 0.216913\n",
      "Loss at 75 minibatches, 12 epoch,(1m 32s) is 0.001749\n",
      "Accuracy at 75 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..75..is 0.001749\n",
      "Validation loss after 75 passes is 0.818803\n",
      "Validation loss is higher than training loss at 75 is 0.818803 , stopping training!\n",
      "Average training loss at 75 is 0.213489\n",
      "Loss at 76 minibatches, 12 epoch,(1m 33s) is 0.001526\n",
      "Accuracy at 76 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..76..is 0.001526\n",
      "Validation loss after 76 passes is 0.936288\n",
      "Validation loss is higher than training loss at 76 is 0.936288 , stopping training!\n",
      "Average training loss at 76 is 0.210167\n",
      "Loss at 77 minibatches, 12 epoch,(1m 34s) is 0.001442\n",
      "Accuracy at 77 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..77..is 0.001442\n",
      "Validation loss after 77 passes is 1.202449\n",
      "Validation loss is higher than training loss at 77 is 1.202449 , stopping training!\n",
      "Average training loss at 77 is 0.206951\n",
      "Reached 13 epochs\n",
      "i 78\n",
      "Test accuracy: 0.875\n",
      "Max test accruacy: 0.8984375\n",
      "Test aucroc: 0.9593900966183574\n",
      "Max test aucroc: 0.9655797101449276\n",
      "Train accuracy: 1.0\n",
      "Max train accruacy: 1.0\n",
      "Train aucroc: 1.0\n",
      "Max train aucroc: 1.0\n",
      "Loss at 79 minibatches, 13 epoch,(1m 36s) is 0.000723\n",
      "Accuracy at 79 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..79..is 0.000723\n",
      "Validation loss after 79 passes is 0.980586\n",
      "Validation loss is higher than training loss at 79 is 0.980586 , stopping training!\n",
      "Average training loss at 79 is 0.203826\n",
      "Loss at 80 minibatches, 13 epoch,(1m 38s) is 0.000872\n",
      "Accuracy at 80 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..80..is 0.000872\n",
      "Validation loss after 80 passes is 1.119329\n",
      "Validation loss is higher than training loss at 80 is 1.119329 , stopping training!\n",
      "Average training loss at 80 is 0.200799\n",
      "Loss at 81 minibatches, 13 epoch,(1m 39s) is 0.001309\n",
      "Accuracy at 81 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..81..is 0.001309\n",
      "Validation loss after 81 passes is 1.178876\n",
      "Validation loss is higher than training loss at 81 is 1.178876 , stopping training!\n",
      "Average training loss at 81 is 0.197878\n",
      "Loss at 82 minibatches, 13 epoch,(1m 40s) is 0.001254\n",
      "Accuracy at 82 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..82..is 0.001254\n",
      "Validation loss after 82 passes is 1.029017\n",
      "Validation loss is higher than training loss at 82 is 1.029017 , stopping training!\n",
      "Average training loss at 82 is 0.195026\n",
      "Loss at 83 minibatches, 13 epoch,(1m 41s) is 0.001422\n",
      "Accuracy at 83 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..83..is 0.001422\n",
      "Validation loss after 83 passes is 1.191036\n",
      "Validation loss is higher than training loss at 83 is 1.191036 , stopping training!\n",
      "Average training loss at 83 is 0.192270\n",
      "Reached 14 epochs\n",
      "i 84\n",
      "Test accuracy: 0.890625\n",
      "Max test accruacy: 0.8984375\n",
      "Test aucroc: 0.9726751207729469\n",
      "Max test aucroc: 0.9726751207729469\n",
      "Train accuracy: 1.0\n",
      "Max train accruacy: 1.0\n",
      "Train aucroc: 1.0\n",
      "Max train aucroc: 1.0\n",
      "Loss at 85 minibatches, 14 epoch,(1m 44s) is 0.000897\n",
      "Accuracy at 85 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..85..is 0.000897\n",
      "Validation loss after 85 passes is 0.759171\n",
      "Validation loss is higher than training loss at 85 is 0.759171 , stopping training!\n",
      "Average training loss at 85 is 0.189575\n",
      "Loss at 86 minibatches, 14 epoch,(1m 45s) is 0.000945\n",
      "Accuracy at 86 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..86..is 0.000945\n",
      "Validation loss after 86 passes is 1.113336\n",
      "Validation loss is higher than training loss at 86 is 1.113336 , stopping training!\n",
      "Average training loss at 86 is 0.186955\n",
      "Loss at 87 minibatches, 14 epoch,(1m 46s) is 0.000958\n",
      "Accuracy at 87 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..87..is 0.000958\n",
      "Validation loss after 87 passes is 1.462147\n",
      "Validation loss is higher than training loss at 87 is 1.462147 , stopping training!\n",
      "Average training loss at 87 is 0.184408\n",
      "Loss at 88 minibatches, 14 epoch,(1m 47s) is 0.000827\n",
      "Accuracy at 88 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..88..is 0.000827\n",
      "Validation loss after 88 passes is 1.201652\n",
      "Validation loss is higher than training loss at 88 is 1.201652 , stopping training!\n",
      "Average training loss at 88 is 0.181922\n",
      "Loss at 89 minibatches, 14 epoch,(1m 49s) is 0.000879\n",
      "Accuracy at 89 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..89..is 0.000879\n",
      "Validation loss after 89 passes is 1.377400\n",
      "Validation loss is higher than training loss at 89 is 1.377400 , stopping training!\n",
      "Average training loss at 89 is 0.179511\n",
      "Reached 15 epochs\n",
      "i 90\n",
      "Test accuracy: 0.8515625\n",
      "Max test accruacy: 0.8984375\n",
      "Test aucroc: 0.9527769527769528\n",
      "Max test aucroc: 0.9726751207729469\n",
      "Train accuracy: 1.0\n",
      "Max train accruacy: 1.0\n",
      "Train aucroc: 1.0\n",
      "Max train aucroc: 1.0\n",
      "Loss at 91 minibatches, 15 epoch,(1m 51s) is 0.001217\n",
      "Accuracy at 91 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..91..is 0.001217\n",
      "Validation loss after 91 passes is 1.027958\n",
      "Validation loss is higher than training loss at 91 is 1.027958 , stopping training!\n",
      "Average training loss at 91 is 0.177165\n",
      "Loss at 92 minibatches, 15 epoch,(1m 52s) is 0.000862\n",
      "Accuracy at 92 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..92..is 0.000862\n",
      "Validation loss after 92 passes is 1.379232\n",
      "Validation loss is higher than training loss at 92 is 1.379232 , stopping training!\n",
      "Average training loss at 92 is 0.174870\n",
      "Loss at 93 minibatches, 15 epoch,(1m 53s) is 0.000778\n",
      "Accuracy at 93 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..93..is 0.000778\n",
      "Validation loss after 93 passes is 1.120438\n",
      "Validation loss is higher than training loss at 93 is 1.120438 , stopping training!\n",
      "Average training loss at 93 is 0.172636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at 94 minibatches, 15 epoch,(1m 55s) is 0.000754\n",
      "Accuracy at 94 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..94..is 0.000754\n",
      "Validation loss after 94 passes is 1.122883\n",
      "Validation loss is higher than training loss at 94 is 1.122883 , stopping training!\n",
      "Average training loss at 94 is 0.170460\n",
      "Loss at 95 minibatches, 15 epoch,(1m 56s) is 0.000696\n",
      "Accuracy at 95 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..95..is 0.000696\n",
      "Validation loss after 95 passes is 1.200686\n",
      "Validation loss is higher than training loss at 95 is 1.200686 , stopping training!\n",
      "Average training loss at 95 is 0.168335\n",
      "Reached 16 epochs\n",
      "i 96\n",
      "Test accuracy: 0.875\n",
      "Max test accruacy: 0.8984375\n",
      "Test aucroc: 0.9602150537634409\n",
      "Max test aucroc: 0.9726751207729469\n",
      "Train accuracy: 1.0\n",
      "Max train accruacy: 1.0\n",
      "Train aucroc: 1.0\n",
      "Max train aucroc: 1.0\n",
      "Loss at 97 minibatches, 16 epoch,(1m 58s) is 0.000588\n",
      "Accuracy at 97 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..97..is 0.000588\n",
      "Validation loss after 97 passes is 1.388146\n",
      "Validation loss is higher than training loss at 97 is 1.388146 , stopping training!\n",
      "Average training loss at 97 is 0.166264\n",
      "Loss at 98 minibatches, 16 epoch,(2m 0s) is 0.000466\n",
      "Accuracy at 98 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..98..is 0.000466\n",
      "Validation loss after 98 passes is 1.298083\n",
      "Validation loss is higher than training loss at 98 is 1.298083 , stopping training!\n",
      "Average training loss at 98 is 0.164240\n",
      "Loss at 99 minibatches, 16 epoch,(2m 1s) is 0.000403\n",
      "Accuracy at 99 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..99..is 0.000403\n",
      "Validation loss after 99 passes is 1.216843\n",
      "Validation loss is higher than training loss at 99 is 1.216843 , stopping training!\n",
      "Average training loss at 99 is 0.162265\n",
      "Loss at 100 minibatches, 16 epoch,(2m 2s) is 0.000445\n",
      "Accuracy at 100 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..100..is 0.000445\n",
      "Validation loss after 100 passes is 1.313807\n",
      "Validation loss is higher than training loss at 100 is 1.313807 , stopping training!\n",
      "Average training loss at 100 is 0.160340\n",
      "Loss at 101 minibatches, 16 epoch,(2m 3s) is 0.000454\n",
      "Accuracy at 101 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..101..is 0.000454\n",
      "Validation loss after 101 passes is 1.137624\n",
      "Validation loss is higher than training loss at 101 is 1.137624 , stopping training!\n",
      "Average training loss at 101 is 0.158459\n",
      "Reached 17 epochs\n",
      "i 102\n",
      "Test accuracy: 0.875\n",
      "Max test accruacy: 0.8984375\n",
      "Test aucroc: 0.9559941520467836\n",
      "Max test aucroc: 0.9726751207729469\n",
      "Train accuracy: 1.0\n",
      "Max train accruacy: 1.0\n",
      "Train aucroc: 1.0\n",
      "Max train aucroc: 1.0\n",
      "Loss at 103 minibatches, 17 epoch,(2m 6s) is 0.000364\n",
      "Accuracy at 103 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..103..is 0.000364\n",
      "Validation loss after 103 passes is 1.226020\n",
      "Validation loss is higher than training loss at 103 is 1.226020 , stopping training!\n",
      "Average training loss at 103 is 0.156621\n",
      "Loss at 104 minibatches, 17 epoch,(2m 7s) is 0.000352\n",
      "Accuracy at 104 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..104..is 0.000352\n",
      "Validation loss after 104 passes is 0.950309\n",
      "Validation loss is higher than training loss at 104 is 0.950309 , stopping training!\n",
      "Average training loss at 104 is 0.154825\n",
      "Loss at 105 minibatches, 17 epoch,(2m 8s) is 0.000348\n",
      "Accuracy at 105 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..105..is 0.000348\n",
      "Validation loss after 105 passes is 1.306859\n",
      "Validation loss is higher than training loss at 105 is 1.306859 , stopping training!\n",
      "Average training loss at 105 is 0.153069\n",
      "Loss at 106 minibatches, 17 epoch,(2m 9s) is 0.000339\n",
      "Accuracy at 106 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..106..is 0.000339\n",
      "Validation loss after 106 passes is 1.122123\n",
      "Validation loss is higher than training loss at 106 is 1.122123 , stopping training!\n",
      "Average training loss at 106 is 0.151353\n",
      "Loss at 107 minibatches, 17 epoch,(2m 11s) is 0.000334\n",
      "Accuracy at 107 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..107..is 0.000334\n",
      "Validation loss after 107 passes is 1.248049\n",
      "Validation loss is higher than training loss at 107 is 1.248049 , stopping training!\n",
      "Average training loss at 107 is 0.149675\n",
      "Reached 18 epochs\n",
      "i 108\n",
      "Test accuracy: 0.9140625\n",
      "Max test accruacy: 0.9140625\n",
      "Test aucroc: 0.9755980861244019\n",
      "Max test aucroc: 0.9755980861244019\n",
      "Train accuracy: 1.0\n",
      "Max train accruacy: 1.0\n",
      "Train aucroc: 1.0\n",
      "Max train aucroc: 1.0\n",
      "Loss at 109 minibatches, 18 epoch,(2m 13s) is 0.000341\n",
      "Accuracy at 109 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..109..is 0.000341\n",
      "Validation loss after 109 passes is 1.233811\n",
      "Validation loss is higher than training loss at 109 is 1.233811 , stopping training!\n",
      "Average training loss at 109 is 0.148033\n",
      "Loss at 110 minibatches, 18 epoch,(2m 14s) is 0.000346\n",
      "Accuracy at 110 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..110..is 0.000346\n",
      "Validation loss after 110 passes is 1.228408\n",
      "Validation loss is higher than training loss at 110 is 1.228408 , stopping training!\n",
      "Average training loss at 110 is 0.146428\n",
      "Loss at 111 minibatches, 18 epoch,(2m 16s) is 0.000330\n",
      "Accuracy at 111 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..111..is 0.000330\n",
      "Validation loss after 111 passes is 0.967125\n",
      "Validation loss is higher than training loss at 111 is 0.967125 , stopping training!\n",
      "Average training loss at 111 is 0.144857\n",
      "Loss at 112 minibatches, 18 epoch,(2m 17s) is 0.000301\n",
      "Accuracy at 112 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..112..is 0.000301\n",
      "Validation loss after 112 passes is 1.070130\n",
      "Validation loss is higher than training loss at 112 is 1.070130 , stopping training!\n",
      "Average training loss at 112 is 0.143318\n",
      "Loss at 113 minibatches, 18 epoch,(2m 18s) is 0.000282\n",
      "Accuracy at 113 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..113..is 0.000282\n",
      "Validation loss after 113 passes is 1.145633\n",
      "Validation loss is higher than training loss at 113 is 1.145633 , stopping training!\n",
      "Average training loss at 113 is 0.141812\n",
      "Reached 19 epochs\n",
      "i 114\n",
      "Test accuracy: 0.8984375\n",
      "Max test accruacy: 0.9140625\n",
      "Test aucroc: 0.9637044270833333\n",
      "Max test aucroc: 0.9755980861244019\n",
      "Train accuracy: 1.0\n",
      "Max train accruacy: 1.0\n",
      "Train aucroc: 1.0\n",
      "Max train aucroc: 1.0\n",
      "Loss at 115 minibatches, 19 epoch,(2m 20s) is 0.000225\n",
      "Accuracy at 115 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..115..is 0.000225\n",
      "Validation loss after 115 passes is 1.160827\n",
      "Validation loss is higher than training loss at 115 is 1.160827 , stopping training!\n",
      "Average training loss at 115 is 0.140337\n",
      "Loss at 116 minibatches, 19 epoch,(2m 22s) is 0.000246\n",
      "Accuracy at 116 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..116..is 0.000246\n",
      "Validation loss after 116 passes is 1.266589\n",
      "Validation loss is higher than training loss at 116 is 1.266589 , stopping training!\n",
      "Average training loss at 116 is 0.138893\n",
      "Loss at 117 minibatches, 19 epoch,(2m 23s) is 0.000222\n",
      "Accuracy at 117 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..117..is 0.000222\n",
      "Validation loss after 117 passes is 1.148483\n",
      "Validation loss is higher than training loss at 117 is 1.148483 , stopping training!\n",
      "Average training loss at 117 is 0.137477\n",
      "Loss at 118 minibatches, 19 epoch,(2m 24s) is 0.000199\n",
      "Accuracy at 118 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..118..is 0.000199\n",
      "Validation loss after 118 passes is 1.162329\n",
      "Validation loss is higher than training loss at 118 is 1.162329 , stopping training!\n",
      "Average training loss at 118 is 0.136090\n",
      "Loss at 119 minibatches, 19 epoch,(2m 25s) is 0.000212\n",
      "Accuracy at 119 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..119..is 0.000212\n",
      "Validation loss after 119 passes is 1.076892\n",
      "Validation loss is higher than training loss at 119 is 1.076892 , stopping training!\n",
      "Average training loss at 119 is 0.134732\n",
      "Reached 20 epochs\n",
      "i 120\n",
      "Test accuracy: 0.890625\n",
      "Max test accruacy: 0.9140625\n",
      "Test aucroc: 0.9618354618354618\n",
      "Max test aucroc: 0.9755980861244019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 1.0\n",
      "Max train accruacy: 1.0\n",
      "Train aucroc: 1.0\n",
      "Max train aucroc: 1.0\n"
     ]
    }
   ],
   "source": [
    "acc, aucroc, max_eval_acc, max_eval_aucroc = train_early_stopping(my_batch, corpus.X_train, corpus.y_train, corpus.pic_train, corpus.X_test, corpus.y_test, corpus.pic_test, word_attn, sent_attn, word_optimizer, sent_optimizer, \n",
    "                            criterion, 5000, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(test_accuracy_full_batch(corpus.X_test, corpus.y_test, my_batch, word_attn, sent_attn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test_accuracy_full_batch(corpus.X_train, corpus.y_train, my_batch, word_attn, sent_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
