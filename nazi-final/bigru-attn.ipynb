{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "        self.pretrain_vec = [] # should match index order of words in dict.\n",
    "\n",
    "    def add_word(self, word, vec=None):\n",
    "        if vec is None:\n",
    "            if word not in self.word2idx:\n",
    "                self.idx2word.append(word)\n",
    "                self.word2idx[word] = len(self.idx2word) - 1\n",
    "        else:\n",
    "            if word not in self.word2idx:\n",
    "                self.pretrain_vec.append(vec)\n",
    "                self.idx2word.append(word)\n",
    "                self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, path, language):\n",
    "        self.dictionary = Dictionary()\n",
    "        if language is not None:\n",
    "            self.pretrained = self.add_pretrained(os.path.join('', 'wiki.' + language + '.vec'))\n",
    "        #self.trainid, self.trainlab, self.trainidx = self.tokenize_by_user(os.path.join(path, 'train.csv'),True)\n",
    "        #self.validid, self.validlab, self.valididx = self.tokenize_by_user(os.path.join(path, 'valid.csv'),False)\n",
    "        #self.testid, self.testlab, self.testidx = self.tokenize_by_user(os.path.join(path, 'test.csv'),False)\n",
    "        self.X_train, self.y_train = self.tokenize(os.path.join('', 'train.csv'),True)\n",
    "        #self.X_valid, self.y_valid = self.tokenize(os.path.join(path, 'valid.csv'),False)\n",
    "        self.X_test, self.y_test = self.tokenize(os.path.join('', 'test.csv'),False)\n",
    "\n",
    "    def add_pretrained(self, path):\n",
    "        assert os.path.exists(path)\n",
    "\n",
    "        # Add words with pretrained vectors to the dictionary\n",
    "        # might be weird because no eos was added?\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                words = line.split()\n",
    "                if len(words) == 2: #first line\n",
    "                    continue\n",
    "                word = words[0]\n",
    "                vec = words[1:]\n",
    "                if len(vec) != 300:\n",
    "                    continue #this skips the space embedding\n",
    "                #vec = np.array(list(map(float, vec)))\n",
    "                vec = list(map(float,vec))\n",
    "                tokens += 1\n",
    "                \n",
    "                self.dictionary.add_word(word, vec)\n",
    "    def tokenize(self, path, header):\n",
    "        \"\"\"Tokenizes a text file.\"\"\"\n",
    "        assert os.path.exists(path)\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.reader(f)\n",
    "            tokens = 0\n",
    "            prev = None\n",
    "            if header:\n",
    "                first = True\n",
    "            else:\n",
    "                first = False\n",
    "            tweet_count = 0\n",
    "            user_idx = -1\n",
    "            for row in reader:\n",
    "                if first:\n",
    "                    first = False\n",
    "                    continue\n",
    "                if len(row) is not 6:\n",
    "                    continue\n",
    "                \n",
    "                tweet = row[0]\n",
    "                label = row[1]\n",
    "                if not label.isdigit():\n",
    "                    continue\n",
    "                extra = row[2:5] #bio, tweet pic, profile pic, user id\n",
    "                if row[2] != prev: #new user\n",
    "                    prev = row[2]\n",
    "                    tweet_count = 0\n",
    "                    user_idx += 1\n",
    "\n",
    "                words = tweet.split()\n",
    "                tokens += len(words)\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "\n",
    "        # Tokenize file content\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            x = np.zeros(user_idx+1,dtype='object')\n",
    "            y = np.zeros(user_idx+1,dtype='int')\n",
    "            #ids = torch.LongTensor(tokens)\n",
    "            #idxs = torch.LongTensor(user_idx+1)\n",
    "            #labels = torch.LongTensor(user_idx+1)\n",
    "            #print(user_idx+1)\n",
    "            token = 0\n",
    "            prev = None\n",
    "\n",
    "            reader = csv.reader(f)\n",
    "            if header:\n",
    "                first = True\n",
    "            else:\n",
    "                first = False\n",
    "            user_idx = -1\n",
    "            for row in reader:\n",
    "                if first:\n",
    "                    first = False\n",
    "                    continue\n",
    "                if len(row) is not 6:\n",
    "                    continue\n",
    "                \n",
    "                tweet = row[0]\n",
    "                label = row[1]\n",
    "                if not label.isdigit():\n",
    "                    continue\n",
    "                extra = row[2:5] #bio, tweet pic, profile pic, user id\n",
    "                if row[2] != prev:\n",
    "                    tweet_idx = -1\n",
    "                    user_idx += 1\n",
    "                    prev = row[2]\n",
    "                    y[user_idx] = int(label)\n",
    "                    x[user_idx] = []\n",
    "                    #print(token, \"NEW USER\")\n",
    "                    #idxs[user_idx] = token\n",
    "                \n",
    "\n",
    "                words = tweet.split()\n",
    "                token = 0\n",
    "                tweet_idx+=1\n",
    "                if tweet_idx >=20:\n",
    "                    #print(tweet_idx)\n",
    "                    continue\n",
    "                if tweet_idx==0:\n",
    "                    x[user_idx].append([])\n",
    "                for word in words:\n",
    "                    #FLAT STRUCTURE SO THERE'S ONLY ONE 'TWEET'\n",
    "                    x[user_idx][0].append(self.dictionary.word2idx[word])\n",
    "                    token+=1\n",
    "                \n",
    "\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nclass AttentionSentRNN(nn.Module):\\n    \\n    \\n    def __init__(self, batch_size, sent_gru_hidden, word_gru_hidden, n_classes, dropout, bidirectional= True):        \\n        \\n        super(AttentionSentRNN, self).__init__()\\n        \\n        self.batch_size = batch_size\\n        self.sent_gru_hidden = sent_gru_hidden\\n        self.n_classes = n_classes\\n        self.word_gru_hidden = word_gru_hidden\\n        self.bidirectional = bidirectional\\n        \\n        self.drop = nn.Dropout(dropout)\\n\\n        initrange = 0.1\\n        \\n        \\n        if bidirectional == True:\\n            self.sent_gru = nn.GRU(2 * word_gru_hidden, sent_gru_hidden, bidirectional= True)        \\n            self.weight_W_sent = nn.Parameter(torch.Tensor(2* sent_gru_hidden ,2* sent_gru_hidden))\\n            self.bias_sent = nn.Parameter(torch.Tensor(2* sent_gru_hidden,1))\\n            self.weight_proj_sent = nn.Parameter(torch.Tensor(2* sent_gru_hidden, 1))\\n            self.final_linear = nn.Linear(2* sent_gru_hidden, n_classes)\\n        else:\\n            self.sent_gru = nn.GRU(word_gru_hidden, sent_gru_hidden, bidirectional= True)        \\n            self.weight_W_sent = nn.Parameter(torch.Tensor(sent_gru_hidden ,sent_gru_hidden))\\n            self.bias_sent = nn.Parameter(torch.Tensor(sent_gru_hidden,1))\\n            self.weight_proj_sent = nn.Parameter(torch.Tensor(sent_gru_hidden, 1))\\n            self.final_linear = nn.Linear(sent_gru_hidden, n_classes)\\n        self.softmax_sent = nn.Softmax()\\n        self.final_softmax = nn.Softmax()\\n        self.bias_sent.data.uniform_(-initrange, initrange)\\n        #self.sent_gru.data.uniform_(-initrange,initrange)\\n        self.weight_W_sent.data.uniform_(-initrange, initrange)\\n        self.weight_proj_sent.data.uniform_(-initrange,initrange)\\n        \\n        \\n    def forward(self, word_attention_vectors, state_sent):\\n        #MANUALLY DROPOUT THE GRU\\n        #state_word = self.drop(state_sent)\\n        output_sent, state_sent = self.sent_gru(word_attention_vectors, state_sent)   \\n        #state_word = self.drop(state_sent)\\n        output_sent = self.drop(output_sent)\\n        sent_squish = self.drop(batch_matmul_bias(output_sent, self.weight_W_sent,self.bias_sent, nonlinearity='tanh'))\\n        sent_attn = self.drop(batch_matmul(sent_squish, self.weight_proj_sent))\\n        sent_attn_norm = self.drop(self.softmax_sent(sent_attn.transpose(1,0)))\\n        sent_attn_vectors = self.drop(attention_mul(output_sent, sent_attn_norm.transpose(1,0)))    \\n        # final classifier\\n        final_map = self.final_linear(sent_attn_vectors.squeeze(0))\\n        return F.log_softmax(final_map), state_sent, sent_attn_norm\\n    \\n    def init_hidden(self):\\n        if self.bidirectional == True:\\n            return Variable(torch.zeros(2, self.batch_size, self.sent_gru_hidden))\\n        else:\\n            return Variable(torch.zeros(1, self.batch_size, self.sent_gru_hidden))\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#attention functions\n",
    "\n",
    "def batch_matmul_bias(seq, weight, bias, nonlinearity=''):\n",
    "    s = None\n",
    "    bias_dim = bias.size()\n",
    "    for i in range(seq.size(0)):\n",
    "        _s = torch.mm(seq[i], weight) \n",
    "        _s_bias = _s + bias.expand(bias_dim[0], _s.size()[0]).transpose(0,1)\n",
    "        if(nonlinearity=='tanh'):\n",
    "            _s_bias = torch.tanh(_s_bias)\n",
    "        _s_bias = _s_bias.unsqueeze(0)\n",
    "        if(s is None):\n",
    "            s = _s_bias\n",
    "        else:\n",
    "            s = torch.cat((s,_s_bias),0)\n",
    "    return s.squeeze()\n",
    "\n",
    "def batch_matmul(seq, weight, nonlinearity=''):\n",
    "    s = None\n",
    "    for i in range(seq.size(0)):\n",
    "        _s = torch.mm(seq[i], weight)\n",
    "        if(nonlinearity=='tanh'):\n",
    "            _s = torch.tanh(_s)\n",
    "        _s = _s.unsqueeze(0)\n",
    "        if(s is None):\n",
    "            s = _s\n",
    "        else:\n",
    "            s = torch.cat((s,_s),0)\n",
    "    return s.squeeze()\n",
    "\n",
    "def attention_mul(rnn_outputs, att_weights):\n",
    "    attn_vectors = None\n",
    "    for i in range(rnn_outputs.size(0)):\n",
    "        h_i = rnn_outputs[i]\n",
    "        a_i = att_weights[i].unsqueeze(1).expand_as(h_i)\n",
    "        h_i = a_i * h_i\n",
    "        h_i = h_i.unsqueeze(0)\n",
    "        if(attn_vectors is None):\n",
    "            attn_vectors = h_i\n",
    "        else:\n",
    "            attn_vectors = torch.cat((attn_vectors,h_i),0)\n",
    "    return torch.sum(attn_vectors, 0).unsqueeze(0)\n",
    "\n",
    "'''\n",
    "def batch_matmul_bias(seq, weight, bias, nonlinearity=''):\n",
    "    s = None\n",
    "    bias_dim = bias.size()\n",
    "    for i in range(seq.size(0)):\n",
    "        _s = torch.mm(seq[i], weight) \n",
    "        _s_bias = _s + bias.expand(bias_dim[0], _s.size()[0]).transpose(0,1)\n",
    "        if(nonlinearity=='tanh'):\n",
    "            _s_bias = torch.tanh(_s_bias)\n",
    "        _s_bias = _s_bias.unsqueeze(0)\n",
    "        if(s is None):\n",
    "            s = _s_bias\n",
    "        else:\n",
    "            s = torch.cat((s,_s_bias),0)\n",
    "    return s.squeeze()\n",
    "    \n",
    "def batch_matmul(seq, weight, nonlinearity=''):\n",
    "    s = None\n",
    "    for i in range(seq.size(0)):\n",
    "        _s = torch.mm(seq[i], weight)\n",
    "        if(nonlinearity=='tanh'):\n",
    "            _s = torch.tanh(_s)\n",
    "        _s = _s.unsqueeze(0)\n",
    "        if(s is None):\n",
    "            s = _s\n",
    "        else:\n",
    "            s = torch.cat((s,_s),0)\n",
    "    return s.squeeze()\n",
    "\n",
    "def attention_mul(self, rnn_outputs, att_weights):\n",
    "    attn_vectors = None\n",
    "    for i in range(rnn_outputs.size(0)):\n",
    "        h_i = rnn_outputs[i]\n",
    "        a_i = att_weights[i].unsqueeze(1).expand_as(h_i)\n",
    "        h_i = a_i * h_i\n",
    "        h_i = h_i.unsqueeze(0)\n",
    "        if(attn_vectors is None):\n",
    "            attn_vectors = h_i\n",
    "        else:\n",
    "            attn_vectors = torch.cat((attn_vectors,h_i),0)\n",
    "    return torch.sum(attn_vectors, 0).unsqueeze(0)\n",
    "'''\n",
    "class AttentionWordRNN(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self, embeds, batch_size, num_tokens, embed_size, word_gru_hidden, dropout, n_classes, bidirectional= True):        \n",
    "        \n",
    "        super(AttentionWordRNN, self).__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.num_tokens = num_tokens\n",
    "        self.embed_size = embed_size\n",
    "        self.word_gru_hidden = word_gru_hidden\n",
    "        self.bidirectional = bidirectional\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        self.lookup = nn.Embedding(num_tokens, embed_size)\n",
    "\n",
    "        #init lookup table\n",
    "        \n",
    "\n",
    "        \n",
    "        initrange = 0.1\n",
    "\n",
    "        k = len(embeds) # the first k indices are pretrained. the rest are unknown\n",
    "        \n",
    "        if k is not 0:\n",
    "            first = np.array(embeds)\n",
    "            second = np.random.uniform(-initrange,initrange,size=(num_tokens-k,embed_size))\n",
    "            self.lookup.weight.data.copy_(torch.from_numpy(np.concatenate((first,second),axis=0)))\n",
    "        else:\n",
    "            self.lookup.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "\n",
    "        if bidirectional == True:\n",
    "            self.word_gru = nn.GRU(embed_size, word_gru_hidden, bidirectional= True)\n",
    "            self.weight_W_word = nn.Parameter(torch.Tensor(2* word_gru_hidden,2*word_gru_hidden))\n",
    "            self.bias_word = nn.Parameter(torch.Tensor(2* word_gru_hidden,1))\n",
    "            self.weight_proj_word = nn.Parameter(torch.Tensor(2*word_gru_hidden, 1))\n",
    "            self.final_linear = nn.Linear(2*word_gru_hidden, n_classes)\n",
    "        else:\n",
    "            self.word_gru = nn.GRU(embed_size, word_gru_hidden, bidirectional= False)\n",
    "            self.weight_W_word = nn.Parameter(torch.Tensor(word_gru_hidden, word_gru_hidden))\n",
    "            self.bias_word = nn.Parameter(torch.Tensor(word_gru_hidden,1))\n",
    "            self.weight_proj_word = nn.Parameter(torch.Tensor(word_gru_hidden, 1))\n",
    "            self.final_linear = nn.Linear(word_gru_hidden, n_classes)\n",
    "            \n",
    "        self.softmax_word = nn.Softmax()\n",
    "        #self.word_gru.data.uniform_(-initrange,initrange)\n",
    "        self.weight_W_word.data.uniform_(-initrange, initrange)\n",
    "        self.weight_proj_word.data.uniform_(-initrange,initrange)\n",
    "        self.bias_word.data.uniform_(-initrange,initrange)\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, embed, state_word):\n",
    "        # embeddings\n",
    "        #print(embed)\n",
    "        embedded = self.drop(self.lookup(embed))\n",
    "        # word level gru\n",
    "        #state_word = self.drop(state_word) #idk\n",
    "        output_word, state_word = self.word_gru(embedded, state_word)\n",
    "        #state_word = self.drop(state_word) #idk\n",
    "        output_word = self.drop(output_word)\n",
    "        #print output_word.size()\n",
    "        word_squish = self.drop(batch_matmul_bias(output_word, self.weight_W_word,self.bias_word, nonlinearity='tanh'))\n",
    "        word_attn = self.drop(batch_matmul(word_squish, self.weight_proj_word))\n",
    "        word_attn_norm = self.drop(self.softmax_word(word_attn.transpose(1,0)))\n",
    "        word_attn_vectors = self.drop(attention_mul(output_word, word_attn_norm.transpose(1,0)))\n",
    "        \n",
    "        #take the average of output (only for non-attention)\n",
    "        #feature_vec = torch.mean(output_word,0)\n",
    "        final_map = self.final_linear(word_attn_vectors.squeeze(0))\n",
    "        return F.log_softmax(final_map), state_word, None\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        if self.bidirectional == True:\n",
    "            return Variable(torch.zeros(2, self.batch_size, self.word_gru_hidden))\n",
    "        else:\n",
    "            return Variable(torch.zeros(1, self.batch_size, self.word_gru_hidden))\n",
    "'''\n",
    "class AttentionSentRNN(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self, batch_size, sent_gru_hidden, word_gru_hidden, n_classes, dropout, bidirectional= True):        \n",
    "        \n",
    "        super(AttentionSentRNN, self).__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.sent_gru_hidden = sent_gru_hidden\n",
    "        self.n_classes = n_classes\n",
    "        self.word_gru_hidden = word_gru_hidden\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "        initrange = 0.1\n",
    "        \n",
    "        \n",
    "        if bidirectional == True:\n",
    "            self.sent_gru = nn.GRU(2 * word_gru_hidden, sent_gru_hidden, bidirectional= True)        \n",
    "            self.weight_W_sent = nn.Parameter(torch.Tensor(2* sent_gru_hidden ,2* sent_gru_hidden))\n",
    "            self.bias_sent = nn.Parameter(torch.Tensor(2* sent_gru_hidden,1))\n",
    "            self.weight_proj_sent = nn.Parameter(torch.Tensor(2* sent_gru_hidden, 1))\n",
    "            self.final_linear = nn.Linear(2* sent_gru_hidden, n_classes)\n",
    "        else:\n",
    "            self.sent_gru = nn.GRU(word_gru_hidden, sent_gru_hidden, bidirectional= True)        \n",
    "            self.weight_W_sent = nn.Parameter(torch.Tensor(sent_gru_hidden ,sent_gru_hidden))\n",
    "            self.bias_sent = nn.Parameter(torch.Tensor(sent_gru_hidden,1))\n",
    "            self.weight_proj_sent = nn.Parameter(torch.Tensor(sent_gru_hidden, 1))\n",
    "            self.final_linear = nn.Linear(sent_gru_hidden, n_classes)\n",
    "        self.softmax_sent = nn.Softmax()\n",
    "        self.final_softmax = nn.Softmax()\n",
    "        self.bias_sent.data.uniform_(-initrange, initrange)\n",
    "        #self.sent_gru.data.uniform_(-initrange,initrange)\n",
    "        self.weight_W_sent.data.uniform_(-initrange, initrange)\n",
    "        self.weight_proj_sent.data.uniform_(-initrange,initrange)\n",
    "        \n",
    "        \n",
    "    def forward(self, word_attention_vectors, state_sent):\n",
    "        #MANUALLY DROPOUT THE GRU\n",
    "        #state_word = self.drop(state_sent)\n",
    "        output_sent, state_sent = self.sent_gru(word_attention_vectors, state_sent)   \n",
    "        #state_word = self.drop(state_sent)\n",
    "        output_sent = self.drop(output_sent)\n",
    "        sent_squish = self.drop(batch_matmul_bias(output_sent, self.weight_W_sent,self.bias_sent, nonlinearity='tanh'))\n",
    "        sent_attn = self.drop(batch_matmul(sent_squish, self.weight_proj_sent))\n",
    "        sent_attn_norm = self.drop(self.softmax_sent(sent_attn.transpose(1,0)))\n",
    "        sent_attn_vectors = self.drop(attention_mul(output_sent, sent_attn_norm.transpose(1,0)))    \n",
    "        # final classifier\n",
    "        final_map = self.final_linear(sent_attn_vectors.squeeze(0))\n",
    "        return F.log_softmax(final_map), state_sent, sent_attn_norm\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        if self.bidirectional == True:\n",
    "            return Variable(torch.zeros(2, self.batch_size, self.sent_gru_hidden))\n",
    "        else:\n",
    "            return Variable(torch.zeros(1, self.batch_size, self.sent_gru_hidden))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import model\n",
    "#import data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import sklearn.metrics\n",
    "\n",
    "dropout=0.5\n",
    "my_batch=64\n",
    "lang='en'\n",
    "datapath = './data/'+lang\n",
    "corpus = Corpus(datapath, lang)\n",
    "ntokens = len(corpus.dictionary)\n",
    "pretrain = corpus.dictionary.pretrain_vec\n",
    "\n",
    "word_attn = AttentionWordRNN(embeds=pretrain, batch_size=my_batch, num_tokens=ntokens, embed_size=300, \n",
    "                             word_gru_hidden=100, dropout=dropout, n_classes=2, bidirectional= True)\n",
    "\n",
    "#sent_attn = AttentionSentRNN(batch_size=my_batch, sent_gru_hidden=100, word_gru_hidden=100, \n",
    "#                             n_classes=2, dropout=dropout, bidirectional= True)\n",
    "\n",
    "def train_data(mini_batch, targets, word_attn_model, word_optimizer, criterion):\n",
    "    state_word = word_attn_model.init_hidden().cuda()\n",
    "    #state_sent = sent_attn_model.init_hidden().cuda()\n",
    "    max_sents, batch_size, max_tokens = mini_batch.size()\n",
    "    word_optimizer.zero_grad()\n",
    "    #sent_optimizer.zero_grad()\n",
    "    #s = None\n",
    "    y_pred = None\n",
    "    for i in range(max_sents):\n",
    "        #torch.cuda.empty_cache()\n",
    "        _s, state_word, _ = word_attn_model(mini_batch[i,:,:].transpose(0,1), state_word) #train ith user\n",
    "        #if(s is None):\n",
    "        if(y_pred is None):\n",
    "            y_pred = _s\n",
    "        else:\n",
    "            y_pred = torch.cat((y_pred,_s),0)            \n",
    "    #y_pred, state_sent, _ = sent_attn_model(s, state_sent)\n",
    "    loss = criterion(y_pred.cuda(), targets)\n",
    "\n",
    "    state_word = None\n",
    "    #state_sent = None\n",
    "    max_sents = None\n",
    "    batch_size = None\n",
    "    max_tokens = None \n",
    "    mini_batch = None\n",
    "    torch.cuda.empty_cache()\n",
    "    loss.backward()\n",
    "    \n",
    "    word_optimizer.step()\n",
    "    #sent_optimizer.step()\n",
    "    \n",
    "    return loss.data.item()\n",
    "\n",
    "\n",
    "\n",
    "def get_predictions(val_tokens, word_attn_model):\n",
    "    max_sents, batch_size, max_tokens = val_tokens.size()\n",
    "    state_word = word_attn_model.init_hidden().cuda()\n",
    "    #state_sent = sent_attn_model.init_hidden().cuda()\n",
    "    s = None\n",
    "    #print(max_sents, max_tokens, \"UHJKSDG\")\n",
    "    for i in range(max_sents):\n",
    "        _s, state_word, _ = word_attn_model(val_tokens[i,:,:].transpose(0,1), state_word)\n",
    "        if(s is None):\n",
    "            s = _s\n",
    "        else:\n",
    "            s = torch.cat((s,_s),0)            \n",
    "    #y_pred, state_sent, _ = sent_attn_model(s, state_sent)    \n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "#learning_rate = 0.001\n",
    "#momentum = 0.9\n",
    "#word_optimizer = torch.optim.SGD(word_attn.parameters(), lr=learning_rate, momentum= momentum)\n",
    "#sent_optimizer = torch.optim.SGD(sent_attn.parameters(), lr=learning_rate, momentum= momentum)\n",
    "word_optimizer = torch.optim.Adam(word_attn.parameters())\n",
    "#sent_optimizer = torch.optim.Adam(sent_attn.parameters())\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "word_attn.cuda()\n",
    "#sent_attn.cuda()\n",
    "\n",
    "\n",
    "\n",
    "def pad_batch(mini_batch):\n",
    "    mini_batch_size = len(mini_batch)\n",
    "    max_sent_len = int(np.mean([len(x) for x in mini_batch]))\n",
    "    max_token_len = int(np.mean([len(val) for sublist in mini_batch for val in sublist]))\n",
    "    main_matrix = np.zeros((mini_batch_size, max_sent_len, max_token_len), dtype= np.int)\n",
    "    for i in range(main_matrix.shape[0]):\n",
    "        for j in range(main_matrix.shape[1]):\n",
    "            for k in range(main_matrix.shape[2]):\n",
    "                try:\n",
    "                    main_matrix[i,j,k] = mini_batch[i][j][k]\n",
    "                except IndexError:\n",
    "                    pass\n",
    "    #return Variable(torch.from_numpy(main_matrix).transpose(0,1))\n",
    "    return Variable(torch.LongTensor(main_matrix).transpose(0,1))\n",
    "\n",
    "\n",
    "\n",
    "def test_accuracy_mini_batch(tokens, labels, word_attn):\n",
    "    y_pred = get_predictions(tokens, word_attn)\n",
    "    #print(\"PRED\",y_pred)\n",
    "    _, y_pred = torch.max(y_pred, 1)\n",
    "    correct = np.ndarray.flatten(y_pred.data.cpu().numpy())\n",
    "    labels = np.ndarray.flatten(labels.data.cpu().numpy())\n",
    "    #print(\"CORR\",correct)\n",
    "    #print(\"LABELS\",labels)\n",
    "    num_correct = sum(correct == labels)\n",
    "    return float(num_correct) / len(correct)\n",
    "\n",
    "def test_accuracy_full_batch(tokens, labels, mini_batch_size, word_attn):\n",
    "    p = []\n",
    "    p_nonlinear = []\n",
    "    l = []\n",
    "    g = gen_minibatch(tokens, labels, mini_batch_size)\n",
    "    for token, label in g:\n",
    "        y_pred = get_predictions(token.cuda(), word_attn)\n",
    "        #print(\"BEFORE\",y_pred)\n",
    "        p_nonlinear.append(np.ndarray.flatten(y_pred[:,1].data.cpu().numpy()))\n",
    "        _, y_pred = torch.max(y_pred, 1)\n",
    "        #print(\"AFTER\",y_pred)\n",
    "        p.append(np.ndarray.flatten(y_pred.data.cpu().numpy()))\n",
    "        l.append(np.ndarray.flatten(label.data.cpu().numpy()))\n",
    "    p = [item for sublist in p for item in sublist]\n",
    "    l = [item for sublist in l for item in sublist]\n",
    "    p_nonlinear = [np.exp(item) for sublist in p_nonlinear for item in sublist]\n",
    "    p = np.array(p)\n",
    "    l = np.array(l)\n",
    "    #print(\"TOKEN LEN\",len(tokens))\n",
    "    #print(\"NONLINEAR\",p_nonlinear)\n",
    "    #print(\"PREDICT\",p)\n",
    "    #print(\"LABEL\",l)\n",
    "    num_correct = sum(p == l)\n",
    "    return float(num_correct)/ len(p), sklearn.metrics.roc_auc_score(l, p_nonlinear)\n",
    "\n",
    "def test_data(mini_batch, targets, word_attn_model):    \n",
    "    state_word = word_attn_model.init_hidden().cuda()\n",
    "    #state_sent = sent_attn_model.init_hidden().cuda()\n",
    "    max_sents, batch_size, max_tokens = mini_batch.size()\n",
    "    s = None\n",
    "    for i in range(max_sents):\n",
    "        _s, state_word, _ = word_attn_model(mini_batch[i,:,:].transpose(0,1), state_word)\n",
    "        if(s is None):\n",
    "            s = _s\n",
    "        else:\n",
    "            s = torch.cat((s,_s),0)            \n",
    "    #y_pred, state_sent,_ = sent_attn_model(s, state_sent)\n",
    "    loss = criterion(s.cuda(), targets)     \n",
    "    return loss.data.item()\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert inputs.shape[0] == targets.shape[0]\n",
    "    if shuffle:\n",
    "        indices = np.arange(inputs.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "    #print(inputs.shape[0] - batchsize+1, batchsize, \"HOO\")\n",
    "    for start_idx in range(0, inputs.shape[0] - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]\n",
    "\n",
    "\n",
    "\n",
    "def gen_minibatch(tokens, labels, mini_batch_size, shuffle= True):\n",
    "    for token, label in iterate_minibatches(tokens, labels, mini_batch_size, shuffle= shuffle):\n",
    "        token = pad_batch(token)\n",
    "        yield token.cuda(), Variable(torch.LongTensor(label), requires_grad= False).cuda()\n",
    "\n",
    "def check_val_loss(val_tokens, val_labels, mini_batch_size, word_attn_model):\n",
    "    val_loss = []\n",
    "    for token, label in iterate_minibatches(val_tokens, val_labels, mini_batch_size, shuffle= True):\n",
    "        val_loss.append(test_data(pad_batch(token).cuda(), Variable(torch.LongTensor(label), requires_grad= False).cuda(), \n",
    "                                  word_attn_model))\n",
    "    return np.mean(val_loss)\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def train_early_stopping(mini_batch_size, X_train, y_train, X_test, y_test, word_attn_model,\n",
    "                         word_attn_optimiser, loss_criterion, num_epoch, \n",
    "                         print_val_loss_every = 1000, print_loss_every = 50):\n",
    "    #for i in word_attn_model.parameters():\n",
    "        #print(i.data, \"PARAM\")\n",
    "    max_eval_acc = 0\n",
    "    max_train_acc = 0\n",
    "    max_eval_aucroc = 0\n",
    "    max_train_aucroc = 0\n",
    "    word_attn_model.train()\n",
    "    #sent_attn_model.train()\n",
    "    start = time.time()\n",
    "    loss_full = []\n",
    "    loss_epoch = []\n",
    "    accuracy_epoch = []\n",
    "    loss_smooth = []\n",
    "    accuracy_full = []\n",
    "    epoch_counter = 0\n",
    "    g = gen_minibatch(X_train, y_train, mini_batch_size)\n",
    "    for i in range(1, num_epoch + 1):\n",
    "        try:\n",
    "            word_attn_model.train()\n",
    "            #sent_attn_model.train()\n",
    "            tokens, labels = next(g)\n",
    "            loss = train_data(tokens, labels, word_attn_model, word_attn_optimiser, loss_criterion)\n",
    "            acc = test_accuracy_mini_batch(tokens, labels, word_attn_model)\n",
    "            accuracy_full.append(acc)\n",
    "            accuracy_epoch.append(acc)\n",
    "            loss_full.append(loss)\n",
    "            loss_epoch.append(loss)\n",
    "            # print loss every n passes\n",
    "            if i % print_loss_every == 0:\n",
    "                print('Loss at %d minibatches, %d epoch,(%s) is %f' %(i, epoch_counter, timeSince(start), np.mean(loss_epoch)))\n",
    "                print('Accuracy at %d minibatches is %f' % (i, np.mean(accuracy_epoch)))\n",
    "            # check validation loss every n passes\n",
    "            if i % print_val_loss_every == 0:\n",
    "                word_attn_model.eval()\n",
    "                #sent_attn_model.eval()\n",
    "                val_loss = check_val_loss(X_test, y_test, mini_batch_size, word_attn_model)\n",
    "                print('Average training loss at this epoch..minibatch..%d..is %f' % (i, np.mean(loss_epoch)))\n",
    "                print('Validation loss after %d passes is %f' %(i, val_loss))\n",
    "                if val_loss > np.mean(loss_full):\n",
    "                    print('Validation loss is higher than training loss at %d is %f , stopping training!' % (i, val_loss))\n",
    "                    print('Average training loss at %d is %f' % (i, np.mean(loss_full)))\n",
    "        except StopIteration:\n",
    "            epoch_counter += 1\n",
    "            print('Reached %d epochs' % epoch_counter)\n",
    "            print('i %d' % i)\n",
    "            word_attn_model.eval()\n",
    "            #sent_attn_model.eval()\n",
    "            acc, aucroc = test_accuracy_full_batch(corpus.X_test, corpus.y_test, my_batch, word_attn)\n",
    "            if acc>max_eval_acc:\n",
    "                max_eval_acc = acc\n",
    "            if aucroc>max_eval_aucroc:\n",
    "                max_eval_aucroc = aucroc\n",
    "            print(\"Test accuracy:\",acc)\n",
    "            print(\"Max test accruacy:\",max_eval_acc)\n",
    "            print(\"Test aucroc:\",aucroc)\n",
    "            print(\"Max test aucroc:\",max_eval_aucroc)\n",
    "            word_attn_model.train()\n",
    "            #sent_attn_model.train()\n",
    "            acc, aucroc = test_accuracy_full_batch(corpus.X_train, corpus.y_train, my_batch, word_attn)\n",
    "            if acc>max_train_acc:\n",
    "                max_train_acc = acc\n",
    "            if aucroc>max_train_aucroc:\n",
    "                max_train_aucroc = aucroc\n",
    "            print(\"Train accuracy:\",acc)\n",
    "            print(\"Max train accruacy:\",max_train_acc)\n",
    "            print(\"Train aucroc:\",aucroc)\n",
    "            print(\"Max train aucroc:\",max_train_aucroc)\n",
    "            #if epoch_counter == 1:\n",
    "                #break\n",
    "            g = gen_minibatch(X_train, y_train, mini_batch_size)\n",
    "            loss_epoch = []\n",
    "            accuracy_epoch = []\n",
    "            if epoch_counter==20:\n",
    "                break\n",
    "    return loss_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:163: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:169: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at 1 minibatches, 0 epoch,(0m 1s) is 0.675891\n",
      "Accuracy at 1 minibatches is 0.671875\n",
      "Average training loss at this epoch..minibatch..1..is 0.675891\n",
      "Validation loss after 1 passes is 0.677166\n",
      "Validation loss is higher than training loss at 1 is 0.677166 , stopping training!\n",
      "Average training loss at 1 is 0.675891\n",
      "Loss at 2 minibatches, 0 epoch,(0m 2s) is 0.666181\n",
      "Accuracy at 2 minibatches is 0.671875\n",
      "Average training loss at this epoch..minibatch..2..is 0.666181\n",
      "Validation loss after 2 passes is 0.673878\n",
      "Validation loss is higher than training loss at 2 is 0.673878 , stopping training!\n",
      "Average training loss at 2 is 0.666181\n",
      "Loss at 3 minibatches, 0 epoch,(0m 3s) is 0.646948\n",
      "Accuracy at 3 minibatches is 0.697917\n",
      "Average training loss at this epoch..minibatch..3..is 0.646948\n",
      "Validation loss after 3 passes is 0.665389\n",
      "Validation loss is higher than training loss at 3 is 0.665389 , stopping training!\n",
      "Average training loss at 3 is 0.646948\n",
      "Loss at 4 minibatches, 0 epoch,(0m 5s) is 0.630977\n",
      "Accuracy at 4 minibatches is 0.714844\n",
      "Average training loss at this epoch..minibatch..4..is 0.630977\n",
      "Validation loss after 4 passes is 0.702725\n",
      "Validation loss is higher than training loss at 4 is 0.702725 , stopping training!\n",
      "Average training loss at 4 is 0.630977\n",
      "Loss at 5 minibatches, 0 epoch,(0m 6s) is 0.635517\n",
      "Accuracy at 5 minibatches is 0.696875\n",
      "Average training loss at this epoch..minibatch..5..is 0.635517\n",
      "Validation loss after 5 passes is 0.692515\n",
      "Validation loss is higher than training loss at 5 is 0.692515 , stopping training!\n",
      "Average training loss at 5 is 0.635517\n",
      "Loss at 6 minibatches, 0 epoch,(0m 7s) is 0.623282\n",
      "Accuracy at 6 minibatches is 0.705729\n",
      "Average training loss at this epoch..minibatch..6..is 0.623282\n",
      "Validation loss after 6 passes is 0.735796\n",
      "Validation loss is higher than training loss at 6 is 0.735796 , stopping training!\n",
      "Average training loss at 6 is 0.623282\n",
      "Loss at 7 minibatches, 0 epoch,(0m 8s) is 0.621259\n",
      "Accuracy at 7 minibatches is 0.705357\n",
      "Average training loss at this epoch..minibatch..7..is 0.621259\n",
      "Validation loss after 7 passes is 0.734619\n",
      "Validation loss is higher than training loss at 7 is 0.734619 , stopping training!\n",
      "Average training loss at 7 is 0.621259\n",
      "Loss at 8 minibatches, 0 epoch,(0m 10s) is 0.619901\n",
      "Accuracy at 8 minibatches is 0.705078\n",
      "Average training loss at this epoch..minibatch..8..is 0.619901\n",
      "Validation loss after 8 passes is 0.727367\n",
      "Validation loss is higher than training loss at 8 is 0.727367 , stopping training!\n",
      "Average training loss at 8 is 0.619901\n",
      "Loss at 9 minibatches, 0 epoch,(0m 11s) is 0.613241\n",
      "Accuracy at 9 minibatches is 0.710069\n",
      "Average training loss at this epoch..minibatch..9..is 0.613241\n",
      "Validation loss after 9 passes is 0.675822\n",
      "Validation loss is higher than training loss at 9 is 0.675822 , stopping training!\n",
      "Average training loss at 9 is 0.613241\n",
      "Reached 1 epochs\n",
      "i 10\n",
      "Test accuracy: 0.578125\n",
      "Max test accruacy: 0.578125\n",
      "Test aucroc: 0.7937937937937938\n",
      "Max test aucroc: 0.7937937937937938\n",
      "Train accuracy: 0.7100694444444444\n",
      "Max train accruacy: 0.7100694444444444\n",
      "Train aucroc: 0.6794446076990891\n",
      "Max train aucroc: 0.6794446076990891\n",
      "Loss at 11 minibatches, 1 epoch,(0m 14s) is 0.489342\n",
      "Accuracy at 11 minibatches is 0.812500\n",
      "Average training loss at this epoch..minibatch..11..is 0.489342\n",
      "Validation loss after 11 passes is 0.700951\n",
      "Validation loss is higher than training loss at 11 is 0.700951 , stopping training!\n",
      "Average training loss at 11 is 0.600851\n",
      "Loss at 12 minibatches, 1 epoch,(0m 15s) is 0.547320\n",
      "Accuracy at 12 minibatches is 0.750000\n",
      "Average training loss at this epoch..minibatch..12..is 0.547320\n",
      "Validation loss after 12 passes is 0.613967\n",
      "Validation loss is higher than training loss at 12 is 0.613967 , stopping training!\n",
      "Average training loss at 12 is 0.601255\n",
      "Loss at 13 minibatches, 1 epoch,(0m 17s) is 0.586072\n",
      "Accuracy at 13 minibatches is 0.713542\n",
      "Average training loss at this epoch..minibatch..13..is 0.586072\n",
      "Validation loss after 13 passes is 0.617776\n",
      "Validation loss is higher than training loss at 13 is 0.617776 , stopping training!\n",
      "Average training loss at 13 is 0.606448\n",
      "Loss at 14 minibatches, 1 epoch,(0m 18s) is 0.587873\n",
      "Accuracy at 14 minibatches is 0.691406\n",
      "Average training loss at this epoch..minibatch..14..is 0.587873\n",
      "Validation loss after 14 passes is 0.604475\n",
      "Loss at 15 minibatches, 1 epoch,(0m 19s) is 0.579983\n",
      "Accuracy at 15 minibatches is 0.693750\n",
      "Average training loss at this epoch..minibatch..15..is 0.579983\n",
      "Validation loss after 15 passes is 0.557125\n",
      "Loss at 16 minibatches, 1 epoch,(0m 20s) is 0.574144\n",
      "Accuracy at 16 minibatches is 0.697917\n",
      "Average training loss at this epoch..minibatch..16..is 0.574144\n",
      "Validation loss after 16 passes is 0.512241\n",
      "Loss at 17 minibatches, 1 epoch,(0m 22s) is 0.571044\n",
      "Accuracy at 17 minibatches is 0.703125\n",
      "Average training loss at this epoch..minibatch..17..is 0.571044\n",
      "Validation loss after 17 passes is 0.501713\n",
      "Loss at 18 minibatches, 1 epoch,(0m 23s) is 0.563034\n",
      "Accuracy at 18 minibatches is 0.708984\n",
      "Average training loss at this epoch..minibatch..18..is 0.563034\n",
      "Validation loss after 18 passes is 0.493508\n",
      "Loss at 19 minibatches, 1 epoch,(0m 24s) is 0.560341\n",
      "Accuracy at 19 minibatches is 0.711806\n",
      "Average training loss at this epoch..minibatch..19..is 0.560341\n",
      "Validation loss after 19 passes is 0.513319\n",
      "Reached 2 epochs\n",
      "i 20\n",
      "Test accuracy: 0.828125\n",
      "Max test accruacy: 0.828125\n",
      "Test aucroc: 0.9020833333333332\n",
      "Max test aucroc: 0.9020833333333332\n",
      "Train accuracy: 0.7100694444444444\n",
      "Max train accruacy: 0.7100694444444444\n",
      "Train aucroc: 0.755302909149063\n",
      "Max train aucroc: 0.755302909149063\n",
      "Loss at 21 minibatches, 2 epoch,(0m 27s) is 0.565341\n",
      "Accuracy at 21 minibatches is 0.703125\n",
      "Average training loss at this epoch..minibatch..21..is 0.565341\n",
      "Validation loss after 21 passes is 0.528599\n",
      "Loss at 22 minibatches, 2 epoch,(0m 29s) is 0.518867\n",
      "Accuracy at 22 minibatches is 0.710938\n",
      "Average training loss at this epoch..minibatch..22..is 0.518867\n",
      "Validation loss after 22 passes is 0.570557\n",
      "Loss at 23 minibatches, 2 epoch,(0m 30s) is 0.529277\n",
      "Accuracy at 23 minibatches is 0.713542\n",
      "Average training loss at this epoch..minibatch..23..is 0.529277\n",
      "Validation loss after 23 passes is 0.631134\n",
      "Validation loss is higher than training loss at 23 is 0.631134 , stopping training!\n",
      "Average training loss at 23 is 0.578575\n",
      "Loss at 24 minibatches, 2 epoch,(0m 31s) is 0.517149\n",
      "Accuracy at 24 minibatches is 0.722656\n",
      "Average training loss at this epoch..minibatch..24..is 0.517149\n",
      "Validation loss after 24 passes is 0.613544\n",
      "Validation loss is higher than training loss at 24 is 0.613544 , stopping training!\n",
      "Average training loss at 24 is 0.574129\n",
      "Loss at 25 minibatches, 2 epoch,(0m 33s) is 0.507943\n",
      "Accuracy at 25 minibatches is 0.728125\n",
      "Average training loss at this epoch..minibatch..25..is 0.507943\n",
      "Validation loss after 25 passes is 0.648551\n",
      "Validation loss is higher than training loss at 25 is 0.648551 , stopping training!\n",
      "Average training loss at 25 is 0.569650\n",
      "Loss at 26 minibatches, 2 epoch,(0m 34s) is 0.526021\n",
      "Accuracy at 26 minibatches is 0.705729\n",
      "Average training loss at this epoch..minibatch..26..is 0.526021\n",
      "Validation loss after 26 passes is 0.642550\n",
      "Validation loss is higher than training loss at 26 is 0.642550 , stopping training!\n",
      "Average training loss at 26 is 0.571598\n",
      "Loss at 27 minibatches, 2 epoch,(0m 35s) is 0.521478\n",
      "Accuracy at 27 minibatches is 0.714286\n",
      "Average training loss at this epoch..minibatch..27..is 0.521478\n",
      "Validation loss after 27 passes is 0.647585\n",
      "Validation loss is higher than training loss at 27 is 0.647585 , stopping training!\n",
      "Average training loss at 27 is 0.568503\n",
      "Loss at 28 minibatches, 2 epoch,(0m 36s) is 0.526354\n",
      "Accuracy at 28 minibatches is 0.710938\n",
      "Average training loss at this epoch..minibatch..28..is 0.526354\n",
      "Validation loss after 28 passes is 0.652518\n",
      "Validation loss is higher than training loss at 28 is 0.652518 , stopping training!\n",
      "Average training loss at 28 is 0.568195\n",
      "Loss at 29 minibatches, 2 epoch,(0m 38s) is 0.527040\n",
      "Accuracy at 29 minibatches is 0.713542\n",
      "Average training loss at this epoch..minibatch..29..is 0.527040\n",
      "Validation loss after 29 passes is 0.634526\n",
      "Validation loss is higher than training loss at 29 is 0.634526 , stopping training!\n",
      "Average training loss at 29 is 0.566874\n",
      "Reached 3 epochs\n",
      "i 30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.578125\n",
      "Max test accruacy: 0.828125\n",
      "Test aucroc: 0.890890890890891\n",
      "Max test aucroc: 0.9020833333333332\n",
      "Train accuracy: 0.7256944444444444\n",
      "Max train accruacy: 0.7256944444444444\n",
      "Train aucroc: 0.7697061584001155\n",
      "Max train aucroc: 0.7697061584001155\n",
      "Loss at 31 minibatches, 3 epoch,(0m 41s) is 0.602595\n",
      "Accuracy at 31 minibatches is 0.750000\n",
      "Average training loss at this epoch..minibatch..31..is 0.602595\n",
      "Validation loss after 31 passes is 0.604774\n",
      "Validation loss is higher than training loss at 31 is 0.604774 , stopping training!\n",
      "Average training loss at 31 is 0.568150\n",
      "Loss at 32 minibatches, 3 epoch,(0m 42s) is 0.530447\n",
      "Accuracy at 32 minibatches is 0.796875\n",
      "Average training loss at this epoch..minibatch..32..is 0.530447\n",
      "Validation loss after 32 passes is 0.561377\n",
      "Loss at 33 minibatches, 3 epoch,(0m 43s) is 0.523794\n",
      "Accuracy at 33 minibatches is 0.760417\n",
      "Average training loss at this epoch..minibatch..33..is 0.523794\n",
      "Validation loss after 33 passes is 0.583567\n",
      "Validation loss is higher than training loss at 33 is 0.583567 , stopping training!\n",
      "Average training loss at 33 is 0.562566\n",
      "Loss at 34 minibatches, 3 epoch,(0m 44s) is 0.512553\n",
      "Accuracy at 34 minibatches is 0.765625\n",
      "Average training loss at this epoch..minibatch..34..is 0.512553\n",
      "Validation loss after 34 passes is 0.532800\n",
      "Loss at 35 minibatches, 3 epoch,(0m 46s) is 0.491660\n",
      "Accuracy at 35 minibatches is 0.778125\n",
      "Average training loss at this epoch..minibatch..35..is 0.491660\n",
      "Validation loss after 35 passes is 0.475783\n",
      "Loss at 36 minibatches, 3 epoch,(0m 47s) is 0.502176\n",
      "Accuracy at 36 minibatches is 0.757812\n",
      "Average training loss at this epoch..minibatch..36..is 0.502176\n",
      "Validation loss after 36 passes is 0.509704\n",
      "Loss at 37 minibatches, 3 epoch,(0m 48s) is 0.513971\n",
      "Accuracy at 37 minibatches is 0.747768\n",
      "Average training loss at this epoch..minibatch..37..is 0.513971\n",
      "Validation loss after 37 passes is 0.526082\n",
      "Loss at 38 minibatches, 3 epoch,(0m 50s) is 0.510934\n",
      "Accuracy at 38 minibatches is 0.748047\n",
      "Average training loss at this epoch..minibatch..38..is 0.510934\n",
      "Validation loss after 38 passes is 0.527517\n",
      "Loss at 39 minibatches, 3 epoch,(0m 51s) is 0.517763\n",
      "Accuracy at 39 minibatches is 0.743056\n",
      "Average training loss at this epoch..minibatch..39..is 0.517763\n",
      "Validation loss after 39 passes is 0.555562\n",
      "Validation loss is higher than training loss at 39 is 0.555562 , stopping training!\n",
      "Average training loss at 39 is 0.554596\n",
      "Reached 4 epochs\n",
      "i 40\n",
      "Test accuracy: 0.765625\n",
      "Max test accruacy: 0.828125\n",
      "Test aucroc: 0.9456410256410257\n",
      "Max test aucroc: 0.9456410256410257\n",
      "Train accuracy: 0.7430555555555556\n",
      "Max train accruacy: 0.7430555555555556\n",
      "Train aucroc: 0.7940939636619182\n",
      "Max train aucroc: 0.7940939636619182\n",
      "Loss at 41 minibatches, 4 epoch,(0m 54s) is 0.508253\n",
      "Accuracy at 41 minibatches is 0.734375\n",
      "Average training loss at this epoch..minibatch..41..is 0.508253\n",
      "Validation loss after 41 passes is 0.561801\n",
      "Validation loss is higher than training loss at 41 is 0.561801 , stopping training!\n",
      "Average training loss at 41 is 0.553344\n",
      "Loss at 42 minibatches, 4 epoch,(0m 55s) is 0.478559\n",
      "Accuracy at 42 minibatches is 0.734375\n",
      "Average training loss at this epoch..minibatch..42..is 0.478559\n",
      "Validation loss after 42 passes is 0.551801\n",
      "Validation loss is higher than training loss at 42 is 0.551801 , stopping training!\n",
      "Average training loss at 42 is 0.550594\n",
      "Loss at 43 minibatches, 4 epoch,(0m 57s) is 0.473431\n",
      "Accuracy at 43 minibatches is 0.734375\n",
      "Average training loss at this epoch..minibatch..43..is 0.473431\n",
      "Validation loss after 43 passes is 0.586633\n",
      "Validation loss is higher than training loss at 43 is 0.586633 , stopping training!\n",
      "Average training loss at 43 is 0.548353\n",
      "Loss at 44 minibatches, 4 epoch,(0m 58s) is 0.502795\n",
      "Accuracy at 44 minibatches is 0.734375\n",
      "Average training loss at this epoch..minibatch..44..is 0.502795\n",
      "Validation loss after 44 passes is 0.569289\n",
      "Validation loss is higher than training loss at 44 is 0.569289 , stopping training!\n",
      "Average training loss at 44 is 0.549416\n",
      "Loss at 45 minibatches, 4 epoch,(0m 59s) is 0.475220\n",
      "Accuracy at 45 minibatches is 0.762500\n",
      "Average training loss at this epoch..minibatch..45..is 0.475220\n",
      "Validation loss after 45 passes is 0.567643\n",
      "Validation loss is higher than training loss at 45 is 0.567643 , stopping training!\n",
      "Average training loss at 45 is 0.544916\n",
      "Loss at 46 minibatches, 4 epoch,(1m 0s) is 0.468577\n",
      "Accuracy at 46 minibatches is 0.765625\n",
      "Average training loss at this epoch..minibatch..46..is 0.468577\n",
      "Validation loss after 46 passes is 0.573858\n",
      "Validation loss is higher than training loss at 46 is 0.573858 , stopping training!\n",
      "Average training loss at 46 is 0.542308\n",
      "Loss at 47 minibatches, 4 epoch,(1m 2s) is 0.471535\n",
      "Accuracy at 47 minibatches is 0.754464\n",
      "Average training loss at this epoch..minibatch..47..is 0.471535\n",
      "Validation loss after 47 passes is 0.595235\n",
      "Validation loss is higher than training loss at 47 is 0.595235 , stopping training!\n",
      "Average training loss at 47 is 0.541075\n",
      "Loss at 48 minibatches, 4 epoch,(1m 3s) is 0.480822\n",
      "Accuracy at 48 minibatches is 0.748047\n",
      "Average training loss at this epoch..minibatch..48..is 0.480822\n",
      "Validation loss after 48 passes is 0.566959\n",
      "Validation loss is higher than training loss at 48 is 0.566959 , stopping training!\n",
      "Average training loss at 48 is 0.541183\n",
      "Loss at 49 minibatches, 4 epoch,(1m 4s) is 0.473333\n",
      "Accuracy at 49 minibatches is 0.753472\n",
      "Average training loss at this epoch..minibatch..49..is 0.473333\n",
      "Validation loss after 49 passes is 0.516373\n",
      "Reached 5 epochs\n",
      "i 50\n",
      "Test accuracy: 0.6875\n",
      "Max test accruacy: 0.828125\n",
      "Test aucroc: 0.922922922922923\n",
      "Max test aucroc: 0.9456410256410257\n",
      "Train accuracy: 0.7777777777777778\n",
      "Max train accruacy: 0.7777777777777778\n",
      "Train aucroc: 0.8533358533358533\n",
      "Max train aucroc: 0.8533358533358533\n",
      "Loss at 51 minibatches, 5 epoch,(1m 7s) is 0.397149\n",
      "Accuracy at 51 minibatches is 0.750000\n",
      "Average training loss at this epoch..minibatch..51..is 0.397149\n",
      "Validation loss after 51 passes is 0.510737\n",
      "Loss at 52 minibatches, 5 epoch,(1m 9s) is 0.451434\n",
      "Accuracy at 52 minibatches is 0.742188\n",
      "Average training loss at this epoch..minibatch..52..is 0.451434\n",
      "Validation loss after 52 passes is 0.514158\n",
      "Loss at 53 minibatches, 5 epoch,(1m 10s) is 0.424169\n",
      "Accuracy at 53 minibatches is 0.770833\n",
      "Average training loss at this epoch..minibatch..53..is 0.424169\n",
      "Validation loss after 53 passes is 0.492368\n",
      "Loss at 54 minibatches, 5 epoch,(1m 11s) is 0.409600\n",
      "Accuracy at 54 minibatches is 0.789062\n",
      "Average training loss at this epoch..minibatch..54..is 0.409600\n",
      "Validation loss after 54 passes is 0.496069\n",
      "Loss at 55 minibatches, 5 epoch,(1m 13s) is 0.417763\n",
      "Accuracy at 55 minibatches is 0.796875\n",
      "Average training loss at this epoch..minibatch..55..is 0.417763\n",
      "Validation loss after 55 passes is 0.457402\n",
      "Loss at 56 minibatches, 5 epoch,(1m 14s) is 0.421246\n",
      "Accuracy at 56 minibatches is 0.796875\n",
      "Average training loss at this epoch..minibatch..56..is 0.421246\n",
      "Validation loss after 56 passes is 0.433406\n",
      "Loss at 57 minibatches, 5 epoch,(1m 15s) is 0.420646\n",
      "Accuracy at 57 minibatches is 0.794643\n",
      "Average training loss at this epoch..minibatch..57..is 0.420646\n",
      "Validation loss after 57 passes is 0.433602\n",
      "Loss at 58 minibatches, 5 epoch,(1m 16s) is 0.416239\n",
      "Accuracy at 58 minibatches is 0.802734\n",
      "Average training loss at this epoch..minibatch..58..is 0.416239\n",
      "Validation loss after 58 passes is 0.435165\n",
      "Loss at 59 minibatches, 5 epoch,(1m 18s) is 0.422264\n",
      "Accuracy at 59 minibatches is 0.800347\n",
      "Average training loss at this epoch..minibatch..59..is 0.422264\n",
      "Validation loss after 59 passes is 0.393316\n",
      "Reached 6 epochs\n",
      "i 60\n",
      "Test accuracy: 0.8125\n",
      "Max test accruacy: 0.828125\n",
      "Test aucroc: 0.9623015873015872\n",
      "Max test aucroc: 0.9623015873015872\n",
      "Train accuracy: 0.8385416666666666\n",
      "Max train accruacy: 0.8385416666666666\n",
      "Train aucroc: 0.9170579939810709\n",
      "Max train aucroc: 0.9170579939810709\n",
      "Loss at 61 minibatches, 6 epoch,(1m 21s) is 0.437083\n",
      "Accuracy at 61 minibatches is 0.765625\n",
      "Average training loss at this epoch..minibatch..61..is 0.437083\n",
      "Validation loss after 61 passes is 0.378699\n",
      "Loss at 62 minibatches, 6 epoch,(1m 22s) is 0.460083\n",
      "Accuracy at 62 minibatches is 0.796875\n",
      "Average training loss at this epoch..minibatch..62..is 0.460083\n",
      "Validation loss after 62 passes is 0.398282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at 63 minibatches, 6 epoch,(1m 23s) is 0.452768\n",
      "Accuracy at 63 minibatches is 0.807292\n",
      "Average training loss at this epoch..minibatch..63..is 0.452768\n",
      "Validation loss after 63 passes is 0.361183\n",
      "Loss at 64 minibatches, 6 epoch,(1m 24s) is 0.413320\n",
      "Accuracy at 64 minibatches is 0.835938\n",
      "Average training loss at this epoch..minibatch..64..is 0.413320\n",
      "Validation loss after 64 passes is 0.329434\n",
      "Loss at 65 minibatches, 6 epoch,(1m 26s) is 0.392932\n",
      "Accuracy at 65 minibatches is 0.856250\n",
      "Average training loss at this epoch..minibatch..65..is 0.392932\n",
      "Validation loss after 65 passes is 0.322961\n",
      "Loss at 66 minibatches, 6 epoch,(1m 27s) is 0.371830\n",
      "Accuracy at 66 minibatches is 0.864583\n",
      "Average training loss at this epoch..minibatch..66..is 0.371830\n",
      "Validation loss after 66 passes is 0.426804\n",
      "Loss at 67 minibatches, 6 epoch,(1m 28s) is 0.344281\n",
      "Accuracy at 67 minibatches is 0.881696\n",
      "Average training loss at this epoch..minibatch..67..is 0.344281\n",
      "Validation loss after 67 passes is 0.523657\n",
      "Validation loss is higher than training loss at 67 is 0.523657 , stopping training!\n",
      "Average training loss at 67 is 0.498948\n",
      "Loss at 68 minibatches, 6 epoch,(1m 30s) is 0.324880\n",
      "Accuracy at 68 minibatches is 0.890625\n",
      "Average training loss at this epoch..minibatch..68..is 0.324880\n",
      "Validation loss after 68 passes is 0.518221\n",
      "Validation loss is higher than training loss at 68 is 0.518221 , stopping training!\n",
      "Average training loss at 68 is 0.493950\n",
      "Loss at 69 minibatches, 6 epoch,(1m 31s) is 0.313727\n",
      "Accuracy at 69 minibatches is 0.894097\n",
      "Average training loss at this epoch..minibatch..69..is 0.313727\n",
      "Validation loss after 69 passes is 0.578182\n",
      "Validation loss is higher than training loss at 69 is 0.578182 , stopping training!\n",
      "Average training loss at 69 is 0.489673\n",
      "Reached 7 epochs\n",
      "i 70\n",
      "Test accuracy: 0.828125\n",
      "Max test accruacy: 0.828125\n",
      "Test aucroc: 0.9392712550607286\n",
      "Max test aucroc: 0.9623015873015872\n",
      "Train accuracy: 0.9427083333333334\n",
      "Max train accruacy: 0.9427083333333334\n",
      "Train aucroc: 0.9584190420217454\n",
      "Max train aucroc: 0.9584190420217454\n",
      "Loss at 71 minibatches, 7 epoch,(1m 34s) is 0.185630\n",
      "Accuracy at 71 minibatches is 0.953125\n",
      "Average training loss at this epoch..minibatch..71..is 0.185630\n",
      "Validation loss after 71 passes is 0.526133\n",
      "Validation loss is higher than training loss at 71 is 0.526133 , stopping training!\n",
      "Average training loss at 71 is 0.484922\n",
      "Loss at 72 minibatches, 7 epoch,(1m 36s) is 0.193567\n",
      "Accuracy at 72 minibatches is 0.945312\n",
      "Average training loss at this epoch..minibatch..72..is 0.193567\n",
      "Validation loss after 72 passes is 0.389083\n",
      "Loss at 73 minibatches, 7 epoch,(1m 37s) is 0.161628\n",
      "Accuracy at 73 minibatches is 0.947917\n",
      "Average training loss at this epoch..minibatch..73..is 0.161628\n",
      "Validation loss after 73 passes is 0.390708\n",
      "Loss at 74 minibatches, 7 epoch,(1m 38s) is 0.158792\n",
      "Accuracy at 74 minibatches is 0.945312\n",
      "Average training loss at this epoch..minibatch..74..is 0.158792\n",
      "Validation loss after 74 passes is 0.347461\n",
      "Loss at 75 minibatches, 7 epoch,(1m 39s) is 0.172335\n",
      "Accuracy at 75 minibatches is 0.943750\n",
      "Average training loss at this epoch..minibatch..75..is 0.172335\n",
      "Validation loss after 75 passes is 0.415128\n",
      "Loss at 76 minibatches, 7 epoch,(1m 41s) is 0.189267\n",
      "Accuracy at 76 minibatches is 0.940104\n",
      "Average training loss at this epoch..minibatch..76..is 0.189267\n",
      "Validation loss after 76 passes is 0.453937\n",
      "Loss at 77 minibatches, 7 epoch,(1m 42s) is 0.176411\n",
      "Accuracy at 77 minibatches is 0.946429\n",
      "Average training loss at this epoch..minibatch..77..is 0.176411\n",
      "Validation loss after 77 passes is 0.473444\n",
      "Validation loss is higher than training loss at 77 is 0.473444 , stopping training!\n",
      "Average training loss at 77 is 0.458347\n",
      "Loss at 78 minibatches, 7 epoch,(1m 43s) is 0.168183\n",
      "Accuracy at 78 minibatches is 0.951172\n",
      "Average training loss at this epoch..minibatch..78..is 0.168183\n",
      "Validation loss after 78 passes is 0.558287\n",
      "Validation loss is higher than training loss at 78 is 0.558287 , stopping training!\n",
      "Average training loss at 78 is 0.453448\n",
      "Loss at 79 minibatches, 7 epoch,(1m 44s) is 0.185663\n",
      "Accuracy at 79 minibatches is 0.951389\n",
      "Average training loss at this epoch..minibatch..79..is 0.185663\n",
      "Validation loss after 79 passes is 0.811308\n",
      "Validation loss is higher than training loss at 79 is 0.811308 , stopping training!\n",
      "Average training loss at 79 is 0.451671\n",
      "Reached 8 epochs\n",
      "i 80\n",
      "Test accuracy: 0.828125\n",
      "Max test accruacy: 0.828125\n",
      "Test aucroc: 0.9574898785425101\n",
      "Max test aucroc: 0.9623015873015872\n",
      "Train accuracy: 0.9739583333333334\n",
      "Max train accruacy: 0.9739583333333334\n",
      "Train aucroc: 0.9754530358818857\n",
      "Max train aucroc: 0.9754530358818857\n",
      "Loss at 81 minibatches, 8 epoch,(1m 48s) is 0.045048\n",
      "Accuracy at 81 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..81..is 0.045048\n",
      "Validation loss after 81 passes is 0.854606\n",
      "Validation loss is higher than training loss at 81 is 0.854606 , stopping training!\n",
      "Average training loss at 81 is 0.446101\n",
      "Loss at 82 minibatches, 8 epoch,(1m 49s) is 0.123254\n",
      "Accuracy at 82 minibatches is 0.984375\n",
      "Average training loss at this epoch..minibatch..82..is 0.123254\n",
      "Validation loss after 82 passes is 0.807899\n",
      "Validation loss is higher than training loss at 82 is 0.807899 , stopping training!\n",
      "Average training loss at 82 is 0.442795\n",
      "Loss at 83 minibatches, 8 epoch,(1m 50s) is 0.156858\n",
      "Accuracy at 83 minibatches is 0.979167\n",
      "Average training loss at this epoch..minibatch..83..is 0.156858\n",
      "Validation loss after 83 passes is 0.947305\n",
      "Validation loss is higher than training loss at 83 is 0.947305 , stopping training!\n",
      "Average training loss at 83 is 0.439879\n",
      "Loss at 84 minibatches, 8 epoch,(1m 51s) is 0.182135\n",
      "Accuracy at 84 minibatches is 0.976562\n",
      "Average training loss at this epoch..minibatch..84..is 0.182135\n",
      "Validation loss after 84 passes is 0.729472\n",
      "Validation loss is higher than training loss at 84 is 0.729472 , stopping training!\n",
      "Average training loss at 84 is 0.437485\n",
      "Loss at 85 minibatches, 8 epoch,(1m 53s) is 0.175058\n",
      "Accuracy at 85 minibatches is 0.978125\n",
      "Average training loss at this epoch..minibatch..85..is 0.175058\n",
      "Validation loss after 85 passes is 0.872884\n",
      "Validation loss is higher than training loss at 85 is 0.872884 , stopping training!\n",
      "Average training loss at 85 is 0.433710\n",
      "Loss at 86 minibatches, 8 epoch,(1m 54s) is 0.188515\n",
      "Accuracy at 86 minibatches is 0.973958\n",
      "Average training loss at this epoch..minibatch..86..is 0.188515\n",
      "Validation loss after 86 passes is 0.738782\n",
      "Validation loss is higher than training loss at 86 is 0.738782 , stopping training!\n",
      "Average training loss at 86 is 0.431429\n",
      "Loss at 87 minibatches, 8 epoch,(1m 55s) is 0.176804\n",
      "Accuracy at 87 minibatches is 0.975446\n",
      "Average training loss at this epoch..minibatch..87..is 0.176804\n",
      "Validation loss after 87 passes is 0.515232\n",
      "Validation loss is higher than training loss at 87 is 0.515232 , stopping training!\n",
      "Average training loss at 87 is 0.427316\n",
      "Loss at 88 minibatches, 8 epoch,(1m 56s) is 0.169124\n",
      "Accuracy at 88 minibatches is 0.974609\n",
      "Average training loss at this epoch..minibatch..88..is 0.169124\n",
      "Validation loss after 88 passes is 0.495794\n",
      "Validation loss is higher than training loss at 88 is 0.495794 , stopping training!\n",
      "Average training loss at 88 is 0.423417\n",
      "Loss at 89 minibatches, 8 epoch,(1m 58s) is 0.154752\n",
      "Accuracy at 89 minibatches is 0.975694\n",
      "Average training loss at this epoch..minibatch..89..is 0.154752\n",
      "Validation loss after 89 passes is 0.429115\n",
      "Validation loss is higher than training loss at 89 is 0.429115 , stopping training!\n",
      "Average training loss at 89 is 0.418680\n",
      "Reached 9 epochs\n",
      "i 90\n",
      "Test accuracy: 0.875\n",
      "Max test accruacy: 0.875\n",
      "Test aucroc: 0.9443319838056681\n",
      "Max test aucroc: 0.9623015873015872\n",
      "Train accuracy: 0.9756944444444444\n",
      "Max train accruacy: 0.9756944444444444\n",
      "Train aucroc: 0.9846328307866768\n",
      "Max train aucroc: 0.9846328307866768\n",
      "Loss at 91 minibatches, 9 epoch,(2m 1s) is 0.111347\n",
      "Accuracy at 91 minibatches is 0.968750\n",
      "Average training loss at this epoch..minibatch..91..is 0.111347\n",
      "Validation loss after 91 passes is 0.351949\n",
      "Loss at 92 minibatches, 9 epoch,(2m 2s) is 0.087758\n",
      "Accuracy at 92 minibatches is 0.968750\n",
      "Average training loss at this epoch..minibatch..92..is 0.087758\n",
      "Validation loss after 92 passes is 0.374177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at 93 minibatches, 9 epoch,(2m 3s) is 0.105242\n",
      "Accuracy at 93 minibatches is 0.973958\n",
      "Average training loss at this epoch..minibatch..93..is 0.105242\n",
      "Validation loss after 93 passes is 0.387151\n",
      "Loss at 94 minibatches, 9 epoch,(2m 5s) is 0.105788\n",
      "Accuracy at 94 minibatches is 0.972656\n",
      "Average training loss at this epoch..minibatch..94..is 0.105788\n",
      "Validation loss after 94 passes is 0.373160\n",
      "Loss at 95 minibatches, 9 epoch,(2m 6s) is 0.111424\n",
      "Accuracy at 95 minibatches is 0.971875\n",
      "Average training loss at this epoch..minibatch..95..is 0.111424\n",
      "Validation loss after 95 passes is 0.335475\n",
      "Loss at 96 minibatches, 9 epoch,(2m 7s) is 0.117337\n",
      "Accuracy at 96 minibatches is 0.973958\n",
      "Average training loss at this epoch..minibatch..96..is 0.117337\n",
      "Validation loss after 96 passes is 0.390301\n",
      "Loss at 97 minibatches, 9 epoch,(2m 8s) is 0.105575\n",
      "Accuracy at 97 minibatches is 0.977679\n",
      "Average training loss at this epoch..minibatch..97..is 0.105575\n",
      "Validation loss after 97 passes is 0.253214\n",
      "Loss at 98 minibatches, 9 epoch,(2m 10s) is 0.110026\n",
      "Accuracy at 98 minibatches is 0.978516\n",
      "Average training loss at this epoch..minibatch..98..is 0.110026\n",
      "Validation loss after 98 passes is 0.388951\n",
      "Loss at 99 minibatches, 9 epoch,(2m 11s) is 0.109407\n",
      "Accuracy at 99 minibatches is 0.977431\n",
      "Average training loss at this epoch..minibatch..99..is 0.109407\n",
      "Validation loss after 99 passes is 0.348475\n",
      "Reached 10 epochs\n",
      "i 100\n",
      "Test accuracy: 0.9375\n",
      "Max test accruacy: 0.9375\n",
      "Test aucroc: 0.9635467980295567\n",
      "Max test aucroc: 0.9635467980295567\n",
      "Train accuracy: 0.984375\n",
      "Max train accruacy: 0.984375\n",
      "Train aucroc: 0.9923868644129834\n",
      "Max train aucroc: 0.9923868644129834\n",
      "Loss at 101 minibatches, 10 epoch,(2m 14s) is 0.056234\n",
      "Accuracy at 101 minibatches is 0.984375\n",
      "Average training loss at this epoch..minibatch..101..is 0.056234\n",
      "Validation loss after 101 passes is 0.395008\n",
      "Validation loss is higher than training loss at 101 is 0.395008 , stopping training!\n",
      "Average training loss at 101 is 0.384110\n",
      "Loss at 102 minibatches, 10 epoch,(2m 16s) is 0.043776\n",
      "Accuracy at 102 minibatches is 0.992188\n",
      "Average training loss at this epoch..minibatch..102..is 0.043776\n",
      "Validation loss after 102 passes is 0.408898\n",
      "Validation loss is higher than training loss at 102 is 0.408898 , stopping training!\n",
      "Average training loss at 102 is 0.380275\n",
      "Loss at 103 minibatches, 10 epoch,(2m 17s) is 0.048534\n",
      "Accuracy at 103 minibatches is 0.989583\n",
      "Average training loss at this epoch..minibatch..103..is 0.048534\n",
      "Validation loss after 103 passes is 0.326698\n",
      "Loss at 104 minibatches, 10 epoch,(2m 18s) is 0.043468\n",
      "Accuracy at 104 minibatches is 0.992188\n",
      "Average training loss at this epoch..minibatch..104..is 0.043468\n",
      "Validation loss after 104 passes is 0.440996\n",
      "Validation loss is higher than training loss at 104 is 0.440996 , stopping training!\n",
      "Average training loss at 104 is 0.373103\n",
      "Loss at 105 minibatches, 10 epoch,(2m 19s) is 0.042817\n",
      "Accuracy at 105 minibatches is 0.993750\n",
      "Average training loss at this epoch..minibatch..105..is 0.042817\n",
      "Validation loss after 105 passes is 0.406246\n",
      "Validation loss is higher than training loss at 105 is 0.406246 , stopping training!\n",
      "Average training loss at 105 is 0.369599\n",
      "Loss at 106 minibatches, 10 epoch,(2m 21s) is 0.041925\n",
      "Accuracy at 106 minibatches is 0.994792\n",
      "Average training loss at this epoch..minibatch..106..is 0.041925\n",
      "Validation loss after 106 passes is 0.437488\n",
      "Validation loss is higher than training loss at 106 is 0.437488 , stopping training!\n",
      "Average training loss at 106 is 0.366139\n",
      "Loss at 107 minibatches, 10 epoch,(2m 22s) is 0.043406\n",
      "Accuracy at 107 minibatches is 0.995536\n",
      "Average training loss at this epoch..minibatch..107..is 0.043406\n",
      "Validation loss after 107 passes is 0.398580\n",
      "Validation loss is higher than training loss at 107 is 0.398580 , stopping training!\n",
      "Average training loss at 107 is 0.362903\n",
      "Loss at 108 minibatches, 10 epoch,(2m 23s) is 0.048049\n",
      "Accuracy at 108 minibatches is 0.994141\n",
      "Average training loss at this epoch..minibatch..108..is 0.048049\n",
      "Validation loss after 108 passes is 0.435783\n",
      "Validation loss is higher than training loss at 108 is 0.435783 , stopping training!\n",
      "Average training loss at 108 is 0.360022\n",
      "Loss at 109 minibatches, 10 epoch,(2m 24s) is 0.054723\n",
      "Accuracy at 109 minibatches is 0.993056\n",
      "Average training loss at this epoch..minibatch..109..is 0.054723\n",
      "Validation loss after 109 passes is 0.456656\n",
      "Validation loss is higher than training loss at 109 is 0.456656 , stopping training!\n",
      "Average training loss at 109 is 0.357478\n",
      "Reached 11 epochs\n",
      "i 110\n",
      "Test accuracy: 0.84375\n",
      "Max test accruacy: 0.9375\n",
      "Test aucroc: 0.9466666666666668\n",
      "Max test aucroc: 0.9635467980295567\n",
      "Train accuracy: 0.9913194444444444\n",
      "Max train accruacy: 0.9913194444444444\n",
      "Train aucroc: 0.9927847000869313\n",
      "Max train aucroc: 0.9927847000869313\n",
      "Loss at 111 minibatches, 11 epoch,(2m 28s) is 0.031846\n",
      "Accuracy at 111 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..111..is 0.031846\n",
      "Validation loss after 111 passes is 0.459271\n",
      "Validation loss is higher than training loss at 111 is 0.459271 , stopping training!\n",
      "Average training loss at 111 is 0.354221\n",
      "Loss at 112 minibatches, 11 epoch,(2m 29s) is 0.029403\n",
      "Accuracy at 112 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..112..is 0.029403\n",
      "Validation loss after 112 passes is 0.507558\n",
      "Validation loss is higher than training loss at 112 is 0.507558 , stopping training!\n",
      "Average training loss at 112 is 0.350981\n",
      "Loss at 113 minibatches, 11 epoch,(2m 30s) is 0.024669\n",
      "Accuracy at 113 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..113..is 0.024669\n",
      "Validation loss after 113 passes is 0.527842\n",
      "Validation loss is higher than training loss at 113 is 0.527842 , stopping training!\n",
      "Average training loss at 113 is 0.347689\n",
      "Loss at 114 minibatches, 11 epoch,(2m 31s) is 0.032372\n",
      "Accuracy at 114 minibatches is 0.996094\n",
      "Average training loss at this epoch..minibatch..114..is 0.032372\n",
      "Validation loss after 114 passes is 0.483313\n",
      "Validation loss is higher than training loss at 114 is 0.483313 , stopping training!\n",
      "Average training loss at 114 is 0.344852\n",
      "Loss at 115 minibatches, 11 epoch,(2m 33s) is 0.030489\n",
      "Accuracy at 115 minibatches is 0.996875\n",
      "Average training loss at this epoch..minibatch..115..is 0.030489\n",
      "Validation loss after 115 passes is 0.419593\n",
      "Validation loss is higher than training loss at 115 is 0.419593 , stopping training!\n",
      "Average training loss at 115 is 0.341757\n",
      "Loss at 116 minibatches, 11 epoch,(2m 34s) is 0.032838\n",
      "Accuracy at 116 minibatches is 0.997396\n",
      "Average training loss at this epoch..minibatch..116..is 0.032838\n",
      "Validation loss after 116 passes is 0.476439\n",
      "Validation loss is higher than training loss at 116 is 0.476439 , stopping training!\n",
      "Average training loss at 116 is 0.338927\n",
      "Loss at 117 minibatches, 11 epoch,(2m 35s) is 0.040367\n",
      "Accuracy at 117 minibatches is 0.995536\n",
      "Average training loss at this epoch..minibatch..117..is 0.040367\n",
      "Validation loss after 117 passes is 0.454568\n",
      "Validation loss is higher than training loss at 117 is 0.454568 , stopping training!\n",
      "Average training loss at 117 is 0.336536\n",
      "Loss at 118 minibatches, 11 epoch,(2m 36s) is 0.036979\n",
      "Accuracy at 118 minibatches is 0.996094\n",
      "Average training loss at this epoch..minibatch..118..is 0.036979\n",
      "Validation loss after 118 passes is 0.435202\n",
      "Validation loss is higher than training loss at 118 is 0.435202 , stopping training!\n",
      "Average training loss at 118 is 0.333515\n",
      "Loss at 119 minibatches, 11 epoch,(2m 38s) is 0.045861\n",
      "Accuracy at 119 minibatches is 0.993056\n",
      "Average training loss at this epoch..minibatch..119..is 0.045861\n",
      "Validation loss after 119 passes is 0.426439\n",
      "Validation loss is higher than training loss at 119 is 0.426439 , stopping training!\n",
      "Average training loss at 119 is 0.331510\n",
      "Reached 12 epochs\n",
      "i 120\n",
      "Test accuracy: 0.890625\n",
      "Max test accruacy: 0.9375\n",
      "Test aucroc: 0.937937937937938\n",
      "Max test aucroc: 0.9635467980295567\n",
      "Train accuracy: 0.9913194444444444\n",
      "Max train accruacy: 0.9913194444444444\n",
      "Train aucroc: 0.999316691624384\n",
      "Max train aucroc: 0.999316691624384\n",
      "Loss at 121 minibatches, 12 epoch,(2m 41s) is 0.045751\n",
      "Accuracy at 121 minibatches is 0.984375\n",
      "Average training loss at this epoch..minibatch..121..is 0.045751\n",
      "Validation loss after 121 passes is 0.308489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at 122 minibatches, 12 epoch,(2m 42s) is 0.029286\n",
      "Accuracy at 122 minibatches is 0.992188\n",
      "Average training loss at this epoch..minibatch..122..is 0.029286\n",
      "Validation loss after 122 passes is 0.414570\n",
      "Validation loss is higher than training loss at 122 is 0.414570 , stopping training!\n",
      "Average training loss at 122 is 0.326015\n",
      "Loss at 123 minibatches, 12 epoch,(2m 43s) is 0.067008\n",
      "Accuracy at 123 minibatches is 0.989583\n",
      "Average training loss at this epoch..minibatch..123..is 0.067008\n",
      "Validation loss after 123 passes is 0.389506\n",
      "Validation loss is higher than training loss at 123 is 0.389506 , stopping training!\n",
      "Average training loss at 123 is 0.324361\n",
      "Loss at 124 minibatches, 12 epoch,(2m 44s) is 0.065410\n",
      "Accuracy at 124 minibatches is 0.988281\n",
      "Average training loss at this epoch..minibatch..124..is 0.065410\n",
      "Validation loss after 124 passes is 0.248444\n",
      "Loss at 125 minibatches, 12 epoch,(2m 46s) is 0.057977\n",
      "Accuracy at 125 minibatches is 0.990625\n",
      "Average training loss at this epoch..minibatch..125..is 0.057977\n",
      "Validation loss after 125 passes is 0.457402\n",
      "Validation loss is higher than training loss at 125 is 0.457402 , stopping training!\n",
      "Average training loss at 125 is 0.319406\n",
      "Loss at 126 minibatches, 12 epoch,(2m 47s) is 0.049656\n",
      "Accuracy at 126 minibatches is 0.992188\n",
      "Average training loss at this epoch..minibatch..126..is 0.049656\n",
      "Validation loss after 126 passes is 0.373628\n",
      "Validation loss is higher than training loss at 126 is 0.373628 , stopping training!\n",
      "Average training loss at 126 is 0.316675\n",
      "Loss at 127 minibatches, 12 epoch,(2m 48s) is 0.043878\n",
      "Accuracy at 127 minibatches is 0.993304\n",
      "Average training loss at this epoch..minibatch..127..is 0.043878\n",
      "Validation loss after 127 passes is 0.391095\n",
      "Validation loss is higher than training loss at 127 is 0.391095 , stopping training!\n",
      "Average training loss at 127 is 0.314002\n",
      "Loss at 128 minibatches, 12 epoch,(2m 49s) is 0.038855\n",
      "Accuracy at 128 minibatches is 0.994141\n",
      "Average training loss at this epoch..minibatch..128..is 0.038855\n",
      "Validation loss after 128 passes is 0.442568\n",
      "Validation loss is higher than training loss at 128 is 0.442568 , stopping training!\n",
      "Average training loss at 128 is 0.311326\n",
      "Loss at 129 minibatches, 12 epoch,(2m 51s) is 0.045384\n",
      "Accuracy at 129 minibatches is 0.993056\n",
      "Average training loss at this epoch..minibatch..129..is 0.045384\n",
      "Validation loss after 129 passes is 0.440535\n",
      "Validation loss is higher than training loss at 129 is 0.440535 , stopping training!\n",
      "Average training loss at 129 is 0.309500\n",
      "Reached 13 epochs\n",
      "i 130\n",
      "Test accuracy: 0.921875\n",
      "Max test accruacy: 0.9375\n",
      "Test aucroc: 0.9544534412955465\n",
      "Max test aucroc: 0.9635467980295567\n",
      "Train accuracy: 0.9965277777777778\n",
      "Max train accruacy: 0.9965277777777778\n",
      "Train aucroc: 0.9980904880655505\n",
      "Max train aucroc: 0.999316691624384\n",
      "Loss at 131 minibatches, 13 epoch,(2m 54s) is 0.005250\n",
      "Accuracy at 131 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..131..is 0.005250\n",
      "Validation loss after 131 passes is 0.321654\n",
      "Validation loss is higher than training loss at 131 is 0.321654 , stopping training!\n",
      "Average training loss at 131 is 0.306921\n",
      "Loss at 132 minibatches, 13 epoch,(2m 55s) is 0.005546\n",
      "Accuracy at 132 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..132..is 0.005546\n",
      "Validation loss after 132 passes is 0.464091\n",
      "Validation loss is higher than training loss at 132 is 0.464091 , stopping training!\n",
      "Average training loss at 132 is 0.304391\n",
      "Loss at 133 minibatches, 13 epoch,(2m 57s) is 0.006313\n",
      "Accuracy at 133 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..133..is 0.006313\n",
      "Validation loss after 133 passes is 0.450192\n",
      "Validation loss is higher than training loss at 133 is 0.450192 , stopping training!\n",
      "Average training loss at 133 is 0.301920\n",
      "Loss at 134 minibatches, 13 epoch,(2m 58s) is 0.005763\n",
      "Accuracy at 134 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..134..is 0.005763\n",
      "Validation loss after 134 passes is 0.482005\n",
      "Validation loss is higher than training loss at 134 is 0.482005 , stopping training!\n",
      "Average training loss at 134 is 0.299459\n",
      "Loss at 135 minibatches, 13 epoch,(2m 59s) is 0.005911\n",
      "Accuracy at 135 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..135..is 0.005911\n",
      "Validation loss after 135 passes is 0.389121\n",
      "Validation loss is higher than training loss at 135 is 0.389121 , stopping training!\n",
      "Average training loss at 135 is 0.297058\n",
      "Loss at 136 minibatches, 13 epoch,(3m 0s) is 0.005624\n",
      "Accuracy at 136 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..136..is 0.005624\n",
      "Validation loss after 136 passes is 0.530628\n",
      "Validation loss is higher than training loss at 136 is 0.530628 , stopping training!\n",
      "Average training loss at 136 is 0.294677\n",
      "Loss at 137 minibatches, 13 epoch,(3m 1s) is 0.013751\n",
      "Accuracy at 137 minibatches is 0.997768\n",
      "Average training loss at this epoch..minibatch..137..is 0.013751\n",
      "Validation loss after 137 passes is 0.535469\n",
      "Validation loss is higher than training loss at 137 is 0.535469 , stopping training!\n",
      "Average training loss at 137 is 0.292804\n",
      "Loss at 138 minibatches, 13 epoch,(3m 3s) is 0.012777\n",
      "Accuracy at 138 minibatches is 0.998047\n",
      "Average training loss at this epoch..minibatch..138..is 0.012777\n",
      "Validation loss after 138 passes is 0.529159\n",
      "Validation loss is higher than training loss at 138 is 0.529159 , stopping training!\n",
      "Average training loss at 138 is 0.290510\n",
      "Loss at 139 minibatches, 13 epoch,(3m 4s) is 0.011975\n",
      "Accuracy at 139 minibatches is 0.998264\n",
      "Average training loss at this epoch..minibatch..139..is 0.011975\n",
      "Validation loss after 139 passes is 0.536423\n",
      "Validation loss is higher than training loss at 139 is 0.536423 , stopping training!\n",
      "Average training loss at 139 is 0.288248\n",
      "Reached 14 epochs\n",
      "i 140\n",
      "Test accuracy: 0.890625\n",
      "Max test accruacy: 0.9375\n",
      "Test aucroc: 0.9709709709709711\n",
      "Max test aucroc: 0.9709709709709711\n",
      "Train accuracy: 0.9965277777777778\n",
      "Max train accruacy: 0.9965277777777778\n",
      "Train aucroc: 0.9996425916395036\n",
      "Max train aucroc: 0.9996425916395036\n",
      "Loss at 141 minibatches, 14 epoch,(3m 7s) is 0.007366\n",
      "Accuracy at 141 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..141..is 0.007366\n",
      "Validation loss after 141 passes is 0.394539\n",
      "Validation loss is higher than training loss at 141 is 0.394539 , stopping training!\n",
      "Average training loss at 141 is 0.286036\n",
      "Loss at 142 minibatches, 14 epoch,(3m 8s) is 0.005993\n",
      "Accuracy at 142 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..142..is 0.005993\n",
      "Validation loss after 142 passes is 0.476278\n",
      "Validation loss is higher than training loss at 142 is 0.476278 , stopping training!\n",
      "Average training loss at 142 is 0.283838\n",
      "Loss at 143 minibatches, 14 epoch,(3m 10s) is 0.005337\n",
      "Accuracy at 143 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..143..is 0.005337\n",
      "Validation loss after 143 passes is 0.389007\n",
      "Validation loss is higher than training loss at 143 is 0.389007 , stopping training!\n",
      "Average training loss at 143 is 0.281669\n",
      "Loss at 144 minibatches, 14 epoch,(3m 11s) is 0.022123\n",
      "Accuracy at 144 minibatches is 0.996094\n",
      "Average training loss at this epoch..minibatch..144..is 0.022123\n",
      "Validation loss after 144 passes is 0.529934\n",
      "Validation loss is higher than training loss at 144 is 0.529934 , stopping training!\n",
      "Average training loss at 144 is 0.280060\n",
      "Loss at 145 minibatches, 14 epoch,(3m 12s) is 0.018730\n",
      "Accuracy at 145 minibatches is 0.996875\n",
      "Average training loss at this epoch..minibatch..145..is 0.018730\n",
      "Validation loss after 145 passes is 0.478031\n",
      "Validation loss is higher than training loss at 145 is 0.478031 , stopping training!\n",
      "Average training loss at 145 is 0.277961\n",
      "Loss at 146 minibatches, 14 epoch,(3m 13s) is 0.016072\n",
      "Accuracy at 146 minibatches is 0.997396\n",
      "Average training loss at this epoch..minibatch..146..is 0.016072\n",
      "Validation loss after 146 passes is 0.532001\n",
      "Validation loss is higher than training loss at 146 is 0.532001 , stopping training!\n",
      "Average training loss at 146 is 0.275876\n",
      "Loss at 147 minibatches, 14 epoch,(3m 15s) is 0.014416\n",
      "Accuracy at 147 minibatches is 0.997768\n",
      "Average training loss at this epoch..minibatch..147..is 0.014416\n",
      "Validation loss after 147 passes is 0.394190\n",
      "Validation loss is higher than training loss at 147 is 0.394190 , stopping training!\n",
      "Average training loss at 147 is 0.273836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at 148 minibatches, 14 epoch,(3m 16s) is 0.013098\n",
      "Accuracy at 148 minibatches is 0.998047\n",
      "Average training loss at this epoch..minibatch..148..is 0.013098\n",
      "Validation loss after 148 passes is 0.354768\n",
      "Validation loss is higher than training loss at 148 is 0.354768 , stopping training!\n",
      "Average training loss at 148 is 0.271821\n",
      "Loss at 149 minibatches, 14 epoch,(3m 17s) is 0.014484\n",
      "Accuracy at 149 minibatches is 0.996528\n",
      "Average training loss at this epoch..minibatch..149..is 0.014484\n",
      "Validation loss after 149 passes is 0.472368\n",
      "Validation loss is higher than training loss at 149 is 0.472368 , stopping training!\n",
      "Average training loss at 149 is 0.269997\n",
      "Reached 15 epochs\n",
      "i 150\n",
      "Test accuracy: 0.921875\n",
      "Max test accruacy: 0.9375\n",
      "Test aucroc: 0.978978978978979\n",
      "Max test aucroc: 0.978978978978979\n",
      "Train accuracy: 0.9965277777777778\n",
      "Max train accruacy: 0.9965277777777778\n",
      "Train aucroc: 0.9998982306674614\n",
      "Max train aucroc: 0.9998982306674614\n",
      "Loss at 151 minibatches, 15 epoch,(3m 20s) is 0.002976\n",
      "Accuracy at 151 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..151..is 0.002976\n",
      "Validation loss after 151 passes is 0.322253\n",
      "Validation loss is higher than training loss at 151 is 0.322253 , stopping training!\n",
      "Average training loss at 151 is 0.268034\n",
      "Loss at 152 minibatches, 15 epoch,(3m 22s) is 0.037207\n",
      "Accuracy at 152 minibatches is 0.992188\n",
      "Average training loss at this epoch..minibatch..152..is 0.037207\n",
      "Validation loss after 152 passes is 0.345684\n",
      "Validation loss is higher than training loss at 152 is 0.345684 , stopping training!\n",
      "Average training loss at 152 is 0.266599\n",
      "Loss at 153 minibatches, 15 epoch,(3m 23s) is 0.029534\n",
      "Accuracy at 153 minibatches is 0.994792\n",
      "Average training loss at this epoch..minibatch..153..is 0.029534\n",
      "Validation loss after 153 passes is 0.418382\n",
      "Validation loss is higher than training loss at 153 is 0.418382 , stopping training!\n",
      "Average training loss at 153 is 0.264770\n",
      "Loss at 154 minibatches, 15 epoch,(3m 24s) is 0.023067\n",
      "Accuracy at 154 minibatches is 0.996094\n",
      "Average training loss at this epoch..minibatch..154..is 0.023067\n",
      "Validation loss after 154 passes is 0.447926\n",
      "Validation loss is higher than training loss at 154 is 0.447926 , stopping training!\n",
      "Average training loss at 154 is 0.262891\n",
      "Loss at 155 minibatches, 15 epoch,(3m 25s) is 0.019218\n",
      "Accuracy at 155 minibatches is 0.996875\n",
      "Average training loss at this epoch..minibatch..155..is 0.019218\n",
      "Validation loss after 155 passes is 0.325624\n",
      "Validation loss is higher than training loss at 155 is 0.325624 , stopping training!\n",
      "Average training loss at 155 is 0.261041\n",
      "Loss at 156 minibatches, 15 epoch,(3m 27s) is 0.017005\n",
      "Accuracy at 156 minibatches is 0.997396\n",
      "Average training loss at this epoch..minibatch..156..is 0.017005\n",
      "Validation loss after 156 passes is 0.339148\n",
      "Validation loss is higher than training loss at 156 is 0.339148 , stopping training!\n",
      "Average training loss at 156 is 0.259232\n",
      "Loss at 157 minibatches, 15 epoch,(3m 28s) is 0.024325\n",
      "Accuracy at 157 minibatches is 0.993304\n",
      "Average training loss at this epoch..minibatch..157..is 0.024325\n",
      "Validation loss after 157 passes is 0.320506\n",
      "Validation loss is higher than training loss at 157 is 0.320506 , stopping training!\n",
      "Average training loss at 157 is 0.257887\n",
      "Loss at 158 minibatches, 15 epoch,(3m 29s) is 0.021962\n",
      "Accuracy at 158 minibatches is 0.994141\n",
      "Average training loss at this epoch..minibatch..158..is 0.021962\n",
      "Validation loss after 158 passes is 0.188348\n",
      "Loss at 159 minibatches, 15 epoch,(3m 30s) is 0.020389\n",
      "Accuracy at 159 minibatches is 0.994792\n",
      "Average training loss at this epoch..minibatch..159..is 0.020389\n",
      "Validation loss after 159 passes is 0.319876\n",
      "Validation loss is higher than training loss at 159 is 0.319876 , stopping training!\n",
      "Average training loss at 159 is 0.254397\n",
      "Reached 16 epochs\n",
      "i 160\n",
      "Test accuracy: 0.90625\n",
      "Max test accruacy: 0.9375\n",
      "Test aucroc: 0.966966966966967\n",
      "Max test aucroc: 0.978978978978979\n",
      "Train accuracy: 0.9947916666666666\n",
      "Max train accruacy: 0.9965277777777778\n",
      "Train aucroc: 0.9999267967732017\n",
      "Max train aucroc: 0.9999267967732017\n",
      "Loss at 161 minibatches, 16 epoch,(3m 33s) is 0.068578\n",
      "Accuracy at 161 minibatches is 0.984375\n",
      "Average training loss at this epoch..minibatch..161..is 0.068578\n",
      "Validation loss after 161 passes is 0.153624\n",
      "Loss at 162 minibatches, 16 epoch,(3m 35s) is 0.040120\n",
      "Accuracy at 162 minibatches is 0.992188\n",
      "Average training loss at this epoch..minibatch..162..is 0.040120\n",
      "Validation loss after 162 passes is 0.204050\n",
      "Loss at 163 minibatches, 16 epoch,(3m 36s) is 0.029970\n",
      "Accuracy at 163 minibatches is 0.994792\n",
      "Average training loss at this epoch..minibatch..163..is 0.029970\n",
      "Validation loss after 163 passes is 0.249781\n",
      "Loss at 164 minibatches, 16 epoch,(3m 37s) is 0.024722\n",
      "Accuracy at 164 minibatches is 0.992188\n",
      "Average training loss at this epoch..minibatch..164..is 0.024722\n",
      "Validation loss after 164 passes is 0.320610\n",
      "Validation loss is higher than training loss at 164 is 0.320610 , stopping training!\n",
      "Average training loss at 164 is 0.248189\n",
      "Loss at 165 minibatches, 16 epoch,(3m 39s) is 0.020775\n",
      "Accuracy at 165 minibatches is 0.990625\n",
      "Average training loss at this epoch..minibatch..165..is 0.020775\n",
      "Validation loss after 165 passes is 0.302869\n",
      "Validation loss is higher than training loss at 165 is 0.302869 , stopping training!\n",
      "Average training loss at 165 is 0.246557\n",
      "Loss at 166 minibatches, 16 epoch,(3m 40s) is 0.018082\n",
      "Accuracy at 166 minibatches is 0.992188\n",
      "Average training loss at this epoch..minibatch..166..is 0.018082\n",
      "Validation loss after 166 passes is 0.301964\n",
      "Validation loss is higher than training loss at 166 is 0.301964 , stopping training!\n",
      "Average training loss at 166 is 0.244944\n",
      "Loss at 167 minibatches, 16 epoch,(3m 41s) is 0.016046\n",
      "Accuracy at 167 minibatches is 0.993304\n",
      "Average training loss at this epoch..minibatch..167..is 0.016046\n",
      "Validation loss after 167 passes is 0.290808\n",
      "Validation loss is higher than training loss at 167 is 0.290808 , stopping training!\n",
      "Average training loss at 167 is 0.243347\n",
      "Loss at 168 minibatches, 16 epoch,(3m 42s) is 0.014390\n",
      "Accuracy at 168 minibatches is 0.994141\n",
      "Average training loss at this epoch..minibatch..168..is 0.014390\n",
      "Validation loss after 168 passes is 0.361524\n",
      "Validation loss is higher than training loss at 168 is 0.361524 , stopping training!\n",
      "Average training loss at 168 is 0.241765\n",
      "Loss at 169 minibatches, 16 epoch,(3m 44s) is 0.013125\n",
      "Accuracy at 169 minibatches is 0.994792\n",
      "Average training loss at this epoch..minibatch..169..is 0.013125\n",
      "Validation loss after 169 passes is 0.376798\n",
      "Validation loss is higher than training loss at 169 is 0.376798 , stopping training!\n",
      "Average training loss at 169 is 0.240204\n",
      "Reached 17 epochs\n",
      "i 170\n",
      "Test accuracy: 0.921875\n",
      "Max test accruacy: 0.9375\n",
      "Test aucroc: 0.9862745098039216\n",
      "Max test aucroc: 0.9862745098039216\n",
      "Train accuracy: 0.9965277777777778\n",
      "Max train accruacy: 0.9965277777777778\n",
      "Train aucroc: 0.9988659988659989\n",
      "Max train aucroc: 0.9999267967732017\n",
      "Loss at 171 minibatches, 17 epoch,(3m 47s) is 0.003254\n",
      "Accuracy at 171 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..171..is 0.003254\n",
      "Validation loss after 171 passes is 0.437373\n",
      "Validation loss is higher than training loss at 171 is 0.437373 , stopping training!\n",
      "Average training loss at 171 is 0.238666\n",
      "Loss at 172 minibatches, 17 epoch,(3m 48s) is 0.004771\n",
      "Accuracy at 172 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..172..is 0.004771\n",
      "Validation loss after 172 passes is 0.444139\n",
      "Validation loss is higher than training loss at 172 is 0.444139 , stopping training!\n",
      "Average training loss at 172 is 0.237166\n",
      "Loss at 173 minibatches, 17 epoch,(3m 49s) is 0.011993\n",
      "Accuracy at 173 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..173..is 0.011993\n",
      "Validation loss after 173 passes is 0.411710\n",
      "Validation loss is higher than training loss at 173 is 0.411710 , stopping training!\n",
      "Average training loss at 173 is 0.235816\n",
      "Loss at 174 minibatches, 17 epoch,(3m 51s) is 0.010511\n",
      "Accuracy at 174 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..174..is 0.010511\n",
      "Validation loss after 174 passes is 0.608630\n",
      "Validation loss is higher than training loss at 174 is 0.608630 , stopping training!\n",
      "Average training loss at 174 is 0.234352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at 175 minibatches, 17 epoch,(3m 52s) is 0.009288\n",
      "Accuracy at 175 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..175..is 0.009288\n",
      "Validation loss after 175 passes is 0.557881\n",
      "Validation loss is higher than training loss at 175 is 0.557881 , stopping training!\n",
      "Average training loss at 175 is 0.232897\n",
      "Loss at 176 minibatches, 17 epoch,(3m 53s) is 0.008140\n",
      "Accuracy at 176 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..176..is 0.008140\n",
      "Validation loss after 176 passes is 0.567012\n",
      "Validation loss is higher than training loss at 176 is 0.567012 , stopping training!\n",
      "Average training loss at 176 is 0.231447\n",
      "Loss at 177 minibatches, 17 epoch,(3m 54s) is 0.015260\n",
      "Accuracy at 177 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..177..is 0.015260\n",
      "Validation loss after 177 passes is 0.485398\n",
      "Validation loss is higher than training loss at 177 is 0.485398 , stopping training!\n",
      "Average training loss at 177 is 0.230363\n",
      "Loss at 178 minibatches, 17 epoch,(3m 56s) is 0.013635\n",
      "Accuracy at 178 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..178..is 0.013635\n",
      "Validation loss after 178 passes is 0.562749\n",
      "Validation loss is higher than training loss at 178 is 0.562749 , stopping training!\n",
      "Average training loss at 178 is 0.228946\n",
      "Loss at 179 minibatches, 17 epoch,(3m 57s) is 0.012486\n",
      "Accuracy at 179 minibatches is 0.998264\n",
      "Average training loss at this epoch..minibatch..179..is 0.012486\n",
      "Validation loss after 179 passes is 0.570455\n",
      "Validation loss is higher than training loss at 179 is 0.570455 , stopping training!\n",
      "Average training loss at 179 is 0.227553\n",
      "Reached 18 epochs\n",
      "i 180\n",
      "Test accuracy: 0.875\n",
      "Max test accruacy: 0.9375\n",
      "Test aucroc: 0.9692307692307692\n",
      "Max test aucroc: 0.9862745098039216\n",
      "Train accuracy: 0.9982638888888888\n",
      "Max train accruacy: 0.9982638888888888\n",
      "Train aucroc: 0.9998975154824824\n",
      "Max train aucroc: 0.9999267967732017\n",
      "Loss at 181 minibatches, 18 epoch,(4m 0s) is 0.001634\n",
      "Accuracy at 181 minibatches is 1.000000\n",
      "Average training loss at this epoch..minibatch..181..is 0.001634\n",
      "Validation loss after 181 passes is 0.556374\n",
      "Validation loss is higher than training loss at 181 is 0.556374 , stopping training!\n",
      "Average training loss at 181 is 0.226167\n",
      "Loss at 182 minibatches, 18 epoch,(4m 2s) is 0.001800\n",
      "Accuracy at 182 minibatches is 0.992188\n",
      "Average training loss at this epoch..minibatch..182..is 0.001800\n",
      "Validation loss after 182 passes is 0.566871\n",
      "Validation loss is higher than training loss at 182 is 0.566871 , stopping training!\n",
      "Average training loss at 182 is 0.224800\n",
      "Loss at 183 minibatches, 18 epoch,(4m 3s) is 0.003056\n",
      "Accuracy at 183 minibatches is 0.994792\n",
      "Average training loss at this epoch..minibatch..183..is 0.003056\n",
      "Validation loss after 183 passes is 0.432012\n",
      "Validation loss is higher than training loss at 183 is 0.432012 , stopping training!\n",
      "Average training loss at 183 is 0.223471\n",
      "Loss at 184 minibatches, 18 epoch,(4m 4s) is 0.002699\n",
      "Accuracy at 184 minibatches is 0.992188\n",
      "Average training loss at this epoch..minibatch..184..is 0.002699\n",
      "Validation loss after 184 passes is 0.503915\n",
      "Validation loss is higher than training loss at 184 is 0.503915 , stopping training!\n",
      "Average training loss at 184 is 0.222135\n",
      "Loss at 185 minibatches, 18 epoch,(4m 5s) is 0.002380\n",
      "Accuracy at 185 minibatches is 0.993750\n",
      "Average training loss at this epoch..minibatch..185..is 0.002380\n",
      "Validation loss after 185 passes is 0.497067\n",
      "Validation loss is higher than training loss at 185 is 0.497067 , stopping training!\n",
      "Average training loss at 185 is 0.220812\n",
      "Loss at 186 minibatches, 18 epoch,(4m 7s) is 0.002287\n",
      "Accuracy at 186 minibatches is 0.994792\n",
      "Average training loss at this epoch..minibatch..186..is 0.002287\n",
      "Validation loss after 186 passes is 0.657682\n",
      "Validation loss is higher than training loss at 186 is 0.657682 , stopping training!\n",
      "Average training loss at 186 is 0.219508\n",
      "Loss at 187 minibatches, 18 epoch,(4m 8s) is 0.002150\n",
      "Accuracy at 187 minibatches is 0.995536\n",
      "Average training loss at this epoch..minibatch..187..is 0.002150\n",
      "Validation loss after 187 passes is 0.515609\n",
      "Validation loss is higher than training loss at 187 is 0.515609 , stopping training!\n",
      "Average training loss at 187 is 0.218217\n",
      "Loss at 188 minibatches, 18 epoch,(4m 9s) is 0.011287\n",
      "Accuracy at 188 minibatches is 0.996094\n",
      "Average training loss at this epoch..minibatch..188..is 0.011287\n",
      "Validation loss after 188 passes is 0.448360\n",
      "Validation loss is higher than training loss at 188 is 0.448360 , stopping training!\n",
      "Average training loss at 188 is 0.217376\n",
      "Loss at 189 minibatches, 18 epoch,(4m 10s) is 0.011654\n",
      "Accuracy at 189 minibatches is 0.993056\n",
      "Average training loss at this epoch..minibatch..189..is 0.011654\n",
      "Validation loss after 189 passes is 0.598543\n",
      "Validation loss is higher than training loss at 189 is 0.598543 , stopping training!\n",
      "Average training loss at 189 is 0.216190\n",
      "Reached 19 epochs\n",
      "i 190\n",
      "Test accuracy: 0.859375\n",
      "Max test accruacy: 0.9375\n",
      "Test aucroc: 0.9712301587301587\n",
      "Max test aucroc: 0.9862745098039216\n",
      "Train accuracy: 0.9965277777777778\n",
      "Max train accruacy: 0.9982638888888888\n",
      "Train aucroc: 0.9994475379090764\n",
      "Max train aucroc: 0.9999267967732017\n",
      "Loss at 191 minibatches, 19 epoch,(4m 14s) is 0.046122\n",
      "Accuracy at 191 minibatches is 0.984375\n",
      "Average training loss at this epoch..minibatch..191..is 0.046122\n",
      "Validation loss after 191 passes is 0.910722\n",
      "Validation loss is higher than training loss at 191 is 0.910722 , stopping training!\n",
      "Average training loss at 191 is 0.215201\n",
      "Loss at 192 minibatches, 19 epoch,(4m 15s) is 0.096755\n",
      "Accuracy at 192 minibatches is 0.953125\n",
      "Average training loss at this epoch..minibatch..192..is 0.096755\n",
      "Validation loss after 192 passes is 1.051692\n",
      "Validation loss is higher than training loss at 192 is 1.051692 , stopping training!\n",
      "Average training loss at 192 is 0.214809\n",
      "Loss at 193 minibatches, 19 epoch,(4m 16s) is 0.086145\n",
      "Accuracy at 193 minibatches is 0.958333\n",
      "Average training loss at this epoch..minibatch..193..is 0.086145\n",
      "Validation loss after 193 passes is 1.078840\n",
      "Validation loss is higher than training loss at 193 is 1.078840 , stopping training!\n",
      "Average training loss at 193 is 0.213948\n",
      "Loss at 194 minibatches, 19 epoch,(4m 17s) is 0.069974\n",
      "Accuracy at 194 minibatches is 0.968750\n",
      "Average training loss at this epoch..minibatch..194..is 0.069974\n",
      "Validation loss after 194 passes is 0.895393\n",
      "Validation loss is higher than training loss at 194 is 0.895393 , stopping training!\n",
      "Average training loss at 194 is 0.212848\n",
      "Loss at 195 minibatches, 19 epoch,(4m 19s) is 0.064879\n",
      "Accuracy at 195 minibatches is 0.975000\n",
      "Average training loss at this epoch..minibatch..195..is 0.064879\n",
      "Validation loss after 195 passes is 0.480296\n",
      "Validation loss is higher than training loss at 195 is 0.480296 , stopping training!\n",
      "Average training loss at 195 is 0.211892\n",
      "Loss at 196 minibatches, 19 epoch,(4m 20s) is 0.059540\n",
      "Accuracy at 196 minibatches is 0.979167\n",
      "Average training loss at this epoch..minibatch..196..is 0.059540\n",
      "Validation loss after 196 passes is 0.307539\n",
      "Validation loss is higher than training loss at 196 is 0.307539 , stopping training!\n",
      "Average training loss at 196 is 0.210880\n",
      "Loss at 197 minibatches, 19 epoch,(4m 21s) is 0.054880\n",
      "Accuracy at 197 minibatches is 0.982143\n",
      "Average training loss at this epoch..minibatch..197..is 0.054880\n",
      "Validation loss after 197 passes is 0.270832\n",
      "Validation loss is higher than training loss at 197 is 0.270832 , stopping training!\n",
      "Average training loss at 197 is 0.209846\n",
      "Loss at 198 minibatches, 19 epoch,(4m 22s) is 0.056982\n",
      "Accuracy at 198 minibatches is 0.982422\n",
      "Average training loss at this epoch..minibatch..198..is 0.056982\n",
      "Validation loss after 198 passes is 0.252737\n",
      "Validation loss is higher than training loss at 198 is 0.252737 , stopping training!\n",
      "Average training loss at 198 is 0.209075\n",
      "Loss at 199 minibatches, 19 epoch,(4m 24s) is 0.052669\n",
      "Accuracy at 199 minibatches is 0.984375\n",
      "Average training loss at this epoch..minibatch..199..is 0.052669\n",
      "Validation loss after 199 passes is 0.252590\n",
      "Validation loss is higher than training loss at 199 is 0.252590 , stopping training!\n",
      "Average training loss at 199 is 0.208014\n",
      "Reached 20 epochs\n",
      "i 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.921875\n",
      "Max test accruacy: 0.9375\n",
      "Test aucroc: 0.9807692307692308\n",
      "Max test aucroc: 0.9862745098039216\n",
      "Train accuracy: 0.9965277777777778\n",
      "Max train accruacy: 0.9982638888888888\n",
      "Train aucroc: 0.9999857036655802\n",
      "Max train aucroc: 0.9999857036655802\n"
     ]
    }
   ],
   "source": [
    "loss_full= train_early_stopping(my_batch, corpus.X_train, corpus.y_train, corpus.X_test, corpus.y_test, word_attn, word_optimizer, \n",
    "                            criterion, 5000, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(test_accuracy_full_batch(corpus.X_test, corpus.y_test, my_batch, word_attn, sent_attn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test_accuracy_full_batch(corpus.X_train, corpus.y_train, my_batch, word_attn, sent_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
